iter:  300.0   lambda:  1e-08   alpha:  0.0   kernel: rbf, method:  cobyla x_init:  [6.08172530e-04 8.24021498e-05 3.08983865e-04 ... 5.52140717e-04
 7.41262573e-04 8.85882443e-04]
loss:  -3.4983042577377534
loss:  -3.990594583410121
loss:  -4.489916664529517
loss:  -4.981442236935114
loss:  -5.480577444831329
loss:  -5.979233147052788
loss:  -6.477822516838581
loss:  -6.977400433425592
loss:  -7.478580616440002
loss:  -7.97853285305959
loss:  -8.47917608944687
loss:  -8.979054526972927
loss:  -9.477258516602447
loss:  -9.976932884693095
loss:  -10.474337598910516
loss:  -10.885187746214484
loss:  -11.384039303887675
loss:  -11.88248815126149
loss:  -12.381250015795787
loss:  -12.868875330086375
loss:  -13.368632389919222
loss:  -13.867928825370818
loss:  -14.36708216550462
loss:  -14.866673375875225
loss:  -15.364025967259863
loss:  -15.860500363831735
loss:  -16.328102473328265
loss:  -16.822671515770793
loss:  -17.158059753968377
loss:  -17.649521591386115
loss:  -18.144121341472378
loss:  -18.64389798463892
loss:  -19.142782052017722
loss:  -19.556551249824153
loss:  -20.056301893497572
loss:  -20.556063994148648
loss:  -21.05527346081362
loss:  -21.515336602198015
loss:  -22.014596539281374
loss:  -22.5104184148443
loss:  -23.009539675341223
loss:  -23.507925429676934
loss:  -23.974829287093343
loss:  -24.47409273865079
loss:  -24.973227017064357
loss:  -25.47309503557968
loss:  -25.972284015368746
loss:  -26.471265849173996
loss:  -26.969787273342423
loss:  -27.469062422746383
loss:  -27.94748653876563
loss:  -28.447258447004252
loss:  -28.939631907951192
loss:  -29.43206818364501
loss:  -29.931180227223393
loss:  -30.430280282098536
loss:  -30.930064540390443
loss:  -31.418409807928718
loss:  -31.90833867947802
loss:  -32.4078298964325
loss:  -32.907199683357135
loss:  -33.406919137582754
loss:  -33.905999011557114
loss:  -34.40583946201464
loss:  -34.822238339082226
loss:  -35.322091840901656
loss:  -35.82130153919313
loss:  -36.32161179267131
loss:  -36.73715112860116
loss:  -37.225057595021
loss:  -37.72452172655431
loss:  -38.217052963687834
loss:  -38.71593276900036
loss:  -39.21540113923415
loss:  -39.68754795198334
loss:  -40.18680001597553
loss:  -40.686052063532074
loss:  -41.2254375512742
loss:  -41.72447405931511
loss:  -42.223973502769084
loss:  -42.724783973419164
loss:  -43.224428293016736
loss:  -43.72775269240958
loss:  -44.22770210395413
loss:  -44.6780984084451
loss:  -45.175673806382626
loss:  -45.675544862281676
loss:  -46.17526849772435
loss:  -46.67312005065459
loss:  -47.167198716723284
loss:  -47.51697310265186
loss:  -47.80972298537167
loss:  -48.437881775062614
loss:  -48.948136651412554
loss:  -49.45868742187758
loss:  -49.933607996901856
loss:  -50.428686090596386
loss:  -50.92798082783415
loss:  -51.37873257587435
loss:  -51.86964135926825
loss:  -52.36880946306927
loss:  -52.86506630784128
loss:  -53.311850466451006
loss:  -53.81173822529028
loss:  -54.31101741123067
loss:  -54.77954365107603
loss:  -55.26557039649111
loss:  -55.764528079588366
loss:  -56.354044808341634
loss:  -56.853379411391714
loss:  -57.35284459591168
loss:  -57.84839500982321
loss:  -58.347562643932775
loss:  -58.79638757037639
loss:  -59.29622605083672
loss:  -59.795714213982244
loss:  -60.27122944388205
loss:  -60.76708410300014
loss:  -61.26552593080436
loss:  -61.670749606668515
loss:  -62.169657845093006
loss:  -62.66734107104981
loss:  -63.16660375824676
loss:  -63.66563196526128
loss:  -64.16186232952269
loss:  -64.68583877225606
loss:  -65.18587244265461
loss:  -65.5430297179982
loss:  -66.0154690579502
loss:  -66.51466925356581
loss:  -67.00940857320022
loss:  -67.50175810058217
loss:  -67.768931767805
loss:  -68.26856932840407
loss:  -68.76833339157447
loss:  -69.26751626980672
loss:  -69.7700724611168
loss:  -70.2698897285062
loss:  -70.76883182265735
loss:  -71.25366752194655
loss:  -71.64025225830899
loss:  -72.20198479435683
loss:  -72.69254014788717
loss:  -73.12959823470726
loss:  -73.62944271107736
loss:  -74.10787699057292
loss:  -74.60709598334097
loss:  -75.10360969436881
loss:  -75.60269957167426
loss:  -76.06393368514328
loss:  -76.56316526953314
loss:  -77.06832900591118
loss:  -77.56776553290699
loss:  -77.92694388295662
loss:  -78.42600674741087
loss:  -78.92564180313416
loss:  -79.42359122443499
loss:  -79.92266579381328
loss:  -80.39947529308579
loss:  -80.89945591439748
loss:  -81.24212239826085
loss:  -81.66314070359076
loss:  -83.32783812778015
loss:  -83.77702970830205
loss:  -84.27070437176376
loss:  -84.76375121664236
loss:  -85.2322262469214
loss:  -85.73181366017117
loss:  -86.13827806026883
loss:  -86.63277311663165
loss:  -87.1016778979059
loss:  -87.5880671410972
loss:  -87.99078363910719
loss:  -88.49072459735318
loss:  -89.07326340261365
loss:  -89.47621377664663
loss:  -89.97574584370172
loss:  -90.50361373502481
loss:  -90.97248440527575
loss:  -91.5289953412265
loss:  -91.80777113940408
loss:  -92.1195273873383
loss:  -92.18246657961996
loss:  -92.68189020080627
loss:  -92.83803002778812
loss:  -93.33745974268687
loss:  -93.80565881849742
loss:  -94.19260254649173
loss:  -94.7227641667615
loss:  -95.18497502752159
loss:  -95.57259587990853
loss:  -95.45845559509789
loss:  -95.80255117354027
loss:  -96.30211446807446
loss:  -96.78079184789182
loss:  -97.07142761129094
loss:  -97.51908551936725
loss:  -98.01859403848265
loss:  -98.51750376440293
loss:  -98.35515383147252
loss:  -98.96436909775345
loss:  -99.46408529594379
loss:  -99.6250399584171
loss:  -100.05223609389283
loss:  -100.52077539038599
loss:  -101.08045005010887
loss:  -100.91014564298938
loss:  -101.57353034084693
loss:  -102.0732896048339
loss:  -102.43847028769586
loss:  -102.93754907985719
loss:  -103.43642267667536
loss:  -103.96107316236917
loss:  -104.46080519794356
loss:  -104.86076213135415
loss:  -105.33197752400362
loss:  -105.63615382426431
loss:  -106.27541567274841
loss:  -106.7751910374581
loss:  -107.34807267622153
loss:  -107.8485921917649
loss:  -108.34766867375035
loss:  -108.84721935388481
loss:  -109.64876755287038
loss:  -110.00781258116506
loss:  -110.34246532575315
loss:  -110.84107938920948
loss:  -111.19239645347383
loss:  -111.66236046890603
loss:  -111.78472102009982
loss:  -112.28291555856823
loss:  -112.66692035841426
loss:  -112.65696490637575
loss:  -113.16214103553114
loss:  -113.518719021743
loss:  -114.07932571157134
loss:  -114.5038656454846
loss:  -115.00342943860466
loss:  -115.50289829725892
loss:  -115.82184577936182
loss:  -116.12339048972457
loss:  -116.6228996340789
loss:  -116.96463076993426
loss:  -117.46394002645306
loss:  -117.96269235281696
loss:  -118.46077234085628
loss:  -118.9606911905314
loss:  -119.05137512390017
loss:  -119.55107778092894
loss:  -119.84064280609974
loss:  -120.161294189492
loss:  -120.66086371855351
loss:  -121.16007830848025
loss:  -121.64852061828458
loss:  -122.61985896029256
loss:  -123.10797296445591
loss:  -123.60512970540438
loss:  -124.1032701211764
loss:  -124.60285006380099
loss:  -125.10056015226397
loss:  -125.37826145693822
loss:  -125.79084017146951
loss:  -126.30492105832028
loss:  -126.80446048502097
loss:  -127.13187615127521
loss:  -127.58120276733615
loss:  -128.05010112726958
loss:  -128.54980872846727
loss:  -128.89020276710679
loss:  -129.38978391781836
loss:  -129.87268978701857
loss:  -130.37029604922498
loss:  -130.8677869040294
loss:  -131.3668770077277
loss:  -131.505462094874
loss:  -131.6196046906908
loss:  -132.10527862954385
loss:  -132.60418494536307
loss:  -133.10395372396735
loss:  -133.60379281775778
loss:  -134.11140401143172
loss:  -134.48442010673054
loss:  -134.9843884940247
loss:  -135.59580008577097
loss:  -135.75299744089486
loss:  -136.53947331470906
loss:  -137.02001063852012
loss:  -137.51957668091185
loss:  -138.01921791186112
loss:  -138.63897386296608
loss:  -139.0219875900511
loss:  -139.51566586347855
loss:  -139.856941801049
loss:  -140.34046741075525
loss:  -140.78894164513267
loss:  -141.27859765015816
loss:  -141.77794669236403
loss:  -142.27708107714216
loss:  -142.77250427432892
loss:  -143.170366746041
weights:  [1.00060817e+00 1.00008240e+00 1.00030898e+00 ... 5.52140717e-04
 7.41262573e-04 8.85882443e-04]
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: -143.170366746041
   maxcv: 7142.857142826513
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 300
  status: 4
 success: False
       x: array([1.00060817e+00, 1.00008240e+00, 1.00030898e+00, ...,
       5.52140717e-04, 7.41262573e-04, 8.85882443e-04])
Train data:
------------
Train accuracy :  0.37314285714285716
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 4051 (86%)
Protected in positive class: 1912 (83%)
P-rule is: 96%

Test data: 
------------
Test accuracy :  0.343
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 1864 (91%)
Protected in positive class: 812 (86%)
P-rule is: 95%
------------------------------------------------------------------------
------------------------------------------------------------------------
