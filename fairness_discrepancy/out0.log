iter:  100.0   lambda:  1e-08   alpha:  0.0   kernel: rbf, method:  cobyla x_init:  [1.58421131e-05 8.46499331e-04 8.32054085e-04 ... 2.17477247e-04
 1.04969509e-04 6.60110882e-04]
loss:  -3.4661789900877955
loss:  -3.959469488600704
loss:  -4.457978893925647
loss:  -4.94936710243399
loss:  -5.448831493978638
loss:  -5.946314448388348
loss:  -6.444635526966497
loss:  -6.943743265254001
loss:  -7.44554876575258
loss:  -7.945523325581345
loss:  -8.445560946573817
loss:  -8.944905450613678
loss:  -9.443380634611657
loss:  -9.943013940366132
loss:  -10.440966664351295
loss:  -10.852214449996861
loss:  -11.351511541628405
loss:  -11.85019842873273
loss:  -12.348451558020537
loss:  -12.836440994504288
loss:  -13.336233860056227
loss:  -13.835458613664647
loss:  -14.335316436595415
loss:  -14.835104596836194
loss:  -15.332843764469562
loss:  -15.829661174712063
loss:  -16.298106348058454
loss:  -16.793627065778132
loss:  -17.127975761283487
loss:  -17.61918143785868
loss:  -18.114356893453834
loss:  -18.61337267615718
loss:  -19.11224317849891
loss:  -19.524898281979713
loss:  -20.024700174771638
loss:  -20.524647270145284
loss:  -21.023867571101977
loss:  -21.484743439484376
loss:  -21.984181164151014
loss:  -22.47961758353468
loss:  -22.979417484583582
loss:  -23.478551923915262
loss:  -23.945190086380645
loss:  -24.44466701652298
loss:  -24.94382559719462
loss:  -25.44358346009946
loss:  -25.943514192906093
loss:  -26.442678918573506
loss:  -26.941188006171732
loss:  -27.440248578969687
loss:  -27.91897434123155
loss:  -28.417904207990166
loss:  -28.909800296645255
loss:  -29.402046479737322
loss:  -29.90166311627573
loss:  -30.401339452718794
loss:  -30.901106748664873
loss:  -31.38992084115144
loss:  -31.879318266112083
loss:  -32.3784866123227
loss:  -32.87662443049977
loss:  -33.37608357348755
loss:  -33.87517625170811
loss:  -34.375012759995215
loss:  -34.79139112328427
loss:  -35.2902085264398
loss:  -35.79005282796298
loss:  -36.290683405377884
loss:  -36.70623233309374
loss:  -37.195274601644606
loss:  -37.69453437012149
loss:  -38.18706662469877
loss:  -38.68484202668353
loss:  -39.184313640202596
loss:  -39.65678521532741
loss:  -40.15588945693992
loss:  -40.65498969732563
loss:  -41.19494242621566
loss:  -41.694259108433556
loss:  -42.19371244706812
loss:  -42.69447381091834
loss:  -43.19338908563453
loss:  -43.69728329883605
loss:  -44.19684620121429
loss:  -44.64632754101616
loss:  -45.143699833293034
loss:  -45.643618670526074
loss:  -46.14358938030099
loss:  -46.641633423579904
loss:  -47.135671869670624
loss:  -47.484633158417516
loss:  -47.77726641721785
loss:  -48.40558968628887
loss:  -48.91620594486453
loss:  -49.42652798322697
loss:  -49.90169377660851
loss:  -50.39685850313763
loss:  -50.89613677353085
loss:  -51.3471998405549
loss:  -51.83746532873674
weights:  [1.00001584e+00 1.00084650e+00 1.00083205e+00 ... 2.17477247e-04
 1.04969509e-04 6.60110882e-04]
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: -51.83746532873674
   maxcv: 7142.857142631552
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 100
  status: 4
 success: False
       x: array([1.00001584e+00, 1.00084650e+00, 1.00083205e+00, ...,
       2.17477247e-04, 1.04969509e-04, 6.60110882e-04])
Train data:
------------
Train accuracy :  0.4442857142857143
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 3604 (77%)
Protected in positive class: 1619 (70%)
P-rule is: 92%

Test data: 
------------
Test accuracy :  0.4156666666666667
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 1658 (81%)
Protected in positive class: 690 (73%)
P-rule is: 90%
------------------------------------------------------------------------
------------------------------------------------------------------------
