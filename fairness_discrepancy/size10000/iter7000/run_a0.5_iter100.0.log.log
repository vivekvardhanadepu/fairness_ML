iter:  100.0 , lambda:  1 , alpha:  0.5 , kernel: rbf method:  cobyla , catol:  0.001 , initiator:  1e-05
c_init:  [6.22300636e-06 1.69693936e-06 6.47108542e-06 ... 1.37858120e-06
 6.16715669e-06 5.39234585e-06]
loss:  1107.4723281545787
loss:  1016.9990991181561
loss:  997.1422037300046
loss:  1083.1590397483635
loss:  1008.8700068893557
loss:  1071.4952797543076
loss:  1062.1375372957123
loss:  997.5676820453389
loss:  1079.6166978763692
loss:  998.9408343567405
loss:  1072.314572701908
loss:  1012.313807300484
loss:  1061.1026648112609
loss:  1072.1258482584942
loss:  1064.0914369333182
loss:  1078.8335609105163
loss:  1007.1276633434907
loss:  1048.482260231905
loss:  1069.9957497936496
loss:  1062.0652228528402
loss:  1073.9784678683982
loss:  1066.2018140634611
loss:  1045.2865674808645
loss:  998.1053670114004
loss:  999.7242163113174
loss:  1073.9512481280133
loss:  1011.7582492263335
loss:  1002.9891370136305
loss:  1080.141871342296
loss:  998.4180299512026
loss:  1078.7566924659618
loss:  1003.156395360892
loss:  1078.3760397441238
loss:  1007.2970408789319
loss:  1079.0894799430257
loss:  1071.4365251202203
loss:  1016.3609236054991
loss:  1083.4160331415349
loss:  1063.1134322295322
loss:  1077.0889133322673
loss:  1066.3529850263296
loss:  1072.5240820980227
loss:  1063.7367373745246
loss:  1080.3171492317065
loss:  1068.0858340922168
loss:  1078.3037731448492
loss:  1078.1966587982254
loss:  1077.0354753918882
loss:  1077.9406135367062
loss:  1010.4422202847693
loss:  1054.8056850229307
loss:  1060.3481585085467
loss:  996.9247192675978
loss:  1080.3670743841874
loss:  998.8406807312823
loss:  999.9315922302665
loss:  1075.6356748879452
loss:  1071.340258902783
loss:  1001.4915223009812
loss:  1066.868561419131
loss:  1065.0697692913466
loss:  1007.1163668409349
loss:  1077.8491323689864
loss:  1071.186386042816
loss:  1069.8512476749245
loss:  1069.81855974583
loss:  998.5747258117539
loss:  1078.8960339420669
loss:  1077.5066803295194
loss:  1001.1614074595221
loss:  995.986893958236
loss:  980.1849599163536
loss:  998.1172821443383
loss:  946.0729985538455
loss:  958.6410807942965
loss:  968.4902225205727
loss:  922.523271486894
loss:  990.0029396870492
loss:  921.3397426324476
loss:  903.2055019163728
loss:  916.1599419093992
loss:  910.9287103323356
loss:  954.115741097087
loss:  973.7589578552017
loss:  880.1418619480792
loss:  990.6489520943755
loss:  961.8686524733187
loss:  879.9406470209913
loss:  881.7604837896954
loss:  948.4033026107091
loss:  879.7615160313482
loss:  984.9317707671046
loss:  923.7560956840213
loss:  854.7226672313526
loss:  915.0808259150995
loss:  874.3637423316762
loss:  835.0516985426699
loss:  832.5270344767519
loss:  841.9883511416028
loss:  886.9497979514027
weights:  [1.00000622e+00 1.00000170e+00 6.47108542e-06 ... 1.37858120e-06
 6.16715669e-06 5.39234585e-06]
cy dot constraint : -7.017549771660925
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 832.5270344767519
   maxcv: 0.0
 message: 'Maximum number of function evaluations has been exceeded.'
    nfev: 100
  status: 2
 success: False
       x: array([1.00000622e+00, 1.00000170e+00, 6.47108542e-06, ...,
       1.37858120e-06, 6.16715669e-06, 5.39234585e-06])
(7000,)
(3000,)
[ True  True  True ...  True  True  True]
[False  True  True ...  True  True  True]
Train data:
------------
Train accuracy :  0.7601428571428571
Total data points: 7000
# non-protected examples: 4752
# protected examples: 2248
Non-protected in positive class: 937 (20%)
Protected in positive class: 283 (13%)
P-rule is: 64%

Test data: 
------------
Test accuracy :  0.7596666666666667
Total data points: 3000
# non-protected examples: 2034
# protected examples: 966
Non-protected in positive class: 411 (20%)
Protected in positive class: 119 (12%)
P-rule is: 61%
------------------------------------------------------------------------
------------------------------------------------------------------------
