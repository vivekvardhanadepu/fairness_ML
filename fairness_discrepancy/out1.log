iter:  200.0   lambda:  1e-08   alpha:  0.0   kernel: rbf, method:  cobyla x_init:  [1.75599818e-04 8.31058108e-04 9.03605041e-04 ... 3.85431656e-05
 6.23939451e-04 4.01044027e-04]
loss:  -3.5134771807452188
loss:  -4.005787410557499
loss:  -4.504304653924507
loss:  -4.9967714661950104
loss:  -5.49574899176717
loss:  -5.9942812291601815
loss:  -6.492452834884709
loss:  -6.991278383289978
loss:  -7.492550208988058
loss:  -7.991952601798904
loss:  -8.491795960315685
loss:  -8.991152537858227
loss:  -9.489622010868747
loss:  -9.988719062749258
loss:  -10.48713480494142
loss:  -10.899004010752723
loss:  -11.398408177296213
loss:  -11.897856225167518
loss:  -12.397055022743944
loss:  -12.88457938458212
loss:  -13.383924117916754
loss:  -13.883582775678612
loss:  -14.38277604945409
loss:  -14.88239136145632
loss:  -15.379687924556626
loss:  -15.875951890344153
loss:  -16.34396216630738
loss:  -16.83885694327918
loss:  -17.17403544063601
loss:  -17.665565484688816
loss:  -18.160559787802967
loss:  -18.65989040032472
loss:  -19.15924622319794
loss:  -19.572605128698235
loss:  -20.07164058421325
loss:  -20.57069179118923
loss:  -21.069923559872567
loss:  -21.5307461023723
loss:  -22.02977752974829
loss:  -22.525773979606
loss:  -23.0250884477794
loss:  -23.523897206645564
loss:  -23.991990714517335
loss:  -24.491535882840648
loss:  -24.990921790326116
loss:  -25.490367352619927
loss:  -25.989557597271737
loss:  -26.488938101907717
loss:  -26.987488844370663
loss:  -27.486723727104852
loss:  -27.96518069449109
loss:  -28.464727055053288
loss:  -28.957470923051613
loss:  -29.449871779162475
loss:  -29.949674993198798
loss:  -30.449389165739596
loss:  -30.948507965652745
loss:  -31.436807906385827
loss:  -31.927366864125716
loss:  -32.4272159304501
loss:  -32.926091631363896
loss:  -33.425247786459344
loss:  -33.92510309165351
loss:  -34.42447717784255
loss:  -34.8413689405031
loss:  -35.340849632987094
loss:  -35.84017369422035
loss:  -36.34111993985307
loss:  -36.75709245817484
loss:  -37.246064233008205
loss:  -37.74523378475651
loss:  -38.237723139631186
loss:  -38.73499460189924
loss:  -39.234640490797595
loss:  -39.706803537361054
loss:  -40.2063194395333
loss:  -40.70551606349148
loss:  -41.245057338090746
loss:  -41.74428041450245
loss:  -42.24401436010893
loss:  -42.74457874475793
loss:  -43.244415139538596
loss:  -43.747974420196385
loss:  -44.24733563590975
loss:  -44.69660240602513
loss:  -45.19370377316661
loss:  -45.69365510460511
loss:  -46.19366797670592
loss:  -46.69172523295137
loss:  -47.185607704614476
loss:  -47.535746233637255
loss:  -47.827982321863914
loss:  -48.4569858524681
loss:  -48.9680249684243
loss:  -49.478734166412885
loss:  -49.95410770087213
loss:  -50.450071730425435
loss:  -50.94921157811195
loss:  -51.39996559795607
loss:  -51.89078384596717
loss:  -52.38972020950373
loss:  -52.886332454855136
loss:  -53.332586553996876
loss:  -53.8324004899829
loss:  -54.33197565846801
loss:  -54.800715197564145
loss:  -55.28741333348436
loss:  -55.78706730048944
loss:  -56.37639564300837
loss:  -56.87646621364558
loss:  -57.37577347923186
loss:  -57.870736700726354
loss:  -58.369873876384766
loss:  -58.81910469760144
loss:  -59.31859376603506
loss:  -59.8182148257072
loss:  -60.293242094616566
loss:  -60.788331155740664
loss:  -61.28645652933875
loss:  -61.69124174835956
loss:  -62.189982596642196
loss:  -62.68694491857846
loss:  -63.1859874022441
loss:  -63.6855963926492
loss:  -64.18156803526
loss:  -64.70541525437281
loss:  -65.20530097114163
loss:  -65.56288244071969
loss:  -66.03468558829103
loss:  -66.5337011346955
loss:  -67.02814969308051
loss:  -67.52002532705431
loss:  -67.78694915514086
loss:  -68.28640614340456
loss:  -68.7860301444852
loss:  -69.28526134687304
loss:  -69.78782341628795
loss:  -70.28728882358867
loss:  -70.7864244321248
loss:  -71.27156292295909
loss:  -71.65800085643802
loss:  -72.2194562853938
loss:  -72.70975472839676
loss:  -73.14757343665917
loss:  -73.64659903441293
loss:  -74.12470696806415
loss:  -74.6242145691133
loss:  -75.11984657319842
loss:  -75.61867491509828
loss:  -76.07994543531315
loss:  -76.57934308055884
loss:  -77.08489418264618
loss:  -77.58448645132346
loss:  -77.9434894892915
loss:  -78.44321683775507
loss:  -78.94256571430839
loss:  -79.44081085407534
loss:  -79.93949145152561
loss:  -80.41572683659692
loss:  -80.91505453733305
loss:  -81.25799949477133
loss:  -81.67784273886124
loss:  -83.34213199658393
loss:  -83.79101863345639
loss:  -84.28507646907157
loss:  -84.77749524279929
loss:  -85.24603199081943
loss:  -85.74511337367562
loss:  -86.15175100597129
loss:  -86.64631218544109
loss:  -87.11447409341291
loss:  -87.6006220209962
loss:  -88.00351185588978
loss:  -88.50304045198878
loss:  -89.08465761883265
loss:  -89.4876057539029
loss:  -89.98726237964637
loss:  -90.51515936352517
loss:  -90.98408806062041
loss:  -91.54092811697346
loss:  -91.82012095864283
loss:  -92.13364958749919
loss:  -92.1972432679595
loss:  -92.69670027748465
loss:  -92.85292259824786
loss:  -93.35261657049973
loss:  -93.8215125914938
loss:  -94.20851447225911
loss:  -94.7385589671666
loss:  -95.20121409524879
loss:  -95.58892250159921
loss:  -95.47442800221773
loss:  -95.81931021374156
loss:  -96.31846779183093
loss:  -96.79714668162478
loss:  -97.0880828977258
loss:  -97.53570831378875
loss:  -98.0350370163489
loss:  -98.53414125948113
loss:  -98.37318337363266
weights:  [1.00017560e+00 1.00083106e+00 1.00090361e+00 ... 3.85431656e-05
 6.23939451e-04 4.01044027e-04]
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: -98.53414125948113
   maxcv: 7142.85714229263
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 200
  status: 4
 success: False
       x: array([1.00017560e+00, 1.00083106e+00, 1.00090361e+00, ...,
       3.85431656e-05, 6.23939451e-04, 4.01044027e-04])
Train data:
------------
Train accuracy :  0.3412857142857143
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 4206 (90%)
Protected in positive class: 1990 (86%)
P-rule is: 97%

Test data: 
------------
Test accuracy :  0.33
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 1887 (92%)
Protected in positive class: 828 (88%)
P-rule is: 95%
------------------------------------------------------------------------
------------------------------------------------------------------------
