Looking for file 'adult.data' in the current directory...
File found in current directory..
Looking for file 'adult.test' in the current directory...
File found in current directory..
Loading only 10000 examples from the data
Total data points: 10000
# non-protected examples: 6754
# protected examples: 3246
Non-protected in positive class: 2080 (31%)
Protected in positive class: 372 (11%)
P-rule is: 37%
iter:  100   lambda:  1   alpha:  0.5   kernel: rbf, method:  cobyla catol:  0.1
x_init:  [0.62230064 0.16969394 0.64710854 ... 0.13785812 0.61671567 0.53923458]
loss:  600.9906395967648
loss:  604.149618734217
loss:  601.0620305358559
loss:  604.9354081624792
loss:  601.2646788334732
loss:  601.4522131098436
loss:  601.4530993534534
loss:  600.8423617422366
loss:  600.8952026019567
loss:  600.9662798123989
loss:  600.6419840599012
loss:  600.7435711799869
loss:  600.9684101467585
loss:  600.8170883038672
loss:  601.6584931379755
loss:  603.3197380265935
loss:  600.675182017701
loss:  600.8112948995652
loss:  601.0396442777426
loss:  600.9482425506869
loss:  600.5703044051984
loss:  600.8388126119064
loss:  600.6912528195578
loss:  600.4231128513711
loss:  601.6857220315592
loss:  601.2276343728033
loss:  599.661552190001
loss:  601.4267951137642
loss:  599.4114795651706
loss:  599.9767554786918
loss:  599.552157613165
loss:  599.1966490912266
loss:  599.3578967413671
loss:  601.5137258528177
loss:  599.1281833886148
loss:  599.3227786539342
loss:  598.9607093280517
loss:  600.2233746949682
loss:  598.7531190470004
loss:  599.7357655804162
loss:  598.6301535000077
loss:  598.7176375986908
loss:  599.644045605877
loss:  598.5247388168948
loss:  598.5418068848742
loss:  598.3287391206043
loss:  598.1708044729382
loss:  598.0849041337779
loss:  598.3560717004236
loss:  598.2314173650718
loss:  599.2137445516864
loss:  597.871846285831
loss:  599.8888085735928
loss:  598.1476117958207
loss:  597.6486232970981
loss:  597.8438463006848
loss:  597.4709500931342
loss:  597.3832021306828
loss:  598.084365301155
loss:  597.4291210211397
loss:  598.1623422248688
loss:  597.2343729901199
loss:  597.042415084418
loss:  596.8257803456154
loss:  597.2285554364487
loss:  596.939144476202
loss:  596.6636962492469
loss:  595.8310425498892
loss:  596.6867981339267
loss:  596.0771189965812
loss:  595.6466939673765
loss:  596.0680570630827
loss:  596.2644096007548
loss:  595.8430876235791
loss:  596.233036034442
loss:  595.7006281425697
loss:  595.4911491599337
loss:  595.0048750716301
loss:  595.0464124462817
loss:  594.7799489523541
loss:  594.8022952012691
loss:  595.0392356091313
loss:  594.9033660364075
loss:  594.8138562863355
loss:  595.5343477637678
loss:  595.2041729092352
loss:  594.5934940665827
loss:  594.3530877748144
loss:  594.4846106034936
loss:  594.391583533021
loss:  597.883893551199
loss:  595.2737042151152
loss:  592.9304461214695
loss:  592.0562591065882
loss:  592.1047136731811
loss:  591.6190733254657
loss:  592.3519334022928
loss:  591.4204917831296
loss:  591.6843197550447
loss:  592.5871738982212
weights:  [0.04612109 0.15463346 0.21993282 ... 0.05615202 0.71775257 0.30534783]
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 591.4204917831296
   maxcv: 1817.539396043331
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 100
  status: 4
 success: False
       x: array([0.04612109, 0.15463346, 0.21993282, ..., 0.05615202, 0.71775257,
       0.30534783])
Train data:
------------
Train accuracy :  0.29614285714285715
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 4422 (94%)
Protected in positive class: 2186 (95%)
P-rule is: 101%

Test data: 
------------
Test accuracy :  0.29233333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 1969 (96%)
Protected in positive class: 903 (96%)
P-rule is: 100%
