iter:  300.0 , lambda:  1 , alpha:  0.4444444444444444 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [4.67701263e-04 1.89609054e-04 8.15879084e-04 ... 7.79847018e-04
 7.80205524e-04 6.52741816e-05]
loss:  877.3709801083291
loss:  933.422739268367
loss:  932.1388655058526
loss:  935.3908525576842
loss:  925.5733356601519
loss:  930.5190477049935
loss:  939.2882907951499
loss:  940.3295181390239
loss:  910.9914545865817
loss:  878.1523390496444
loss:  939.0759284694877
loss:  878.4473503853379
loss:  940.0592018519092
loss:  926.5869785027178
loss:  920.3305969140871
loss:  934.9507912783539
loss:  915.2861336763377
loss:  939.9195347615768
loss:  936.6073104615288
loss:  941.071667864101
loss:  937.0161971251583
loss:  939.0419754273844
loss:  879.3564114922702
loss:  876.967765605981
loss:  937.4372880331362
loss:  939.1565085882831
loss:  911.1511182724945
loss:  934.3843566901826
loss:  910.4832038613321
loss:  938.9084518613755
loss:  924.7438092901549
loss:  926.7598574700276
loss:  933.7618615120394
loss:  929.5902734441294
loss:  924.5772106702094
loss:  885.1993429243703
loss:  878.0229014921243
loss:  930.2173473747611
loss:  903.3746016170552
loss:  937.7188477584488
loss:  883.8127062635249
loss:  938.540030601732
loss:  919.4123932477286
loss:  926.974834756407
loss:  942.188599959368
loss:  939.0851455453911
loss:  935.9018365679666
loss:  929.0036177511375
loss:  935.9495585794748
loss:  917.2444380422702
loss:  935.2591558771645
loss:  939.4638677515162
loss:  935.7343100407575
loss:  936.3298199770095
loss:  920.6492382582296
loss:  903.4682427307283
loss:  878.7256832903305
loss:  933.889594354898
loss:  938.0760331467188
loss:  940.2796166296059
loss:  910.8108673268912
loss:  881.0117931877533
loss:  877.2754918995229
loss:  934.0802893731153
loss:  935.226163468907
loss:  909.9059502181101
loss:  877.5467498989909
loss:  938.9896737683189
loss:  939.0690258508629
loss:  939.227131046043
loss:  934.3510783617813
loss:  939.0514621301897
loss:  927.5148974655926
loss:  934.6697991439107
loss:  935.2898581435549
loss:  937.8893515284103
loss:  880.351708225828
loss:  920.5109312449391
loss:  940.7313655741091
loss:  933.8680290341707
loss:  938.4757902594135
loss:  936.8713474876406
loss:  917.7012369427151
loss:  885.0174505759217
loss:  939.0164697161724
loss:  939.1158811516157
loss:  878.4191916376336
loss:  938.0200437550831
loss:  938.2437208691591
loss:  937.2975914681889
loss:  933.5061563889874
loss:  939.090452403677
loss:  915.3594706563699
loss:  919.9349639197063
loss:  939.6949194060991
loss:  917.7089010978015
loss:  935.3648768913545
loss:  935.4491142615838
loss:  938.2828442524996
loss:  938.7967344490115
loss:  939.1027318417351
loss:  925.8745358796272
loss:  938.5798490635368
loss:  903.2369323081239
loss:  930.6233195257499
loss:  939.1177556720204
loss:  918.2986457386177
loss:  939.110730167434
loss:  938.8833792925684
loss:  941.7446698274873
loss:  933.992054551403
loss:  937.630560764956
loss:  885.1850986888368
loss:  935.2463414821873
loss:  934.2506884596405
loss:  933.200150348765
loss:  938.3933847869822
loss:  938.5273386591386
loss:  932.987387599866
loss:  939.0104904861411
loss:  930.6645620024442
loss:  929.1316464761985
loss:  933.0037543002051
loss:  910.8476309432501
loss:  914.6331298257367
loss:  937.5473764625345
loss:  941.7115015123176
loss:  939.0464035852311
loss:  936.2270264942312
loss:  928.2684431813121
loss:  932.9194850306139
loss:  924.6931288182146
loss:  925.2842586732799
loss:  927.9014457430694
loss:  942.216220461564
loss:  886.8034478370176
loss:  939.105658289442
loss:  933.3744429663872
loss:  887.5524909284296
loss:  936.8300555080277
loss:  934.0041511161179
loss:  916.866839611311
loss:  931.4593352240588
loss:  937.7238774239116
loss:  938.061692965839
loss:  932.8224827919341
loss:  933.968571941408
loss:  931.3388570368355
loss:  939.3407755138583
loss:  938.4660708493988
loss:  929.5534689170705
loss:  920.8550602536374
loss:  941.2217399140693
loss:  933.7516945705381
loss:  915.706515345999
loss:  932.5855203974359
loss:  935.9690388896244
loss:  933.9524815255206
loss:  936.5725365714792
loss:  933.9770947635745
loss:  931.3859591707952
loss:  938.4373555757038
loss:  937.9199111263935
loss:  925.2805587893635
loss:  918.098875525087
loss:  938.8814068495325
loss:  935.7014030357024
loss:  881.4987272222248
loss:  937.6788973417798
loss:  927.9539798214447
loss:  912.8813903587636
loss:  939.88093623357
loss:  935.6580132130346
loss:  940.0959327609887
loss:  920.3828645650153
loss:  937.7017782587104
loss:  900.1211730286037
loss:  936.5451133410588
loss:  935.3426451657041
loss:  917.9981430873275
loss:  937.6402230260529
loss:  918.8762865102406
loss:  931.3754850311893
loss:  932.4433309681931
loss:  918.8696222344034
loss:  938.7256439404738
loss:  934.9424854814158
loss:  936.0427830815693
loss:  918.6678790310092
loss:  928.0346082970789
loss:  935.3251066638544
loss:  933.0399330159228
loss:  933.9993927203931
loss:  934.391426527175
loss:  919.661506927582
loss:  910.4800788535349
loss:  937.7197887892283
loss:  878.0894446940239
loss:  939.0242997523093
loss:  934.4656173537157
loss:  932.5595772354883
loss:  929.31068361669
loss:  932.9938632627582
loss:  936.8353491705232
loss:  917.7360119869012
loss:  918.3361816663341
loss:  919.4100761550465
loss:  938.8328612406942
loss:  937.0875899865787
loss:  930.8017092194492
loss:  927.8120924558784
loss:  934.2684894669361
loss:  920.5106562492145
loss:  886.4598745277968
loss:  939.0692584372594
loss:  935.3134343790437
loss:  938.881303964734
loss:  920.8416282007281
loss:  878.2768209589644
loss:  939.2910181775267
loss:  914.8927214809729
loss:  922.7535112459032
loss:  881.1864448070501
loss:  937.9105878143631
loss:  924.6963056232249
loss:  938.9059611122681
loss:  911.4784290743195
loss:  935.938181862125
loss:  934.8825401505493
loss:  934.0080121548907
loss:  938.277516932046
loss:  938.8700849516963
loss:  934.4742172406762
loss:  909.3267344129265
loss:  932.9283988836652
loss:  934.8736251925349
loss:  934.4587923144665
loss:  887.4122361791291
loss:  905.5701693517399
loss:  930.7989133230712
loss:  930.7958025573512
loss:  940.3732264885265
loss:  938.401216372968
loss:  936.5590232433595
loss:  922.2892305358879
loss:  921.4010938677388
loss:  936.328971101619
loss:  925.868289194094
loss:  940.3374847463318
loss:  933.5167449849854
loss:  938.7432740625213
loss:  940.2732602641191
loss:  942.6604651682537
loss:  930.5236381285648
loss:  914.2714390595669
loss:  942.6124041640251
loss:  916.5022900548543
loss:  920.9607097559215
loss:  936.5077948404162
loss:  910.6127269887038
loss:  937.1585840361129
loss:  936.9679890637783
loss:  920.8032388934861
loss:  941.7345692675898
loss:  938.8730134982686
loss:  938.8821704883252
loss:  911.1905366475663
loss:  876.7989522782105
loss:  937.7938210907236
loss:  881.3521069013348
loss:  935.3332415383304
loss:  938.1350433746511
loss:  940.1073711920698
loss:  939.5231271278511
loss:  920.958485176364
loss:  921.0154682957987
loss:  938.0920503158206
loss:  883.7220710584376
loss:  883.5465559677432
loss:  923.1373615655814
loss:  939.8070555718622
loss:  936.8635427620824
loss:  906.5328566229744
loss:  913.274390825985
loss:  937.5093174614095
loss:  920.3502783899409
loss:  930.2738377662288
loss:  883.4706736741359
loss:  915.3282156566409
loss:  921.0212056519441
loss:  937.9071810638596
loss:  919.9072366120278
loss:  933.7428878889407
loss:  935.238564127613
loss:  933.109515036176
loss:  932.9308501736332
loss:  877.0620575607686
loss:  935.6875534155397
loss:  913.2149633009846
loss:  932.8240646369445
weights:  [4.67701263e-04 1.89609054e-04 8.15879084e-04 ... 7.79847018e-04
 7.80205524e-04 6.52741816e-05]
cy dot constraint : -3.766596834863476
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 876.7989522782105
   maxcv: 3.766696834863476
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 300
  status: 4
 success: False
       x: array([4.67701263e-04, 1.89609054e-04, 8.15879084e-04, ...,
       7.79847018e-04, 7.80205524e-04, 6.52741816e-05])
Train data:
------------
Train accuracy :  0.7764285714285715
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 687 (15%)
Protected in positive class: 211 (9%)
P-rule is: 63%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
