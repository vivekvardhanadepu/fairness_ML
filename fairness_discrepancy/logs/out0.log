iter:  100.0 , lambda:  1 , alpha:  0.0 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.0006223  0.00016969 0.00064711 ... 0.00013786 0.00061672 0.00053923]
loss:  -1.839787484517794
loss:  -0.5462608312999464
loss:  -0.5571871542839117
loss:  -0.5338862482305644
loss:  -0.5774047428821554
loss:  -0.5693284935827623
loss:  -0.6110204263702657
loss:  -0.5453754821243244
loss:  -4.044670335347526
loss:  -4.540791087518947
loss:  -4.188983083790458
loss:  -5.044248958109546
loss:  -4.732892104508627
loss:  -4.646267110212118
loss:  -4.637053620262574
loss:  -4.637090692918083
loss:  -6.303969244212732
loss:  -7.032831765208318
loss:  -6.880135296228875
loss:  -6.671272658204622
loss:  -6.632681977257212
loss:  -6.674153024039634
loss:  -7.526152043972465
loss:  -8.021670714566538
loss:  -7.647099286996654
loss:  -7.735526245777724
loss:  -9.235304327783394
loss:  -9.735161792238
loss:  -10.841527546671651
loss:  -11.631126596767874
loss:  -11.08434000186363
loss:  -11.504049297163444
loss:  -11.260903736622996
loss:  -11.013187463939378
loss:  -11.636329051227152
loss:  -12.110408409179996
loss:  -11.7266245979222
loss:  -10.731453996267327
loss:  -14.114923790901123
loss:  -13.743413427271687
loss:  -14.579680625183567
loss:  -14.256514963500287
loss:  -13.971829345541082
loss:  -14.14903447943517
loss:  -14.532435870366456
loss:  -14.773376754670197
loss:  -13.558326211558626
loss:  -13.49465768726159
loss:  -13.55757242735022
loss:  -13.540746378061593
loss:  -13.511243454900455
loss:  -13.558190949382853
loss:  -13.512650990095883
loss:  -13.84220874771705
loss:  -16.94500933646295
loss:  -17.923864994149877
loss:  -18.416581909451708
loss:  -19.15667665748959
loss:  -18.882378390951093
loss:  -18.908507128441034
loss:  -18.654158679766375
loss:  -19.63177248201293
loss:  -19.60660707620007
loss:  -19.48863673443195
loss:  -19.7428632447117
loss:  -21.814587780748603
loss:  -22.30534109506178
loss:  -22.137279809732863
loss:  -22.13642758206275
loss:  -22.180726270285152
loss:  -22.03323461350531
loss:  -22.149791869177605
loss:  -21.933356636774786
loss:  -22.29542909062674
loss:  -22.03633815926871
loss:  -22.0856009676114
loss:  -23.351167446128397
loss:  -24.575354795893958
loss:  -25.61648332011073
loss:  -25.600905922282017
loss:  -25.595862676586805
loss:  -25.545194132884774
loss:  -26.680671891893482
loss:  -26.160515638220254
loss:  -27.63107801650717
loss:  -27.561693382618223
loss:  -28.13324942729299
loss:  -28.15258708206118
loss:  -27.114164820044834
loss:  -27.123785777807345
loss:  -27.160430778065336
loss:  -27.118226392169902
loss:  -30.181588948743748
loss:  -31.227181247941097
loss:  -32.171339290513274
loss:  -33.1783178203645
loss:  -34.187351996273456
loss:  -34.230876044787884
loss:  -33.299305926185525
loss:  -33.28933480383598
weights:  [0.0006223  0.00016969 0.00064711 ... 0.00013786 0.00061672 0.00053923]
cy dot constraint : -0.807713144045083
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: -34.230876044787884
   maxcv: 1.0009980687280349
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 100
  status: 4
 success: False
       x: array([0.0006223 , 0.00016969, 0.00064711, ..., 0.00013786, 0.00061672,
       0.00053923])
Train data:
------------
Train accuracy :  0.7795714285714286
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 613 (13%)
Protected in positive class: 191 (8%)
P-rule is: 64%

Test data: 
------------
Test accuracy :  0.7746666666666666
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 277 (13%)
Protected in positive class: 90 (10%)
P-rule is: 71%
------------------------------------------------------------------------
------------------------------------------------------------------------
