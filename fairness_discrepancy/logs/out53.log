iter:  400.0 , lambda:  1 , alpha:  0.5555555555555556 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00081767 0.000376   0.00081466 ... 0.00067701 0.00086157 0.00033317]
loss:  1097.8616548841608
loss:  1169.9714174116998
loss:  1168.3590026163884
loss:  1172.4436255845178
loss:  1160.138906820456
loss:  1166.3259064649562
loss:  1177.3874737574563
loss:  1178.635835792358
loss:  1135.8074733043163
loss:  1098.9678202143073
loss:  1177.0807675553006
loss:  1099.2519086799055
loss:  1178.3281336378789
loss:  1161.4309198022022
loss:  1153.5842989889804
loss:  1171.8903680020062
loss:  1141.1607689972666
loss:  1178.1675332755078
loss:  1174.0469625475341
loss:  1179.5665079027178
loss:  1174.4677768405643
loss:  1177.037062540938
loss:  1100.5232988034322
loss:  1097.4622541891802
loss:  1175.135092835539
loss:  1177.3192001421194
loss:  1136.1578928631359
loss:  1171.2957072102135
loss:  1135.3261874412374
loss:  1177.015475680983
loss:  1159.2301952859555
loss:  1161.819954585517
loss:  1170.5522261767462
loss:  1165.2743434097247
loss:  1159.0969128575625
loss:  1108.3906251222083
loss:  1098.8262027473115
loss:  1166.061384940464
loss:  1126.4574568174396
loss:  1175.4895505738643
loss:  1106.2853245832068
loss:  1176.526938239182
loss:  1152.5455147450414
loss:  1162.058962610511
loss:  1181.115370148142
loss:  1177.2199668445255
loss:  1173.222145246989
loss:  1164.5625151306106
loss:  1173.281513796106
loss:  1149.8384814056797
loss:  1172.3951644797462
loss:  1177.661530381932
loss:  1172.9918731472278
loss:  1173.8081464501201
loss:  1148.0293128605658
loss:  1126.5770155891325
loss:  1099.6302619188166
loss:  1170.714580747406
loss:  1175.938531717014
loss:  1178.680127066478
loss:  1141.774631109413
loss:  1102.780087109133
loss:  1097.8579636896418
loss:  1170.9726361332002
loss:  1172.4301559336907
loss:  1134.624668437645
loss:  1098.2268757266763
loss:  1177.0964109424124
loss:  1177.1982579250327
loss:  1177.407374682026
loss:  1171.282252769137
loss:  1177.1787909971695
loss:  1162.692412873546
loss:  1171.7320115474804
loss:  1172.4336021478418
loss:  1175.6898775525628
loss:  1101.9417216324887
loss:  1147.8719031206474
loss:  1179.2762446990591
loss:  1170.6090427495153
loss:  1176.4784178576967
loss:  1174.4565755262718
loss:  1144.3357988004768
loss:  1108.1608254454584
loss:  1177.1484375836362
loss:  1177.2635615544418
loss:  1099.2794698866546
loss:  1175.9212748529517
loss:  1176.166060029911
loss:  1174.9680381916344
loss:  1170.19174883016
loss:  1177.2388567569221
loss:  1141.4064764619118
loss:  1147.1425546925677
loss:  1178.0126751696562
loss:  1144.3453259385176
loss:  1172.5264048983381
loss:  1172.7004805903084
loss:  1176.1998890868208
loss:  1176.8454907364442
loss:  1177.2041330535083
loss:  1160.6319518272792
loss:  1176.5768033948855
loss:  1126.2848371819932
loss:  1166.5803474941367
loss:  1177.261639776609
loss:  1151.1461143206707
loss:  1177.2280257742832
loss:  1176.9857011497036
loss:  1180.5509287800503
loss:  1170.7877722561145
loss:  1175.3794109613173
loss:  1108.373347637402
loss:  1172.3774843221586
loss:  1171.1072701710525
loss:  1169.890486687764
loss:  1176.340917586786
loss:  1176.4927285723495
loss:  1169.539021188178
loss:  1177.1410381421115
loss:  1166.6350971453742
loss:  1164.7091890159527
loss:  1169.6239720936076
loss:  1135.8226350652862
loss:  1146.5560687766028
loss:  1175.326040944057
loss:  1180.5184739972028
loss:  1177.1725517204757
loss:  1173.6103220182583
loss:  1163.545629278355
loss:  1169.4555115027133
loss:  1159.148702033962
loss:  1159.89106284978
loss:  1163.1955285309868
loss:  1181.1484741434863
loss:  1110.4052547199651
loss:  1177.2467006653585
loss:  1170.0001204771313
loss:  1111.348068088666
loss:  1174.3701890778136
loss:  1170.8172222705011
loss:  1143.2907889929297
loss:  1167.6203266685084
loss:  1175.4958863989968
loss:  1175.8873324116892
loss:  1169.3327589929333
loss:  1170.8371567008207
loss:  1167.463974967935
loss:  1177.5259601943303
loss:  1176.4680922850425
loss:  1165.2821666382358
loss:  1148.3132574450126
loss:  1179.8931206845298
loss:  1170.5395497137492
loss:  1141.84524950893
loss:  1169.128117306071
loss:  1173.2687431407112
loss:  1170.7443604784614
loss:  1174.044828819705
loss:  1170.8139480382633
loss:  1167.5289646570825
loss:  1176.3949857374912
loss:  1175.789761462894
loss:  1159.8862531365025
loss:  1150.8980853731462
loss:  1176.9587554688521
loss:  1172.9686806489076
loss:  1103.3532664869206
loss:  1175.4398360484545
loss:  1163.2307561369535
loss:  1144.37121090149
loss:  1178.2161278549706
loss:  1172.896760386072
loss:  1178.5065003148802
loss:  1147.708211424276
loss:  1175.4680099635023
loss:  1122.4199817764213
loss:  1174.010981829192
loss:  1172.4995001540196
loss:  1144.7104613920374
loss:  1175.3913959516276
loss:  1151.8717694011752
loss:  1167.5159680729691
loss:  1168.9498813996681
loss:  1151.8633362759967
loss:  1176.7831535133714
loss:  1171.995799473612
loss:  1173.3805649287913
loss:  1145.5503624824394
loss:  1163.3449971897332
loss:  1172.477651766448
loss:  1169.605027911303
loss:  1170.8112537080126
loss:  1171.2763875234014
loss:  1152.851836116342
loss:  1135.3224991199133
loss:  1175.4905971960784
loss:  1098.7539154531325
loss:  1177.151730397116
loss:  1171.3971481330193
loss:  1169.0016761836248
loss:  1165.0273402611338
loss:  1169.5483327805432
loss:  1174.3768060812367
loss:  1150.4561899502871
loss:  1145.1338046569815
loss:  1152.5425841866647
loss:  1176.8951238296488
loss:  1174.663120094105
loss:  1166.7951892838314
loss:  1163.0715462123183
loss:  1171.139298408083
loss:  1147.8763294424025
loss:  1108.5373829685368
loss:  1177.1957643144563
loss:  1172.4631425410043
loss:  1176.9583831322216
loss:  1148.3202876171724
loss:  1099.1432787754595
loss:  1177.4824898894199
loss:  1140.8265937472854
loss:  1156.8140210483955
loss:  1102.3665902316386
loss:  1175.7781773553857
loss:  1159.1525423810267
loss:  1177.012589054357
loss:  1136.565366796535
loss:  1173.2674048147496
loss:  1172.0006215194821
loss:  1170.822052900419
loss:  1176.2255507891564
loss:  1176.9693470551085
loss:  1171.407797798227
loss:  1139.9082412359855
loss:  1169.4665814234463
loss:  1171.9102098103215
loss:  1171.3888017694685
loss:  1111.1706226700258
loss:  1129.2266961695395
loss:  1166.7917025448821
loss:  1166.7878617194253
loss:  1178.8192403361631
loss:  1176.3506611136784
loss:  1174.0000744039166
loss:  1156.143123931652
loss:  1155.044541534289
loss:  1173.8051040598978
loss:  1160.6243895774817
loss:  1178.7513423429452
loss:  1170.2050901324562
loss:  1176.7834739320808
loss:  1178.7332140514616
loss:  1181.6835206178994
loss:  1166.497769208453
loss:  1140.0477829994481
loss:  1181.6229156333195
loss:  1148.9021127601304
loss:  1154.4747962456179
loss:  1173.9949631067152
loss:  1135.5326001395956
loss:  1174.7836165325716
loss:  1174.5397225488366
loss:  1148.2489726703172
loss:  1180.5153685812759
loss:  1176.9482307371245
loss:  1176.9829120953723
loss:  1136.204420900944
loss:  1097.3320119099092
loss:  1175.7025953812135
loss:  1103.2505544445696
loss:  1172.5991834778897
loss:  1176.099997987283
loss:  1178.6117168792036
loss:  1177.8770671973382
loss:  1148.5980413758377
loss:  1148.6733872094267
loss:  1176.0917735742048
loss:  1106.252269875594
loss:  1106.0335409221375
loss:  1151.3310782991173
loss:  1178.2248089978018
loss:  1174.526411512971
loss:  1130.599125933182
loss:  1138.9565812955266
loss:  1175.3419891656154
loss:  1147.8195079745972
loss:  1166.2413521284846
loss:  1105.9383995068886
loss:  1147.600463435777
loss:  1148.703539172516
loss:  1175.8493871934581
loss:  1153.27558100603
loss:  1170.5999888891743
loss:  1172.5015455549924
loss:  1169.793516552376
loss:  1169.621619285916
loss:  1097.7382925769646
loss:  1173.0089424906332
loss:  1138.8828573800179
loss:  1169.4452900258943
loss:  1178.464807378148
loss:  1140.157708984488
loss:  1150.6295902558313
loss:  1100.3847328222873
loss:  1125.2742948932823
loss:  1159.875982584529
loss:  1173.6458654084652
loss:  1148.162015131373
loss:  1174.5438269798296
loss:  1106.0899172324073
loss:  1174.1325434310156
loss:  1175.23887242061
loss:  1141.666039057755
loss:  1173.8359963146343
loss:  1174.860987828287
loss:  1172.7995390964297
loss:  1152.2897550223352
loss:  1174.5012269591955
loss:  1164.2377930653759
loss:  1099.9436677327653
loss:  1174.4946677753724
loss:  1158.667511506669
loss:  1176.5575433767444
loss:  1174.440393791986
loss:  1171.6917353958352
loss:  1177.9962193521462
loss:  1168.32983728244
loss:  1153.5504440801606
loss:  1154.995836400287
loss:  1126.1163208503244
loss:  1165.4166118263042
loss:  1098.9265442787532
loss:  1137.333993645092
loss:  1166.2846174753715
loss:  1141.801911174887
loss:  1178.0948441891007
loss:  1164.8306725610767
loss:  1144.6394282577107
loss:  1140.4115333541897
loss:  1161.308888812989
loss:  1172.7799701146678
loss:  1173.1160645986033
loss:  1163.1679899260982
loss:  1178.9948837530364
loss:  1172.236016257466
loss:  1152.646505518845
loss:  1177.3722608616313
loss:  1175.2068248598541
loss:  1178.9389830715636
loss:  1138.9901043213388
loss:  1176.557170296541
loss:  1163.9769031398666
loss:  1170.6877886187299
loss:  1156.4744070422569
loss:  1169.4392956792165
loss:  1165.3331648176552
loss:  1177.0244006608427
loss:  1176.896088144035
loss:  1168.4189621030698
loss:  1175.1295529859017
loss:  1171.949932439073
loss:  1175.7219942278325
loss:  1141.758171374824
loss:  1176.0419750288931
loss:  1175.6826399895128
loss:  1110.8170984172202
loss:  1140.3875877053842
loss:  1179.555913267115
loss:  1136.5226872824455
loss:  1162.957867168606
loss:  1175.21915897743
loss:  1175.2217016817078
loss:  1173.7775266441677
loss:  1136.3421774745543
loss:  1173.5382096194273
loss:  1176.2282159832087
loss:  1143.0385377639434
loss:  1174.6114642662142
loss:  1106.1632492524807
loss:  1166.8880302496357
loss:  1171.6907446355258
loss:  1170.1496196151256
loss:  1127.539747016057
loss:  1171.7875710125502
loss:  1099.3413721954857
loss:  1171.4030835990734
loss:  1165.5356053975045
loss:  1174.0073273307462
loss:  1171.3691198938168
loss:  1169.0409541069407
loss:  1103.076700630261
loss:  1166.9717876964676
loss:  1158.8176405742727
loss:  1178.0429161385678
loss:  1175.3355834353042
loss:  1101.3565816698933
loss:  1144.2975690997428
loss:  1168.330022064103
loss:  1173.9516692746706
loss:  1172.3676002233885
weights:  [0.00081767 0.000376   0.00081466 ... 0.00067701 0.00086157 0.00033317]
cy dot constraint : -3.7993331317802728
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 1097.3320119099092
   maxcv: 3.799433131780273
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 400
  status: 4
 success: False
       x: array([0.00081767, 0.000376  , 0.00081466, ..., 0.00067701, 0.00086157,
       0.00033317])
Train data:
------------
Train accuracy :  0.7764285714285715
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 687 (15%)
Protected in positive class: 211 (9%)
P-rule is: 63%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
