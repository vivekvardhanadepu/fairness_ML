iter:  400.0 , lambda:  1 , alpha:  0.2222222222222222 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00077749 0.00082428 0.00065273 ... 0.0004893  0.00020571 0.00058729]
loss:  438.5520955546552
loss:  469.34388703714274
loss:  468.6891454193261
loss:  470.34432050983696
loss:  465.388291538914
loss:  467.8653138585108
loss:  472.27806221543744
loss:  472.8123013207556
loss:  449.9949227928173
loss:  438.70391590034825
loss:  472.2046286577035
loss:  438.75423145692235
loss:  472.69648771700116
loss:  465.92404302276987
loss:  462.74733033232644
loss:  470.1203806717908
loss:  452.082785895283
loss:  472.61310008310534
loss:  470.8846143815487
loss:  473.1780104812548
loss:  471.12024094996946
loss:  472.18804297217633
loss:  439.3614264190275
loss:  438.08455338032286
loss:  471.1267659640846
loss:  471.97742296281194
loss:  449.8433254100207
loss:  469.5789587701072
loss:  449.5200787169372
loss:  471.84432506881797
loss:  464.7343603938506
loss:  465.70746967778297
loss:  469.28872120031747
loss:  467.14078421821426
loss:  464.5608850893833
loss:  442.34952681730056
loss:  438.27598967800304
loss:  467.4598509138863
loss:  446.00648040021224
loss:  471.2686202550349
loss:  441.4248623447727
loss:  471.6798763149975
loss:  462.0277122663547
loss:  465.8620343070746
loss:  473.4358202664992
loss:  471.724038665753
loss:  470.3625962461127
loss:  466.8838570837638
loss:  470.38742532373107
loss:  460.9314963369305
loss:  470.0228432429477
loss:  472.1098407154271
loss:  470.2653248649707
loss:  470.4993377854496
loss:  454.50558913136234
loss:  446.0726314386618
loss:  438.5780381476563
loss:  469.35151831955193
loss:  471.4476709866621
loss:  472.4956024846988
loss:  457.65225922673903
loss:  439.9958586074267
loss:  438.2657216937039
loss:  469.42896807665466
loss:  469.92588050885627
loss:  449.2025560215382
loss:  438.0406013029623
loss:  470.8131202146258
loss:  470.8545381034962
loss:  470.92939012039443
loss:  468.49320271005723
loss:  470.8437416378709
loss:  465.04357428487083
loss:  468.6084628406416
loss:  468.94945291199986
loss:  470.24950457107997
loss:  439.6053841094447
loss:  454.79007897473963
loss:  471.5853320981283
loss:  468.07904733311864
loss:  470.54374123539327
loss:  469.7529929431445
loss:  453.3978347878983
loss:  442.2989764318583
loss:  470.8219049090314
loss:  470.8759202817363
loss:  438.3756026091897
loss:  470.28770687738063
loss:  470.44245254395673
loss:  469.96738367229943
loss:  468.04838461709033
loss:  470.8603027565611
loss:  452.25903795948346
loss:  454.4985985218459
loss:  471.12382418828196
loss:  453.4014237470682
loss:  468.9871881110641
loss:  469.01866007209094
loss:  470.4593936810835
loss:  470.71713427352506
loss:  470.84114738048936
loss:  464.212733349579
loss:  470.6096958996777
loss:  446.337963841807
loss:  466.5982516478873
loss:  470.87853222061966
loss:  460.3969185227818
loss:  470.8650137794033
loss:  470.7516771712381
loss:  472.18106593041705
loss:  468.26885572511975
loss:  470.1307337950171
loss:  442.3857961894816
loss:  468.92668168721104
loss:  468.38611766822044
loss:  467.8840277576133
loss:  470.5161964363608
loss:  470.5692460204244
loss:  467.78461989331083
loss:  470.81887823792414
loss:  466.62354457048235
loss:  465.8486549432333
loss:  467.8108499178944
loss:  450.05245148675635
loss:  458.54678240748575
loss:  470.06932986854434
loss:  472.15450202614636
loss:  470.84127926554146
loss:  469.42276356477925
loss:  465.1344966824875
loss:  467.7503944903805
loss:  463.61424047979824
loss:  463.91387735369676
loss:  465.2492748880751
loss:  472.37570914960776
loss:  443.230383418005
loss:  470.87187786234256
loss:  467.7717805412475
loss:  443.613282182832
loss:  469.72816042708916
loss:  468.30000770835846
loss:  452.9907678111504
loss:  467.0116232549053
loss:  470.1789529971338
loss:  470.2396620990856
loss:  467.7017562413098
loss:  468.2912238834431
loss:  466.94185534739404
loss:  470.98487447617737
loss:  470.53927442712904
loss:  466.0831494239715
loss:  454.97228458830347
loss:  471.9194939501114
loss:  468.1930794439657
loss:  452.4020960379257
loss:  467.53357963117134
loss:  469.2619526230622
loss:  468.26182146755076
loss:  469.59704881666244
loss:  468.1136660712347
loss:  466.9751349673958
loss:  470.5373122702461
loss:  470.2583409353644
loss:  463.91200584135333
loss:  460.2975420101538
loss:  470.76078221030895
loss:  469.1692182934792
loss:  440.22065573247954
loss:  470.15553859758427
loss:  465.2521296246896
loss:  457.6649255337802
loss:  471.2586366681377
loss:  469.1350313087965
loss:  471.34970030873876
loss:  454.7243032631097
loss:  470.1677316450103
loss:  444.8166462862785
loss:  469.5825186917511
loss:  468.9767281681694
loss:  453.5511682771459
loss:  470.13546427812287
loss:  460.68963359590896
loss:  466.9699578108744
loss:  467.462387831408
loss:  460.68642654757167
loss:  470.6739671965465
loss:  468.77396683924786
loss:  469.32941977210766
loss:  453.87665196079763
loss:  465.3065982727092
loss:  468.9676280630548
loss:  467.8113900342168
loss:  468.2976537550535
loss:  468.42781165825625
loss:  461.08361516617003
loss:  449.9106274071943
loss:  470.1747747294514
loss:  438.1235264676356
loss:  470.82711486026744
loss:  468.53356684975944
loss:  467.568334206844
loss:  465.8823057203663
loss:  467.7892697501149
loss:  469.7307282086387
loss:  460.12046491648675
loss:  453.7132828814715
loss:  460.9605005301626
loss:  470.7363112371449
loss:  469.79283123744034
loss:  466.6792836308233
loss:  465.19641822184764
loss:  468.41674856888665
loss:  454.7990711779574
loss:  441.6964708226638
loss:  470.85462079068185
loss:  468.96199874670856
loss:  470.7589644195553
loss:  454.9984915764763
loss:  439.4737994759094
loss:  470.9635260382052
loss:  452.0049094169288
loss:  462.5964469778049
loss:  439.46538380868896
loss:  470.25359407847617
loss:  463.6159290735085
loss:  470.76502311798555
loss:  450.3292156311871
loss:  469.2885131808027
loss:  468.7069401390167
loss:  468.30199284930166
loss:  470.4441097831204
loss:  470.7449639251468
loss:  468.5377844263859
loss:  455.8622345168648
loss:  467.75484348375534
loss:  468.73814093780516
loss:  468.5299415534242
loss:  443.540274227921
loss:  447.46438232373987
loss:  466.67794360137236
loss:  466.6763361480913
loss:  471.50161070081816
loss:  470.52002964782827
loss:  469.52608209481406
loss:  462.40829724783674
loss:  461.9681535821168
loss:  469.44535452561144
loss:  464.20970903833864
loss:  471.4378333975391
loss:  468.05348726004206
loss:  470.69186599936467
loss:  471.4172791496099
loss:  472.6077885897524
loss:  466.57055339906657
loss:  451.7332561720248
loss:  472.5833538451879
loss:  459.49348968933015
loss:  461.73507404457393
loss:  469.57037259263745
loss:  449.94568991871205
loss:  469.8938673639673
loss:  469.7943739284358
loss:  454.9492639154284
loss:  472.159703727014
loss:  470.75681751818917
loss:  470.7527777041289
loss:  450.1903476545475
loss:  437.65971536828727
loss:  469.94578916710424
loss:  439.85054535165244
loss:  468.7061830303382
loss:  470.0916917073166
loss:  471.0918840844268
loss:  470.8049091623085
loss:  454.80523261959326
loss:  454.83826152969056
loss:  470.09059000944103
loss:  441.0931000434929
loss:  441.00471048524537
loss:  455.9073939404127
loss:  470.9471277065042
loss:  469.47827621991337
loss:  447.73253767992475
loss:  451.03157878675376
loss:  469.8030906825941
loss:  454.48482903769815
loss:  466.1460091955204
loss:  440.9659594215728
loss:  458.58889699030635
loss:  454.86995546492534
loss:  470.0004646635509
loss:  460.9464320976816
loss:  467.9012731878045
loss:  468.66846761692995
loss:  467.5655219878217
loss:  467.510826368351
loss:  437.49167478384106
loss:  468.1019411630383
loss:  451.09411082987634
loss:  466.7546125610454
loss:  470.3504643885919
loss:  451.53429149505234
loss:  459.20860626763425
loss:  438.32341413204256
loss:  446.00966830844436
loss:  462.9211851817471
loss:  468.4409085758313
loss:  454.7060847042711
loss:  468.79975920574213
loss:  440.884310234963
loss:  468.608129095514
loss:  469.0591016444053
loss:  452.16775853370433
loss:  468.51716300614277
loss:  468.92702029270123
loss:  468.1018316397075
loss:  459.87115698449907
loss:  468.77780204666
loss:  464.66146381811956
loss:  437.8945596386924
loss:  468.75509679934913
loss:  462.4343050454456
loss:  469.6024830936437
loss:  468.7371732880857
loss:  467.6578393669589
loss:  470.1750865514093
loss:  466.30769748625636
loss:  460.33909123694536
loss:  460.9631423616333
loss:  446.053611556293
loss:  465.1254392940719
loss:  437.7794441789711
loss:  450.50325571876687
loss:  465.4855385133306
loss:  455.659218884474
loss:  470.21367169233514
loss:  464.90732445338494
loss:  456.7893195275876
loss:  451.6813412068757
loss:  463.4969030645556
loss:  468.06022443005725
loss:  468.22896209662645
loss:  464.0912254969976
loss:  470.56936833441756
loss:  467.87619049649766
loss:  460.0198066184899
loss:  469.9226647813119
loss:  469.06298918041176
loss:  470.54959341173094
loss:  454.5277121722409
loss:  469.5729397147611
loss:  464.5574858515367
loss:  467.2544132373532
loss:  461.5323211875453
loss:  466.7514373907032
loss:  465.1028732106587
loss:  469.7848983340139
loss:  469.71470897126784
loss:  466.3492646455144
loss:  469.0339313168024
loss:  467.72168701720864
loss:  469.26939766488573
loss:  452.1672613188522
loss:  469.3979804456612
loss:  469.25376676901794
loss:  442.93575162841586
loss:  451.67224624938456
loss:  470.78419870779254
loss:  450.1134963431442
loss:  464.14705835606344
loss:  469.0512659390761
loss:  469.070615203746
loss:  468.4924296915225
loss:  450.0473658631739
loss:  468.39797893482637
loss:  469.46707955147275
loss:  452.6962279917375
loss:  468.77288125855387
loss:  440.91408031052964
loss:  465.72690869364493
loss:  467.61617560207037
loss:  467.038697299873
loss:  446.60544439915395
loss:  467.68961063102284
loss:  438.2523383176617
loss:  467.51224599138396
loss:  465.1727274337277
loss:  468.585331530468
loss:  467.5279135984279
loss:  466.57720306524755
loss:  439.62781800847813
loss:  465.7616809612008
loss:  462.4986451630545
loss:  470.1725957878702
loss:  469.11349237393415
loss:  438.8859537437579
loss:  453.17108220087886
loss:  466.3077606493509
loss:  468.563332766681
loss:  467.909357037205
weights:  [0.00077749 0.00082428 0.00065273 ... 0.0004893  0.00020571 0.00058729]
cy dot constraint : -5.833040530569606
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 437.49167478384106
   maxcv: 5.833140530569605
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 400
  status: 4
 success: False
       x: array([0.00077749, 0.00082428, 0.00065273, ..., 0.0004893 , 0.00020571,
       0.00058729])
Train data:
------------
Train accuracy :  0.7777142857142857
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 680 (14%)
Protected in positive class: 209 (9%)
P-rule is: 63%

Test data: 
------------
Test accuracy :  0.7733333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 303 (15%)
Protected in positive class: 98 (10%)
P-rule is: 70%
------------------------------------------------------------------------
------------------------------------------------------------------------
