iter:  200.0 , lambda:  1 , alpha:  0.1111111111111111 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00024848 0.00080908 0.00064105 ... 0.00023382 0.00018142 0.00033056]
loss:  218.02818898750834
loss:  233.421744001979
loss:  233.0890824641516
loss:  233.9288655887667
loss:  231.42282106149733
loss:  232.67095498296177
loss:  234.87953827335906
loss:  235.16671394247945
loss:  223.92639798460277
loss:  217.85295847254625
loss:  234.69597015486463
loss:  217.73324905450207
loss:  234.27222848896423
loss:  230.8696669485901
loss:  229.27006291979848
loss:  232.9814285429252
loss:  224.60924594147082
loss:  234.22615193625913
loss:  233.34520895938377
loss:  234.51454858922398
loss:  233.47979660361145
loss:  234.01980494081522
loss:  217.8755878637583
loss:  217.25538178202743
loss:  233.24313700370968
loss:  233.66445099282956
loss:  223.25255914792527
loss:  232.464911767881
loss:  223.09485307432905
loss:  233.59530712598738
loss:  230.02792348577447
loss:  230.502869131764
loss:  232.3170423016005
loss:  231.23792158970872
loss:  229.92093046005056
loss:  219.0628338536034
loss:  218.13272422902668
loss:  231.39860785048765
loss:  221.34053345411303
loss:  233.3143249684305
loss:  218.6741309248855
loss:  233.5193661351184
loss:  228.66492081068512
loss:  230.5906705317695
loss:  234.38809146611754
loss:  233.5117272246173
loss:  232.8582215410695
loss:  231.11038031484892
loss:  232.8711514464983
loss:  228.11186925399133
loss:  232.68815317192036
loss:  233.73373652412187
loss:  232.81074252753803
loss:  232.90975108549327
loss:  225.5479333267707
loss:  221.38412268996615
loss:  217.2929884064654
loss:  232.34790182049517
loss:  233.40401602384782
loss:  233.9246921062618
loss:  226.4602247096676
loss:  217.94206450818865
loss:  217.3528356200143
loss:  232.38181002891008
loss:  232.61884789513053
loss:  222.9088048884427
loss:  217.01333145196202
loss:  232.8508094147476
loss:  232.87179493155745
loss:  232.90632676062367
loss:  231.6872891888324
loss:  232.86517766689846
loss:  229.95396360300248
loss:  231.72783710319226
loss:  231.91758850882232
loss:  232.57001377656562
loss:  217.51700414882538
loss:  225.4176577412329
loss:  233.2174704506265
loss:  231.4567866105956
loss:  232.7081576025308
loss:  232.31611473609493
loss:  224.72994919085008
loss:  218.8180584547721
loss:  232.8510776922051
loss:  232.88094415068062
loss:  216.95710018219117
loss:  231.7973750053364
loss:  231.85929610667475
loss:  231.62039775810806
loss:  230.66263455045868
loss:  232.07151129719165
loss:  224.31795181039186
loss:  225.40602425479358
loss:  232.2177516854649
loss:  224.86998521830904
loss:  231.1321521832927
loss:  231.152786144735
loss:  231.8680339510187
loss:  231.99765037357656
loss:  232.068266390476
loss:  228.7408963941686
loss:  231.9433891234652
loss:  221.4132776443187
loss:  229.937586852214
loss:  232.0790186064917
loss:  226.8333276215159
loss:  232.07385110711144
loss:  232.01870045678947
loss:  232.7370780385711
loss:  230.78020242842672
loss:  231.7031093357115
loss:  218.85933107674808
loss:  231.10241577199895
loss:  230.84335712365527
loss:  230.58727597526527
loss:  231.89662043876996
loss:  231.92603897404362
loss:  230.53177692671795
loss:  232.0512158461089
loss:  229.9468847448848
loss:  229.5606976428351
loss:  230.54073653321814
loss:  223.2173122248089
loss:  225.9109462841174
loss:  231.68080533306096
loss:  232.7263296607513
loss:  232.05994985949476
loss:  231.349552685536
loss:  229.33958282452033
loss:  230.51343136721195
loss:  228.44242930232588
loss:  228.59170473389645
loss:  229.25555520012722
loss:  232.84900613755826
loss:  219.289697386483
loss:  232.07544128496033
loss:  230.61781489682446
loss:  219.48394139804972
loss:  231.50206134910664
loss:  230.78834697833776
loss:  224.67119929746798
loss:  230.14542750456377
loss:  231.72767884286327
loss:  231.79610593836372
loss:  230.49045819675564
loss:  230.78328647352558
loss:  230.11340374885611
loss:  232.13264432357954
loss:  231.91333794228584
loss:  229.6723243672488
loss:  225.6471327780419
loss:  232.6063593618774
loss:  230.72972940728104
loss:  224.36573234645658
loss:  230.4290255114606
loss:  231.27745478714323
loss:  230.77265768705823
loss:  231.43691754692438
loss:  230.7738946569034
loss:  230.1268992286515
loss:  231.90778529024914
loss:  231.774961279609
loss:  228.59095279125557
loss:  226.7836641817514
loss:  232.01935515066214
loss:  231.21972172125862
loss:  217.80827016451684
loss:  231.71560445753178
loss:  229.26306607775408
loss:  225.47057040293834
loss:  232.27048376989447
loss:  231.20506880235533
loss:  232.32203149578456
loss:  225.5189429186535
loss:  231.72224949290327
loss:  220.65768173908492
loss:  231.42911383418516
loss:  231.12622386147092
loss:  224.9474253694608
loss:  231.70550927298112
loss:  226.97887788480762
loss:  230.12426641875138
loss:  230.39389759601735
loss:  226.97740686466148
loss:  231.9799368466716
loss:  231.0253824788563
loss:  231.3022438287568
loss:  225.1036634945267
loss:  229.28641533413082
loss:  231.1215652911804
loss:  230.54492223471618
loss:  230.7871541891452
loss:  230.87469842319734
loss:  227.17652331099754
loss:  223.19006772867263
loss:  231.72534343845916
loss:  216.78324965062708
loss:  230.6886801050672
loss:  229.5340157257319
weights:  [0.00024848 0.00080908 0.00064105 ... 0.00023382 0.00018142 0.00033056]
cy dot constraint : -5.785253860054061
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 216.78324965062708
   maxcv: 5.785353860054061
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 200
  status: 4
 success: False
       x: array([0.00024848, 0.00080908, 0.00064105, ..., 0.00023382, 0.00018142,
       0.00033056])
Train data:
------------
Train accuracy :  0.7792857142857142
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 640 (14%)
Protected in positive class: 192 (8%)
P-rule is: 61%

Test data: 
------------
Test accuracy :  0.777
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 277 (13%)
Protected in positive class: 89 (9%)
P-rule is: 70%
------------------------------------------------------------------------
------------------------------------------------------------------------
