iter:  100.0 , lambda:  1 , alpha:  0.4444444444444444 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.0002697  0.00054752 0.00087879 ... 0.0003897  0.00043392 0.00060245]
loss:  878.082215575949
loss:  936.792681815514
loss:  935.4964418200843
loss:  938.7791999556778
loss:  928.9005428699098
loss:  933.8629712946752
loss:  942.7460726250818
loss:  943.7458558163252
loss:  906.4244613506975
loss:  878.8746078025805
loss:  942.502917357843
loss:  879.0604074217565
loss:  943.5049738673936
loss:  929.9439133505111
loss:  923.6403016874073
loss:  938.3344739263388
loss:  910.6883820932252
loss:  943.3748168303647
loss:  940.0577725805105
loss:  944.4913966592633
loss:  940.3944471675103
loss:  942.4677919196881
loss:  880.1483954479303
loss:  877.659205963744
loss:  940.8429234793579
loss:  942.5953055176078
loss:  906.6111480290308
loss:  937.7582666266378
loss:  905.9492887722159
loss:  942.3505613887062
loss:  928.0772139897895
loss:  930.1504055044664
loss:  937.1683641287141
loss:  932.9197494860119
loss:  927.955754959597
loss:  886.460671400699
loss:  878.6162895014222
loss:  933.5521244179861
loss:  898.8542872844992
loss:  941.1274919133538
loss:  884.7234444154932
loss:  941.9602723649052
loss:  922.7072339246804
loss:  930.3499286488707
loss:  945.6295307496961
loss:  942.471989628307
loss:  939.3098762011208
loss:  932.3576484192607
loss:  939.3576487293257
loss:  920.5337931907196
loss:  938.6415690321875
loss:  942.8626611096382
loss:  939.1213178964113
loss:  939.7690889015572
loss:  916.0892766106382
loss:  898.960107580988
loss:  879.233220958208
loss:  937.2985099769297
loss:  941.4875839600737
loss:  943.6749497703539
loss:  914.0521875029858
loss:  881.8347462614025
loss:  877.9854834192341
loss:  937.5033546636439
loss:  938.6597994736699
loss:  905.370506515008
loss:  878.131185615381
loss:  942.4167694958887
loss:  942.4992205992064
loss:  942.6656963610425
loss:  937.753933105652
loss:  942.4830375552457
loss:  930.854964067924
loss:  938.1005811923635
loss:  938.6726343192124
loss:  941.2839488331632
loss:  881.1817304409443
loss:  915.9625134641095
loss:  944.1303511811356
loss:  937.1479086716218
loss:  941.9180377720635
loss:  940.2998819620562
loss:  913.130054757203
loss:  886.2761881178201
loss:  942.4576370211676
loss:  942.5510111450485
loss:  878.9667922453256
loss:  941.4620169895338
loss:  941.6716454884141
loss:  940.7100298677805
loss:  936.8716258083372
loss:  942.530555951816
loss:  910.7922366635895
loss:  915.3759495827194
loss:  943.1389936785426
loss:  913.1375995041744
loss:  938.7467213486593
loss:  938.8841952004275
loss:  941.6974275014832
loss:  942.2156673262853
weights:  [0.0002697  0.00054752 0.00087879 ... 0.0003897  0.00043392 0.00060245]
cy dot constraint : -2.7868258755221986
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 877.659205963744
   maxcv: 2.786925875522199
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 100
  status: 4
 success: False
       x: array([0.0002697 , 0.00054752, 0.00087879, ..., 0.0003897 , 0.00043392,
       0.00060245])
Train data:
------------
Train accuracy :  0.776
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 690 (15%)
Protected in positive class: 211 (9%)
P-rule is: 62%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
