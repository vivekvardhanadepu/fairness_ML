iter:  400.0 , lambda:  1 , alpha:  0.1111111111111111 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [8.83119381e-04 1.12037455e-05 9.26468955e-04 ... 2.02791169e-04
 6.41507156e-04 9.29995873e-04]
loss:  218.0433341687702
loss:  233.52599557843942
loss:  233.19412408485306
loss:  234.03133718009713
loss:  231.5273364787114
loss:  232.77716214956442
loss:  234.96543987462147
loss:  235.2673004285208
loss:  223.80233926527043
loss:  217.8697882581384
loss:  234.79350185579594
loss:  217.74553992271495
loss:  234.36320274391483
loss:  230.9673054393541
loss:  229.36961777845323
loss:  233.0826185882925
loss:  224.4863024393775
loss:  234.31345066314012
loss:  233.42352634053927
loss:  234.6131147938535
loss:  233.58296634083246
loss:  234.11480610762538
loss:  217.8933753899227
loss:  217.26644544780524
loss:  233.338992960707
loss:  233.7521388204626
loss:  223.13488205306052
loss:  232.56470639036027
loss:  222.97793552304836
loss:  233.68110858985406
loss:  230.1255804460866
loss:  230.58143684381045
loss:  232.40770929588388
loss:  231.34146314423344
loss:  229.99384652334223
loss:  219.08776232566944
loss:  218.14722963778428
loss:  231.50177045596078
loss:  221.2191475092773
loss:  233.40965255817434
loss:  218.67035743767744
loss:  233.61239621361193
loss:  228.76359094183337
loss:  230.67909591477692
loss:  234.47591595620196
loss:  233.59620511924908
loss:  232.95154508505658
loss:  231.2089166280592
loss:  232.9645069135029
loss:  228.20792066690953
loss:  232.78700172863213
loss:  233.83239473680067
loss:  232.90920110295562
loss:  232.9884839472021
loss:  225.4207243945565
loss:  221.26741293986052
loss:  217.29653462832263
loss:  232.43795376373006
loss:  233.4987415023538
loss:  234.02376537686246
loss:  226.55476000649656
loss:  217.95271215241183
loss:  217.3643495298899
loss:  232.46639784643392
loss:  232.69563220756976
loss:  222.7767856742186
loss:  217.01671644960234
loss:  232.93746037370605
loss:  232.95796592414004
loss:  232.9893562010289
loss:  231.77550636986695
loss:  232.95067395338808
loss:  230.04916644251105
loss:  231.80004445290896
loss:  232.0119048461284
loss:  232.66409882419623
loss:  217.5260808696074
loss:  225.28785124991725
loss:  233.30055174573863
loss:  231.5520285741119
loss:  232.78793806105904
loss:  232.39948585706432
loss:  224.60619550877897
loss:  218.83522121813502
loss:  232.93289235882082
loss:  232.96527993299955
loss:  216.95874144566864
loss:  231.87196151418436
loss:  231.94520433838815
loss:  231.7097963529983
loss:  230.75887675156017
loss:  232.15400215567118
loss:  224.2022258176126
loss:  225.28290622700553
loss:  232.2944351779427
loss:  224.75111506290492
loss:  231.22654430624453
loss:  231.228299292419
loss:  231.9577342946191
loss:  232.0870256272071
loss:  232.16354674554114
loss:  228.83723593463313
loss:  232.03200554409688
loss:  221.30033829416217
loss:  230.03331070962597
loss:  232.1646635828116
loss:  226.92776150030258
loss:  232.1659001296863
loss:  232.09920867990007
loss:  232.82260351618123
loss:  230.87888391099003
loss:  231.79391608657133
loss:  218.8747735267292
loss:  231.196757711126
loss:  230.94261054664187
loss:  230.6602309999385
loss:  231.98583258902633
loss:  232.01902251649378
loss:  230.62834099145024
loss:  232.13305842715613
loss:  230.0426095093474
loss:  229.65769295326623
loss:  230.62085407847
loss:  223.08897074540891
loss:  226.00375546204572
loss:  231.75734246206412
loss:  232.80951807540538
loss:  232.14552772197914
loss:  231.44298135247413
loss:  229.42356670466563
loss:  230.60997176763507
loss:  228.53946952408953
loss:  228.68841529860282
loss:  229.3459803083425
loss:  232.93186925681738
loss:  219.30894854591628
loss:  232.16103757273342
loss:  230.7014452646546
loss:  219.50339282547773
loss:  231.59453890224557
loss:  230.88413396239758
loss:  224.55356813448063
loss:  230.24318574617735
loss:  231.8185577361411
loss:  231.89076283792912
loss:  230.58703901739494
loss:  230.86191713653486
loss:  230.2122947753132
loss:  232.22216939766383
loss:  231.9925732085421
loss:  229.75820105286365
loss:  225.52061098146456
loss:  232.69229917871303
loss:  230.81586532460335
loss:  224.24478413565737
loss:  230.4965738796612
loss:  231.37507034454424
loss:  230.87021955503417
loss:  231.5298335007342
loss:  230.84490290611075
loss:  230.22470554892703
loss:  231.9969428348349
loss:  231.85214358312425
loss:  228.68764759475718
loss:  226.8774431786279
loss:  232.1068094915575
loss:  231.30917357467547
loss:  217.8074920542882
loss:  231.80644221271916
loss:  229.36075779630858
loss:  225.56022164113318
loss:  232.35643868086177
loss:  231.2989349824514
loss:  232.40205178539804
loss:  225.3946634889469
loss:  231.81312961870972
loss:  220.53908569303752
loss:  231.52197676909418
loss:  231.22066579103029
loss:  224.82800676486698
loss:  231.7963025594172
loss:  227.0729709446493
loss:  230.22207400687194
loss:  230.46148025558537
loss:  227.07150541905577
loss:  232.06172121878586
loss:  231.12025213720045
loss:  231.39563585241655
loss:  224.9830543471416
loss:  229.38121630186737
loss:  231.21598697637822
loss:  230.6415061573374
loss:  230.88295242751295
loss:  230.97422663192327
loss:  227.2715394493587
loss:  223.07961783066736
loss:  231.81607099115132
loss:  216.7754392699805
loss:  230.7679137657423
loss:  229.62497113436
loss:  229.15099295590014
loss:  228.38773900722774
loss:  229.2584329367088
loss:  230.216234458259
loss:  225.43815930221933
loss:  225.09398200080398
loss:  225.8565976956609
loss:  230.71709602762843
loss:  230.32288034998112
loss:  228.71344817214816
loss:  227.95132922679292
loss:  229.58440698329628
loss:  225.62379981083396
loss:  218.31893600614558
loss:  230.77700933123967
loss:  229.83599283847906
loss:  230.72881667910647
loss:  225.74464578967664
loss:  217.4538817901439
loss:  230.83483214685472
loss:  224.244377049045
loss:  226.73919850911145
loss:  217.22569797962288
loss:  230.501292006384
loss:  227.1803103454943
loss:  230.74231671262018
loss:  223.42322455681582
loss:  229.98685569260712
loss:  229.7616358736553
loss:  229.51101534215587
loss:  230.5902162529677
loss:  230.73447991771533
loss:  229.62715944588385
loss:  223.35553838000772
loss:  229.24138004642842
loss:  229.72587528283879
loss:  229.62297569320924
loss:  219.3877256545394
loss:  222.03465522032764
loss:  228.71273536216492
loss:  228.7119610142279
loss:  231.10892328274616
loss:  230.60841197431483
loss:  230.19039573256364
loss:  226.57640297970133
loss:  226.3501601280988
loss:  230.1120568181253
loss:  227.47089986294174
loss:  231.13058979980804
loss:  229.3883631338997
loss:  230.6945780459784
loss:  231.10845600879338
loss:  231.7134037949988
loss:  228.62833225805548
loss:  224.15981850319434
loss:  231.7021088101799
loss:  225.1329290803433
loss:  226.24801227775848
loss:  230.13503844705005
loss:  223.24619921363464
loss:  230.29704876307372
loss:  230.25088812569123
loss:  225.69990741478085
loss:  231.46861330692698
loss:  230.72740427836828
loss:  230.7361467215544
loss:  223.36000949367434
loss:  216.33188195425626
loss:  230.06839274509758
loss:  217.22842320025978
loss:  229.4521933078833
loss:  230.1680764769122
loss:  230.65952252971343
loss:  230.50886673233407
loss:  225.37375461326988
loss:  225.3930995785615
loss:  230.14677581370964
loss:  217.88345700076727
loss:  217.83747004435864
loss:  225.94551642480565
loss:  230.57977947986794
loss:  229.83467542291777
loss:  221.91835206225522
loss:  223.57427359271819
loss:  229.9962562045283
loss:  225.20533430189138
loss:  228.1903470601332
loss:  217.81717196427468
loss:  224.49056956852806
loss:  225.4288042274056
loss:  230.09680367256232
loss:  225.5876621556823
loss:  229.05383992887866
loss:  229.42322184728508
loss:  228.90496943742394
loss:  228.84343962215385
loss:  216.03421471874668
loss:  229.10442579508174
loss:  223.35107994030685
loss:  228.24614235803156
loss:  230.0732979390765
loss:  223.48450944156374
loss:  224.49390165480793
loss:  216.1976785350973
loss:  221.0915310372573
loss:  226.319127910562
loss:  229.07937551451758
loss:  225.06057238117955
loss:  229.25675578078327
loss:  217.56430394421474
loss:  229.19049880732732
loss:  229.40496345912618
loss:  223.85132871143367
loss:  229.11761005298254
loss:  229.32047729690987
loss:  228.91141475538026
loss:  224.81621612794515
loss:  229.2600452459206
loss:  227.22298330039123
loss:  215.08322476490252
loss:  228.9956821872824
loss:  225.83734458815303
loss:  229.4188696545885
loss:  228.98192295156707
loss:  228.4441183795072
loss:  229.70316711982943
loss:  227.77443601957924
loss:  224.8076058721145
loss:  225.09301601599273
loss:  219.53993248118985
loss:  227.17024261316686
loss:  215.957713919266
loss:  221.76706884620003
loss:  227.3673464511838
loss:  222.46833145900538
loss:  229.72250555925535
loss:  227.07050137815176
loss:  223.02467705642698
loss:  222.31256091910208
loss:  226.36368766622695
loss:  228.68917246161217
loss:  228.72691786742428
loss:  226.89555240489835
loss:  229.90880237263653
loss:  228.5518800233105
loss:  224.62699269827766
loss:  229.58807630374739
loss:  229.14086120504652
loss:  229.89538084542326
loss:  221.90870123250903
loss:  229.4430193959452
loss:  226.90871111818387
loss:  228.2431775605361
loss:  225.3821949863818
loss:  227.9939248117612
loss:  227.179086962356
loss:  229.5173082215581
loss:  229.48000674337712
loss:  227.77744735853122
loss:  229.12673696431312
loss:  228.48971291345518
loss:  229.24395414808959
loss:  222.50138143859525
loss:  229.31130816582325
loss:  229.2362768201135
loss:  217.5894842446608
loss:  222.30840764188028
loss:  230.0182878792115
loss:  221.50180183443737
loss:  226.70693737536138
loss:  229.13977504364956
loss:  229.14520606006315
loss:  228.85723001575042
loss:  221.47353498779802
loss:  228.81092997898602
loss:  229.34338190650178
loss:  222.78876894808815
loss:  229.03146561858523
loss:  216.59065680136922
loss:  227.48633126352263
loss:  228.43626052608406
loss:  228.13663246868043
loss:  219.801313106233
loss:  228.46943899943994
loss:  214.43242319578468
loss:  226.975421168855
loss:  225.7884565908652
loss:  227.47658423747623
loss:  226.95263024053605
loss:  226.48895450950081
loss:  215.30202596911454
loss:  226.08421669172674
loss:  224.44206432948943
loss:  228.33617966299926
loss:  227.75007167596803
loss:  214.91522456772412
loss:  222.51293178943843
loss:  226.3522351827603
loss:  227.46471753766224
loss:  227.15537355040243
weights:  [8.83119381e-04 1.12037455e-05 9.26468955e-04 ... 2.02791169e-04
 6.41507156e-04 9.29995873e-04]
cy dot constraint : -7.806032253821286
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 214.43242319578468
   maxcv: 7.806132253821286
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 400
  status: 4
 success: False
       x: array([8.83119381e-04, 1.12037455e-05, 9.26468955e-04, ...,
       2.02791169e-04, 6.41507156e-04, 9.29995873e-04])
Train data:
------------
Train accuracy :  0.7802857142857142
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 631 (13%)
Protected in positive class: 190 (8%)
P-rule is: 61%

Test data: 
------------
Test accuracy :  0.777
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 277 (13%)
Protected in positive class: 89 (9%)
P-rule is: 70%
------------------------------------------------------------------------
------------------------------------------------------------------------
