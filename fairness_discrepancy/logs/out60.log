iter:  100.0 , lambda:  1 , alpha:  0.6666666666666666 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00091594 0.00031309 0.00065826 ... 0.00039646 0.00094009 0.0004163 ]
loss:  1318.6560035250027
loss:  1407.2525154786274
loss:  1405.315617197005
loss:  1410.2199745331582
loss:  1395.438248500453
loss:  1402.8752961512453
loss:  1416.1340863606158
loss:  1417.657984716084
loss:  1359.9786246674066
loss:  1320.0963669523383
loss:  1415.781377772853
loss:  1320.3159141343942
loss:  1417.2746531255607
loss:  1396.9835863933445
loss:  1387.5574909571392
loss:  1409.5557494157486
loss:  1366.3571242348398
loss:  1417.0767794827275
loss:  1412.1096518897828
loss:  1418.7762695883064
loss:  1412.656984175138
loss:  1415.7281660129602
loss:  1322.0369164241574
loss:  1318.2538287053385
loss:  1413.5337253623584
loss:  1416.1453906357149
loss:  1360.520476730507
loss:  1408.9271388408063
loss:  1359.5301510158977
loss:  1415.7777511883817
loss:  1394.4265078762494
loss:  1397.5030499697534
loss:  1408.0185681124372
loss:  1401.6976524607533
loss:  1394.2156401820814
loss:  1331.7907568777073
loss:  1319.8819829984836
loss:  1402.6426577690243
loss:  1348.8813008422812
loss:  1413.9585610254417
loss:  1329.1425984722011
loss:  1415.2015377721696
loss:  1386.3946601348491
loss:  1397.809458385765
loss:  1420.710731924103
loss:  1416.0164486530637
loss:  1411.230939393709
loss:  1400.8350028590166
loss:  1411.3020527158776
loss:  1383.136606497364
loss:  1410.2466039068172
loss:  1416.5755744606618
loss:  1410.9627287339852
loss:  1411.9115546220507
loss:  1374.7071898580205
loss:  1349.0536410997515
loss:  1320.788771663539
loss:  1408.2127786730762
loss:  1414.4965173974651
loss:  1417.8005098492404
loss:  1373.4373348789452
loss:  1324.8550594721103
loss:  1318.7390788178782
loss:  1408.513754129742
loss:  1410.2510692931723
loss:  1358.622624460221
loss:  1319.152815189209
loss:  1415.8825827970538
loss:  1416.004618164876
loss:  1416.2508813014447
loss:  1408.8992010089733
loss:  1415.9803912629004
loss:  1398.5901998799845
loss:  1409.4120123461173
loss:  1410.2928553736951
loss:  1414.2054164509484
loss:  1323.8436790964433
loss:  1374.5050521808341
loss:  1418.4975984344264
loss:  1408.089280067454
loss:  1415.130552701354
loss:  1412.7059888583578
loss:  1370.2734419657613
loss:  1331.5139600193265
loss:  1415.938422533758
loss:  1416.0802293574257
loss:  1320.393799814493
loss:  1414.452056618071
loss:  1414.763665375252
loss:  1413.329718701907
loss:  1407.6022013522145
loss:  1416.0480113461713
loss:  1366.7763817584432
loss:  1373.6302994217438
loss:  1416.9702672134806
loss:  1370.2847960465306
loss:  1410.403366862171
loss:  1410.5828376776494
loss:  1414.8097738514075
loss:  1415.5856558427502
weights:  [0.00091594 0.00031309 0.00065826 ... 0.00039646 0.00094009 0.0004163 ]
cy dot constraint : -2.8175283484882208
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 1318.2538287053385
   maxcv: 2.817628348488221
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 100
  status: 4
 success: False
       x: array([0.00091594, 0.00031309, 0.00065826, ..., 0.00039646, 0.00094009,
       0.0004163 ])
Train data:
------------
Train accuracy :  0.776
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 690 (15%)
Protected in positive class: 211 (9%)
P-rule is: 62%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
