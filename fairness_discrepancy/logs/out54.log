iter:  500.0 , lambda:  1 , alpha:  0.5555555555555556 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00075986 0.00071834 0.00033083 ... 0.00029443 0.00019955 0.00077006]
loss:  1097.6289466149067
loss:  1168.8813887446133
loss:  1167.271755966669
loss:  1171.3498358475517
loss:  1159.060257040005
loss:  1165.2416190704635
loss:  1176.2875763585416
loss:  1177.5361265233628
loss:  1137.2651267864755
loss:  1098.7351921288239
loss:  1175.9809450230916
loss:  1099.0473322176379
loss:  1177.226106918685
loss:  1160.3477565554542
loss:  1152.512788914857
loss:  1170.7975277710775
loss:  1142.632430068827
loss:  1177.0657969037568
loss:  1172.9537927231802
loss:  1178.466203280344
loss:  1173.3753536108804
loss:  1175.937369470433
loss:  1100.2817452648967
loss:  1097.2367571191285
loss:  1174.0408267052633
loss:  1176.222180978164
loss:  1137.6069695167616
loss:  1170.2072057396865
loss:  1136.7727027374901
loss:  1175.9190317918546
loss:  1158.1540392214038
loss:  1160.7429274414837
loss:  1169.4613667268443
loss:  1164.1952173086502
loss:  1158.0278466016798
loss:  1108.0372715534652
loss:  1098.6286828658758
loss:  1164.98107759307
loss:  1127.902356837677
loss:  1174.3948536143237
loss:  1105.9416548163472
loss:  1175.4308876131888
loss:  1151.4793313456287
loss:  1160.9782753647091
loss:  1180.0185787671635
loss:  1176.1423438587121
loss:  1172.1290221689867
loss:  1163.4800846397936
loss:  1172.1882996387305
loss:  1148.7758009283714
loss:  1171.3050371046613
loss:  1176.567446425926
loss:  1171.9007196845557
loss:  1172.7182075850858
loss:  1149.4937713710817
loss:  1128.014472541019
loss:  1099.444955301679
loss:  1169.6235660649545
loss:  1174.843528989933
loss:  1177.5869291641266
loss:  1140.7266736111383
loss:  1102.4903448180967
loss:  1097.6260919249141
loss:  1169.8819582723324
loss:  1171.3434048224997
loss:  1136.084047214426
loss:  1098.0271786278047
loss:  1175.9999180362033
loss:  1176.1012063334038
loss:  1176.310484636887
loss:  1170.1908391140332
loss:  1176.0819464039314
loss:  1161.6128614538127
loss:  1170.6456570665473
loss:  1171.3433346447587
loss:  1174.5967110689066
loss:  1101.6848791822279
loss:  1149.3371368085488
loss:  1178.1919253437782
loss:  1169.548744055794
loss:  1175.3831090902513
loss:  1173.3620022960201
loss:  1145.8021700173867
loss:  1107.8078998688482
loss:  1176.0517509398398
loss:  1176.1665538257453
loss:  1099.1012241004626
loss:  1174.830256526578
loss:  1175.0698330554892
loss:  1173.8734070841444
loss:  1169.1047501579562
loss:  1176.14196932235
loss:  1142.8685486522486
loss:  1148.609381757256
loss:  1176.9196876800315
loss:  1145.8117578229871
loss:  1171.4363314857542
loss:  1171.6102106298247
loss:  1175.104514824546
loss:  1175.7491882596244
loss:  1176.1115473201253
loss:  1159.5558508392824
loss:  1175.4805379724319
loss:  1127.7271394175395
loss:  1165.4981872419762
loss:  1176.1644833335538
loss:  1150.0825312451677
loss:  1176.132722704223
loss:  1175.889704346592
loss:  1179.4522197518722
loss:  1169.703593327094
loss:  1174.284948920981
loss:  1108.0198315093505
loss:  1171.287841291798
loss:  1170.0246907599908
loss:  1168.8042748783228
loss:  1175.2449648695454
loss:  1175.3989032299942
loss:  1168.4535659232067
loss:  1176.0443444172975
loss:  1165.551599922375
loss:  1163.6286324852988
loss:  1168.5345893586139
loss:  1137.2775928174945
loss:  1145.4997824336892
loss:  1174.2328664490733
loss:  1179.4204700415369
loss:  1176.0756737559766
loss:  1172.5186246520489
loss:  1162.5319292255504
loss:  1168.369779527011
loss:  1158.0750187970236
loss:  1158.816082908943
loss:  1162.1137388877155
loss:  1180.0539593312021
loss:  1110.0476359898819
loss:  1176.1496236508876
loss:  1168.9552740288404
loss:  1110.9894331236258
loss:  1173.2770478350772
loss:  1169.7293565946402
loss:  1144.756006504295
loss:  1166.5376620255527
loss:  1174.4011580119752
loss:  1174.8085282288246
loss:  1168.2474227028222
loss:  1169.7469063691187
loss:  1166.3827953780522
loss:  1176.42961113853
loss:  1175.3726972652423
loss:  1164.1972500343786
loss:  1149.7757568488714
loss:  1178.79541406811
loss:  1169.4486858443258
loss:  1143.3132953638608
loss:  1168.0498065519544
loss:  1172.1816049089493
loss:  1169.6583308280935
loss:  1172.9525996283637
loss:  1169.759162125613
loss:  1166.4461742943786
loss:  1175.299424422437
loss:  1174.696102975385
loss:  1158.8113230569418
loss:  1149.8347270175677
loss:  1175.8620529914754
loss:  1171.8758476396288
loss:  1103.0834812459748
loss:  1174.3452465044252
loss:  1162.1525703102182
loss:  1143.3181762687288
loss:  1177.1183411667982
loss:  1171.8058691754934
loss:  1177.4097517099924
loss:  1149.174135183567
loss:  1174.373380892908
loss:  1123.8568761450817
loss:  1172.9188056907137
loss:  1171.4090759053918
loss:  1146.1757968628453
loss:  1174.2969758012657
loss:  1150.8068915582205
loss:  1166.4331427114878
loss:  1167.8717147869709
loss:  1150.7984579464392
loss:  1175.6873874889523
loss:  1170.9063769808834
loss:  1172.288979782514
loss:  1147.0166078776701
loss:  1162.2644786383896
loss:  1171.38727659268
loss:  1168.5193923757286
loss:  1169.723385791202
loss:  1170.1983399591259
loss:  1151.7859342746685
loss:  1136.7689730311297
loss:  1174.396336053393
loss:  1098.5987868189561
loss:  1176.055080308675
loss:  1170.3083456436518
loss:  1167.9169511843393
loss:  1163.9560527492197
loss:  1168.4622939223004
loss:  1173.2836823669932
loss:  1149.3924453652194
loss:  1146.5999390811844
loss:  1151.4764100043392
loss:  1175.7987548790725
loss:  1173.579224528301
loss:  1165.7136343912457
loss:  1161.9912206869278
loss:  1170.0535324512127
loss:  1149.339741699428
loss:  1108.557936364398
loss:  1176.0989683904663
loss:  1171.3727146576844
loss:  1175.8620845794867
loss:  1149.775392411558
loss:  1098.9424989568051
loss:  1176.385465999268
loss:  1142.29342017744
loss:  1155.7523144318461
loss:  1102.276012370874
loss:  1174.6845238757926
loss:  1158.0788687969373
loss:  1175.9160908253368
loss:  1138.029123446504
loss:  1172.1741422838415
loss:  1170.915203325364
loss:  1169.7341732546524
loss:  1175.1311346864711
loss:  1175.873300563113
loss:  1170.3190363553251
loss:  1138.864298843422
loss:  1168.380865230225
loss:  1170.8209430113905
loss:  1170.3000036572228
loss:  1110.8125240197028
loss:  1130.6758956494682
loss:  1165.7101327511855
loss:  1165.706292633325
loss:  1177.7217507897524
loss:  1175.2547360016924
loss:  1172.9169103590893
loss:  1155.0726691906711
loss:  1153.9742405168308
loss:  1172.7155284721744
loss:  1159.5482087106006
loss:  1177.6599353238335
loss:  1169.1181051593082
loss:  1175.6869543429268
loss:  1177.6391220195399
loss:  1180.5877956366498
loss:  1165.411725584712
loss:  1141.5070426859993
loss:  1180.5274049897928
loss:  1147.8414277075929
loss:  1153.4072957875453
loss:  1172.90144867237
loss:  1136.985512218664
loss:  1173.6899218642789
loss:  1173.4470348204552
loss:  1149.711026844711
loss:  1179.4190842461555
loss:  1175.8514909536486
loss:  1175.88647027753
loss:  1137.6676286390687
loss:  1097.1211925629505
loss:  1174.6127231895464
loss:  1102.9955420359108
loss:  1171.5137431879361
loss:  1175.013683753886
loss:  1177.520313497649
loss:  1176.7856636748304
loss:  1150.0486228379675
loss:  1150.1228581091198
loss:  1175.0017053436604
loss:  1105.9235282410918
loss:  1105.7049589850933
loss:  1152.776615527987
loss:  1177.1334368063622
loss:  1173.4381629954862
loss:  1132.0341694457745
loss:  1140.401800529374
loss:  1174.2525753764128
loss:  1149.27448647279
loss:  1165.1658747460415
loss:  1105.6103445002882
loss:  1146.5595072859214
loss:  1150.1463231346554
loss:  1174.7595088426324
loss:  1152.2134680969098
loss:  1169.5176151850967
loss:  1171.414459459492
loss:  1168.7147125604433
loss:  1168.5373494581784
loss:  1097.5401691211457
loss:  1171.9496535330516
loss:  1140.327649022944
loss:  1168.3646690002936
loss:  1177.3771549889498
loss:  1141.6117832464977
loss:  1149.5723253108458
loss:  1100.231581186001
loss:  1126.630679235217
loss:  1158.8035455887966
loss:  1172.5591641455992
loss:  1149.615218665212
loss:  1173.4555245456459
loss:  1105.7614823980498
loss:  1173.0466865586395
loss:  1174.1506410131674
loss:  1143.1164912266295
loss:  1172.7488555123978
loss:  1173.7722597463894
loss:  1171.7140094795311
loss:  1151.2290242470044
loss:  1173.4152950851867
loss:  1163.1664719529401
loss:  1099.7119783976477
loss:  1173.408509423777
loss:  1157.599885638145
loss:  1175.4684889373543
loss:  1173.353387744913
loss:  1170.607721086959
loss:  1176.904799034616
loss:  1167.2509033544459
loss:  1152.497708715546
loss:  1153.9310281902385
loss:  1127.5529714185195
loss:  1164.3392856165929
loss:  1098.7842334889954
loss:  1138.7740285603395
loss:  1165.2089790270218
loss:  1140.7604143755807
loss:  1177.0033118332665
loss:  1163.7549879535939
loss:  1143.5946454970713
loss:  1141.859995555638
loss:  1160.2369339435825
loss:  1171.7042654665556
loss:  1172.0297656694036
loss:  1162.1641301401207
loss:  1177.9043009877678
loss:  1171.1510156070483
loss:  1151.5855705371794
loss:  1176.2839879543546
loss:  1174.1177292888237
loss:  1177.8476274102154
loss:  1137.9542053241605
loss:  1175.4758649552587
loss:  1162.9052770877208
loss:  1169.6051293780708
loss:  1155.4113201634566
loss:  1168.3589503714607
loss:  1164.2594238413444
loss:  1175.9360057754923
loss:  1175.807766324856
loss:  1167.3364887875803
loss:  1174.0404834867993
loss:  1170.8697662408952
loss:  1174.6321030152176
loss:  1143.214698784195
loss:  1174.9522519015416
loss:  1174.5927506768642
loss:  1110.478711411858
loss:  1141.835964131824
loss:  1178.4648779474276
loss:  1137.97543357267
loss:  1161.8884468817719
loss:  1174.130874694844
loss:  1174.1327271619816
loss:  1172.690378295233
loss:  1137.7937762217655
loss:  1172.451384942516
loss:  1175.1380727209664
loss:  1144.4916950914283
loss:  1173.5315780379328
loss:  1105.8347030004995
loss:  1165.8115121515357
loss:  1170.6104259593426
loss:  1169.067836120778
loss:  1128.978725237376
loss:  1170.7053999523132
loss:  1099.1768987342605
loss:  1170.3207524120776
loss:  1164.4583677449784
loss:  1172.9200677377532
loss:  1170.285833748413
loss:  1167.9592899722338
loss:  1102.8216851631155
loss:  1165.8949507670823
loss:  1157.7477092047343
loss:  1176.9583204675598
loss:  1174.2480206813034
loss:  1101.1244550884428
loss:  1145.7526947238605
loss:  1167.2510727610513
loss:  1172.8646563500586
loss:  1171.2823573561009
loss:  1168.4555562869261
loss:  1173.871936427676
loss:  1145.2325035992772
loss:  1174.668823216227
loss:  1174.902128957071
loss:  1167.2311076829835
loss:  1167.3727716464207
loss:  1166.2312810554874
loss:  1116.477807934968
loss:  1097.8554119400599
loss:  1130.1634427113481
loss:  1174.311598895517
loss:  1141.866208090392
loss:  1174.1264303820085
loss:  1142.3598428261757
loss:  1169.6003683647848
loss:  1128.989967223553
loss:  1142.5891027871908
loss:  1174.9752055256158
loss:  1141.545090611723
loss:  1165.1896847424346
loss:  1176.1827543909983
loss:  1170.5511822394903
loss:  1150.265853273555
loss:  1130.828382582058
loss:  1155.1648531276921
loss:  1176.9471165472721
loss:  1177.317936350706
loss:  1166.5879700498988
loss:  1171.3229785343906
loss:  1135.918610122867
loss:  1178.4970786553422
loss:  1149.9402212252737
loss:  1176.7520528405316
loss:  1171.633211093914
loss:  1150.279451000545
loss:  1174.8831095429898
loss:  1172.0337023537134
loss:  1174.9788949023723
loss:  1173.4782634823237
loss:  1172.9956926626046
loss:  1174.1316725878692
loss:  1097.7791706465953
loss:  1175.1052565113473
loss:  1152.3711945839796
loss:  1098.7804336590407
loss:  1173.1668355392346
loss:  1174.3616955385305
loss:  1174.4494361971492
loss:  1159.2135897896037
loss:  1110.7410258611567
loss:  1172.1849445591324
loss:  1164.671583345775
loss:  1175.8056343439785
loss:  1175.013533521931
loss:  1105.93513516518
loss:  1169.0804213753183
loss:  1179.375868234894
loss:  1146.2340165081455
loss:  1125.7684108246522
loss:  1151.505274897007
loss:  1147.274857532158
loss:  1111.01561758624
loss:  1161.4906895628646
loss:  1172.8807343785554
loss:  1158.0583630478175
loss:  1143.3328841327182
loss:  1110.4956468010112
loss:  1176.319856949832
loss:  1169.5167451590303
loss:  1171.7595713179599
loss:  1171.4176394819265
loss:  1171.8626201748755
loss:  1174.3601936002876
loss:  1173.5099002017598
loss:  1177.9553111344947
loss:  1168.373138993699
loss:  1139.8174812869252
loss:  1166.8294880545407
loss:  1175.023303683487
loss:  1097.2242402870038
loss:  1156.8533307613886
loss:  1174.961606534097
loss:  1168.497385269068
loss:  1174.226779144076
loss:  1149.2210102494903
loss:  1110.779792017577
loss:  1131.060091368222
loss:  1168.392816738998
loss:  1098.9510773505833
loss:  1171.8467321081032
loss:  1176.3537318186811
loss:  1147.9475068375648
loss:  1099.5522812914803
loss:  1166.6019691633044
loss:  1099.9442354652967
loss:  1171.6136548171362
loss:  1098.1799741456775
loss:  1170.4430931227428
loss:  1148.677851210298
weights:  [0.00075986 0.00071834 0.00033083 ... 0.00029443 0.00019955 0.00077006]
cy dot constraint : -3.7703702484298014
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 1097.1211925629505
   maxcv: 3.7704702484298016
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 500
  status: 4
 success: False
       x: array([0.00075986, 0.00071834, 0.00033083, ..., 0.00029443, 0.00019955,
       0.00077006])
Train data:
------------
Train accuracy :  0.7764285714285715
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 687 (15%)
Protected in positive class: 211 (9%)
P-rule is: 63%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
