iter:  100.0 , lambda:  1 , alpha:  0.2222222222222222 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [1.69907997e-04 6.83531138e-05 9.56685989e-04 ... 6.96995527e-05
 8.43640866e-04 7.22375904e-04]
loss:  437.56886120268354
loss:  466.257752695498
loss:  465.6066382478445
loss:  467.25487748426093
loss:  462.2948122694256
loss:  464.7860247901618
loss:  469.21709053512313
loss:  469.7416330109573
loss:  453.1512885496275
loss:  437.7123917184207
loss:  469.117696235724
loss:  437.8541870697889
loss:  469.61443609979324
loss:  462.8156741339819
loss:  459.6486511271578
loss:  467.03177431105877
loss:  455.28578955954805
loss:  469.5406220402971
loss:  467.8514686937571
loss:  470.1135313809842
loss:  468.05957562732334
loss:  469.1009130019465
loss:  438.3151557665034
loss:  437.11870724802134
loss:  468.04767545386983
loss:  468.9127359578048
loss:  452.98054551103115
loss:  466.50187174597403
loss:  452.6491840883208
loss:  468.78536497558633
loss:  461.63958074341406
loss:  462.64720489448615
loss:  466.1976678331903
loss:  464.07322735658505
loss:  461.5326867103785
loss:  441.06196350007843
loss:  437.3931096214603
loss:  464.3908842344872
loss:  449.0919764043699
loss:  468.1899451898873
loss:  440.2897819785261
loss:  468.60421845855814
loss:  458.94123031624184
loss:  462.76852569287433
loss:  470.4200662877286
loss:  468.80516379717017
loss:  467.27607623577177
loss:  463.79122385958436
loss:  467.30052161659904
loss:  457.84570777963694
loss:  466.94479268828513
loss:  469.05683771328944
loss:  467.1860697272231
loss:  467.4708271048943
loss:  457.71956725888737
loss:  449.14592284704645
loss:  437.7514513010696
loss:  466.26170306531964
loss:  468.3699834668454
loss:  469.4596412928989
loss:  454.58598513018063
loss:  438.898803292715
loss:  437.2849489507073
loss:  466.3533121263192
loss:  466.90869846724024
loss:  452.34882164960743
loss:  437.16527461637764
loss:  468.8300501469089
loss:  468.8709858041625
loss:  468.94793703012147
loss:  466.49487451767527
loss:  468.86093609283313
loss:  463.0370416511652
loss:  466.63011672030206
loss:  466.9606506809659
loss:  468.2689328277953
loss:  438.5574182750172
loss:  457.650530474115
loss:  469.6476823375159
loss:  466.14936962965726
loss:  468.5640736618097
loss:  467.7629882029756
loss:  456.23945329224784
loss:  440.9705231666751
loss:  468.8406683145892
loss:  468.8932107394121
loss:  437.59102821129795
loss:  468.3188634490258
loss:  468.45587340784425
loss:  467.9788009542551
loss:  466.0577826603995
loss:  468.8787417510638
loss:  455.0719401642649
loss:  457.3587376309554
loss:  469.16095941350954
loss:  456.24318409655024
loss:  466.99835973931175
loss:  467.03365436516964
loss:  468.4743204206875
loss:  468.73344283445783
weights:  [1.69907997e-04 6.83531138e-05 9.56685989e-04 ... 6.96995527e-05
 8.43640866e-04 7.22375904e-04]
cy dot constraint : -2.7563092737371733
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 437.11870724802134
   maxcv: 2.7564092737371735
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 100
  status: 4
 success: False
       x: array([1.69907997e-04, 6.83531138e-05, 9.56685989e-04, ...,
       6.96995527e-05, 8.43640866e-04, 7.22375904e-04])
Train data:
------------
Train accuracy :  0.776
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 690 (15%)
Protected in positive class: 211 (9%)
P-rule is: 62%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
