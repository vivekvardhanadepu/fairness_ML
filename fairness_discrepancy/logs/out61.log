iter:  200.0 , lambda:  1 , alpha:  0.6666666666666666 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00067905 0.00084175 0.0003399  ... 0.0001976  0.00063846 0.00082611]
loss:  1318.5237776248089
loss:  1406.5226376697362
loss:  1404.5849591003166
loss:  1409.4931074176654
loss:  1394.7232891100005
loss:  1402.1427304367226
loss:  1415.4464670203852
loss:  1416.9236136380155
loss:  1361.0044296126644
loss:  1319.9632074243746
loss:  1415.0667185679113
loss:  1320.2140140256352
loss:  1416.5691454527553
loss:  1396.286167896143
loss:  1386.862930753204
loss:  1408.8281167189164
loss:  1367.404798096941
loss:  1416.3805358309785
loss:  1411.438993483343
loss:  1418.0398411085894
loss:  1411.9122032127068
loss:  1415.0135703547223
loss:  1321.8975276727137
loss:  1318.1243634957498
loss:  1412.8165061348052
loss:  1415.4475482945957
loss:  1361.543446752466
loss:  1408.2020885808442
loss:  1360.549604847716
loss:  1415.0847905724913
loss:  1393.7290579246678
loss:  1396.8568090378947
loss:  1407.3273248313783
loss:  1400.9667439744098
loss:  1393.5859751093294
loss:  1331.5513604441408
loss:  1319.7829618456885
loss:  1401.9122523233946
loss:  1349.92918365127
loss:  1413.24253470439
loss:  1328.9657699748052
loss:  1414.4906716668745
loss:  1385.7018152763533
loss:  1397.1379051077442
loss:  1419.993467068668
loss:  1415.2946784535839
loss:  1410.526447952835
loss:  1400.1268514335359
loss:  1410.5975112110955
loss:  1382.45623286075
loss:  1409.5233143034359
loss:  1415.8387650539646
loss:  1410.2401833988479
loss:  1411.238574293926
loss:  1375.7600701052445
loss:  1350.0786460515894
loss:  1320.6903247127332
loss:  1407.522964148553
loss:  1413.7818342931173
loss:  1417.056605208625
loss:  1372.771509469229
loss:  1324.705011064993
loss:  1318.6061544768736
loss:  1407.8376833239474
loss:  1409.5841536476992
loss:  1359.7082049209428
loss:  1319.052142497283
loss:  1415.1760156481578
loss:  1415.2991804851229
loss:  1415.553170460773
loss:  1408.200198001105
loss:  1415.276578227906
loss:  1397.8796713467882
loss:  1408.74691952793
loss:  1409.5694733446305
loss:  1413.475425717232
loss:  1323.684943272926
loss:  1375.576749279896
loss:  1417.766030954272
loss:  1407.3255272347865
loss:  1414.4412564169704
loss:  1412.0142546841664
loss:  1371.3293882395094
loss:  1331.2750581270352
loss:  1415.2438032374919
loss:  1415.3793715067188
loss:  1320.2975292158947
loss:  1413.7706942646898
loss:  1414.061815716901
loss:  1412.6197714154637
loss:  1406.875332278572
loss:  1415.3517683880498
loss:  1367.8206523898698
loss:  1374.698307396481
loss:  1416.2767739316
loss:  1371.3407860068203
loss:  1409.6801483048214
loss:  1409.9115101258637
loss:  1414.096358034992
loss:  1414.8715485464827
loss:  1415.2865013014098
loss:  1395.4034913442372
loss:  1414.5505677588326
loss:  1349.7239474058676
loss:  1402.5410594493217
loss:  1415.3759722106429
loss:  1384.0196122171187
loss:  1415.3247115928702
loss:  1415.0491848571544
loss:  1419.3186568646477
loss:  1407.580750980359
loss:  1413.1105172345272
loss:  1331.530884904042
loss:  1409.5010112255225
loss:  1407.9596077385788
loss:  1406.5406894705131
loss:  1414.2666605212567
loss:  1414.4402621639701
loss:  1406.0902245152774
loss:  1415.234952084225
loss:  1402.6094173081553
loss:  1400.2949667556263
loss:  1406.2198283502216
loss:  1361.1543222032926
loss:  1378.5097105868922
loss:  1413.0600437694093
loss:  1419.2810162422454
loss:  1415.2691394558449
loss:  1410.9828483553256
loss:  1398.7888146092766
loss:  1405.9909393087294
loss:  1393.6212648976477
loss:  1394.5133949593035
loss:  1398.4928480089823
loss:  1420.0284344028903
loss:  1333.948090540139
loss:  1415.3581287084112
loss:  1406.5793709382144
loss:  1335.0798038226083
loss:  1411.8968079313856
loss:  1407.6268183261589
loss:  1370.0769846382307
loss:  1403.785121049583
loss:  1413.2501521523807
loss:  1413.683122490709
loss:  1405.8427834476258
loss:  1407.6761347053714
loss:  1403.5934298010834
loss:  1415.686784881311
loss:  1414.4292696066848
loss:  1401.0040619590368
loss:  1376.111306874601
loss:  1418.528395627138
loss:  1407.3121444635658
loss:  1368.3448445465285
loss:  1405.617461777133
loss:  1410.5596787547427
loss:  1407.5337513690936
loss:  1411.50496834373
loss:  1407.5866215957967
loss:  1403.675807296546
loss:  1414.330876300522
loss:  1413.6158318061082
loss:  1394.5075061826599
loss:  1383.722934339762
loss:  1415.0103271018202
loss:  1410.2213701445473
loss:  1325.3920891238645
loss:  1413.1830144299126
loss:  1398.518651177126
loss:  1375.8909019841603
loss:  1416.520030869916
loss:  1410.1261695848143
loss:  1416.8721873351014
loss:  1375.3788606134317
loss:  1413.2165624574823
loss:  1345.1074278210328
loss:  1411.4646020385214
loss:  1409.6485027473202
loss:  1371.7805190681295
loss:  1413.1248378638988
loss:  1384.8918471523268
loss:  1403.6602830914442
loss:  1405.4034179810467
loss:  1384.8816770923142
loss:  1414.8045249824543
loss:  1409.0427209957422
loss:  1410.7076105692306
loss:  1372.7874108330077
loss:  1398.663710364791
loss:  1409.6223206059763
loss:  1406.1696189138497
loss:  1407.61965511266
loss:  1408.1531760301
loss:  1386.0664198388608
loss:  1360.5453329908664
loss:  1413.2435772580413
loss:  1319.6176289229504
loss:  1415.2460301052688
loss:  1408.3237431366051
weights:  [0.00067905 0.00084175 0.0003399  ... 0.0001976  0.00063846 0.00082611]
cy dot constraint : -2.8233588223391193
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 1318.1243634957498
   maxcv: 2.8234588223391195
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 200
  status: 4
 success: False
       x: array([0.00067905, 0.00084175, 0.0003399 , ..., 0.0001976 , 0.00063846,
       0.00082611])
Train data:
------------
Train accuracy :  0.776
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 690 (15%)
Protected in positive class: 211 (9%)
P-rule is: 62%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
