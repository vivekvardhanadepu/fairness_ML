iter:  600.0 , lambda:  1 , alpha:  0.5555555555555556 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.0003034  0.00077697 0.00087249 ... 0.00092401 0.0005113  0.00012043]
loss:  1097.0178179539662
loss:  1167.1188258096358
loss:  1165.508599952427
loss:  1169.5904476164255
loss:  1157.2733181411302
loss:  1163.4767384311972
loss:  1174.5798627989036
loss:  1175.8069712125398
loss:  1138.7801753526312
loss:  1098.1208370387833
loss:  1174.2371945759462
loss:  1098.4821993110766
loss:  1175.4937392218085
loss:  1158.554928408725
loss:  1150.7123121346215
loss:  1169.037344576879
loss:  1144.1767164594555
loss:  1175.3459782643863
loss:  1171.2706829604124
loss:  1176.7448311451212
loss:  1171.643256859705
loss:  1174.193290074076
loss:  1099.6293449094526
loss:  1096.6340210248497
loss:  1172.2969835063352
loss:  1174.5038176218934
loss:  1139.112148841052
loss:  1168.4542549820958
loss:  1138.273045736457
loss:  1174.207375384152
loss:  1156.3658734714422
loss:  1159.0069999891793
loss:  1167.7034697522797
loss:  1162.4362999571424
loss:  1156.3193262493178
loss:  1107.1881728105207
loss:  1098.072014223525
loss:  1163.2226259125969
loss:  1129.3624544766053
loss:  1172.652495745318
loss:  1105.3176210770876
loss:  1173.695367418658
loss:  1149.6850859117035
loss:  1159.2047417818605
loss:  1178.3527709054945
loss:  1174.556913717067
loss:  1170.377325422864
loss:  1161.7025933802406
loss:  1170.4363395308121
loss:  1146.9792513871685
loss:  1169.5540876761559
loss:  1174.849459029586
loss:  1170.150314730878
loss:  1171.03519080423
loss:  1151.0594565838303
loss:  1129.4706190575782
loss:  1098.938129682681
loss:  1167.8676386742864
loss:  1173.1030992411484
loss:  1175.8858434419667
loss:  1138.9313959831861
loss:  1101.8467627243303
loss:  1097.015295299609
loss:  1168.1448099170818
loss:  1169.6694856332372
loss:  1137.5908463325216
loss:  1097.4698323973648
loss:  1174.270302023232
loss:  1174.3725878496884
loss:  1174.5924995914265
loss:  1168.432066382717
loss:  1174.3557570621715
loss:  1159.8333284809016
loss:  1168.964363014157
loss:  1169.5921711698752
loss:  1172.8617119063254
loss:  1101.0084576552442
loss:  1150.9052031512983
loss:  1176.5842552465922
loss:  1167.9449088742656
loss:  1173.6770019484256
loss:  1171.6272789281495
loss:  1147.3505266586371
loss:  1106.9599834777714
loss:  1174.3377856471093
loss:  1174.4440191934004
loss:  1098.5582527450229
loss:  1173.1609380715022
loss:  1173.3360123246052
loss:  1172.1291292576166
loss:  1167.3494457486527
loss:  1174.4257850890326
loss:  1144.4020527238997
loss:  1150.1729074650252
loss:  1175.2592381413335
loss:  1147.360203892676
loss:  1169.6853326391454
loss:  1169.9048739495006
loss:  1173.365779359428
loss:  1174.0142660656852
loss:  1174.3974405631964
loss:  1157.775207792459
loss:  1173.7446555392123
loss:  1129.185071592425
loss:  1163.734689783684
loss:  1174.4377948175659
loss:  1148.288319342196
loss:  1174.4035559683973
loss:  1174.1807283172272
loss:  1177.7530184151353
loss:  1167.9688924941331
loss:  1172.54305424112
loss:  1107.1704914259626
loss:  1169.5373214960596
loss:  1168.3024617986546
loss:  1167.1005697155663
loss:  1173.5072638153695
loss:  1173.667666681634
loss:  1166.6982896838751
loss:  1174.330451868129
loss:  1163.7837722885947
loss:  1161.858532002793
loss:  1166.7912704227485
loss:  1138.7916798197946
loss:  1143.7034517977897
loss:  1172.5386982089333
loss:  1177.736169214272
loss:  1174.3494830504592
loss:  1170.7698252254122
loss:  1161.0440722789924
loss:  1166.6143392447818
loss:  1156.2939138992758
loss:  1157.0351083316896
loss:  1160.3323774163143
loss:  1178.4071958010802
loss:  1109.1920254239085
loss:  1174.42316680974
loss:  1167.427451957048
loss:  1110.130581907602
loss:  1171.5303153578209
loss:  1167.9749906022305
loss:  1146.299050933636
loss:  1164.7801870497717
loss:  1172.6588067334558
loss:  1173.171174864568
loss:  1166.4920407263194
loss:  1168.0144821969925
loss:  1164.6323421936265
loss:  1174.7012024864673
loss:  1173.6681977400744
loss:  1162.4253726175402
loss:  1151.3470358562697
loss:  1177.0919496708011
loss:  1167.6907627755263
loss:  1144.851291296791
loss:  1166.3960581361546
loss:  1170.4557236124185
loss:  1167.9127722308824
loss:  1171.2051734813172
loss:  1168.220912265658
loss:  1164.6883933965285
loss:  1173.562306595415
loss:  1172.9995890531368
loss:  1157.0304057483354
loss:  1148.0399749080632
loss:  1174.129744156283
loss:  1170.1222536937157
loss:  1102.4206357027222
loss:  1172.6031463759996
loss:  1160.3818413244321
loss:  1141.5217221159432
loss:  1175.3972367790457
loss:  1170.0561100265209
loss:  1175.7189552695354
loss:  1150.7408640548156
loss:  1172.6310635809934
loss:  1125.2972062466094
loss:  1171.1716520021077
loss:  1169.6575645164176
loss:  1147.725087138614
loss:  1172.5551111097554
loss:  1149.0126629366914
loss:  1164.6753200513497
loss:  1166.2178216988157
loss:  1149.0041987768025
loss:  1173.9742579414997
loss:  1169.1541900753903
loss:  1170.5403293608222
loss:  1148.570840271985
loss:  1160.4855966129362
loss:  1169.6359169114683
loss:  1166.7639228522892
loss:  1167.968979049748
loss:  1168.504558993439
loss:  1149.9935979196034
loss:  1138.269306750179
loss:  1172.6549468780208
loss:  1098.1231851793916
loss:  1174.3353352082067
loss:  1168.5548084498662
loss:  1166.160717919229
loss:  1162.2994763791255
loss:  1166.7062176713887
loss:  1171.5370858763454
loss:  1147.5961401662887
loss:  1148.151986017652
loss:  1149.6821245042304
loss:  1174.0649078833715
loss:  1171.888442258337
loss:  1163.9554395252078
loss:  1160.2111022976112
loss:  1168.311826789793
loss:  1150.907585151173
loss:  1108.394870826061
loss:  1174.3687478564202
loss:  1169.6213093070664
loss:  1174.1306283307847
loss:  1151.3486809224403
loss:  1098.3790110895548
loss:  1174.663207617332
loss:  1143.826294168704
loss:  1154.0622349250684
loss:  1101.9086212009274
loss:  1172.9880608943229
loss:  1156.297796197851
loss:  1174.2044210885688
loss:  1139.5422514213544
loss:  1170.4221608951943
loss:  1169.244764895599
loss:  1167.9798474465206
loss:  1173.4234728782162
loss:  1174.164296023226
loss:  1168.5655122746086
loss:  1137.0699128907372
loss:  1166.6254110500843
loss:  1169.069216250722
loss:  1168.5465687648834
loss:  1109.9546374150304
loss:  1132.156264553741
loss:  1163.9519029421888
loss:  1163.9481167498443
loss:  1176.0002283171034
loss:  1173.5170964137462
loss:  1171.2249161203483
loss:  1153.284039041747
loss:  1152.1817638346145
loss:  1171.0314209040748
loss:  1157.7674410057923
loss:  1175.9763659749535
loss:  1167.362847935783
loss:  1173.9527366321652
loss:  1175.9742524742471
loss:  1178.921203105877
loss:  1163.643381552415
loss:  1143.033425169921
loss:  1178.860947019729
loss:  1146.0447988138887
loss:  1151.618885220112
loss:  1171.163424669512
loss:  1138.4971866743979
loss:  1171.9446098549033
loss:  1171.702211105922
loss:  1151.2816806015778
loss:  1177.7263375336156
loss:  1174.1191181183317
loss:  1174.1749033026215
loss:  1139.1783750643792
loss:  1096.5388624405218
loss:  1172.8896482046248
loss:  1102.3532287883722
loss:  1169.7751632235954
loss:  1173.3040234839996
loss:  1175.8286589941329
loss:  1175.0835661375907
loss:  1151.6038371446207
loss:  1151.6789306548187
loss:  1173.29270185345
loss:  1105.3201075079846
loss:  1105.1010103983704
loss:  1154.3447642432768
loss:  1175.4285882317135
loss:  1171.7066014117042
loss:  1133.5064398804468
loss:  1141.9061761015423
loss:  1172.525935190978
loss:  1150.8242435825498
loss:  1163.418009799553
loss:  1105.0062373581081
loss:  1144.8540064840106
loss:  1151.7055672845395
loss:  1173.0407797961238
loss:  1150.4299428568486
loss:  1167.7750692599575
loss:  1169.6749891584484
loss:  1166.9822086389431
loss:  1166.7925144055512
loss:  1096.984386453225
loss:  1170.365105656518
loss:  1141.8314126962973
loss:  1166.620211089657
loss:  1175.6901229986663
loss:  1143.1279423467743
loss:  1147.7883557866708
loss:  1099.7269770558175
loss:  1128.0272361872453
loss:  1157.0331017478848
loss:  1170.823490192255
loss:  1151.1670242563582
loss:  1171.724063614395
loss:  1105.1576560968706
loss:  1171.3623898543246
loss:  1172.4558288397056
loss:  1144.6343583081762
loss:  1171.0144611407507
loss:  1172.0421624183557
loss:  1169.9756415184343
loss:  1149.455647527762
loss:  1171.6903568243265
loss:  1161.419813239778
loss:  1099.0717761880846
loss:  1171.7219901581561
loss:  1155.828810391685
loss:  1173.7499279845288
loss:  1171.657294766703
loss:  1168.866821165096
loss:  1175.1977194392148
loss:  1165.5043598024622
loss:  1150.7976652943978
loss:  1152.1495830983147
loss:  1129.0001900051786
loss:  1162.6132998721328
loss:  1098.3321322515344
loss:  1140.268709233727
loss:  1163.4607827905593
loss:  1138.9759382332895
loss:  1175.2981719105542
loss:  1161.995450094895
loss:  1141.8304971911218
loss:  1143.3719143551355
loss:  1158.46746154761
loss:  1170.022367409379
loss:  1170.2922975476672
loss:  1160.6983403122476
loss:  1176.2082281668866
loss:  1169.4110354706006
loss:  1149.8019082705343
loss:  1174.5781233309567
loss:  1172.3916804893602
loss:  1176.1482543935997
loss:  1136.171157366522
loss:  1173.8075582244048
loss:  1161.1563522362278
loss:  1167.862015516635
loss:  1153.6767329923807
loss:  1166.6151026289795
loss:  1162.5115697697445
loss:  1174.2266806707798
loss:  1174.1321038313558
loss:  1165.58164407252
loss:  1172.3120839765163
loss:  1169.2012687076212
loss:  1172.9089713849262
loss:  1144.735367490844
loss:  1173.2294005446108
loss:  1172.8695954663326
loss:  1109.6533149527872
loss:  1143.3477591240185
loss:  1176.7899884291403
loss:  1139.4719578083868
loss:  1160.1404993518897
loss:  1172.435995626384
loss:  1172.4046934255007
loss:  1170.9562246077157
loss:  1139.2889732718088
loss:  1170.7153667151788
loss:  1173.4251366596432
loss:  1146.016724560423
loss:  1171.8964209836427
loss:  1105.231171977145
loss:  1164.0643997589398
loss:  1168.9436700869765
loss:  1167.3237404690092
loss:  1130.4341356007951
loss:  1168.9745953139193
loss:  1098.7040056435062
loss:  1168.632579767602
loss:  1162.732693109458
loss:  1171.1874528160138
loss:  1168.54415304388
loss:  1166.2443640623255
loss:  1102.1793670894672
loss:  1164.147172222144
loss:  1155.9707155517012
loss:  1175.2863039520385
loss:  1172.5248970550083
loss:  1100.512479337212
loss:  1147.286114259217
loss:  1165.5045373389144
loss:  1171.1305642702562
loss:  1169.5780105797514
loss:  1166.7043129678264
loss:  1172.1422187676937
loss:  1146.7608148673714
loss:  1173.0248237171877
loss:  1173.1963317469106
loss:  1165.484655662532
loss:  1165.7260611399286
loss:  1164.472046279643
loss:  1117.8277993889883
loss:  1097.3149233149836
loss:  1131.625555464003
loss:  1172.5872350607906
loss:  1143.3839482419528
loss:  1172.43160947534
loss:  1143.8736534755465
loss:  1167.857417754293
loss:  1130.445408418707
loss:  1144.1120744270827
loss:  1173.2670986456187
loss:  1143.0570339109172
loss:  1163.44159460828
loss:  1174.5024497706245
loss:  1168.8071648157836
loss:  1148.4818693891207
loss:  1132.294295098542
loss:  1153.3870874060224
loss:  1175.2443579788285
loss:  1175.6181300727653
loss:  1164.84078890899
loss:  1169.6162553517333
loss:  1137.4077227113917
loss:  1176.8059012961633
loss:  1151.4945756568022
loss:  1175.049292462622
loss:  1169.894518115647
loss:  1151.83773001478
loss:  1173.176331552908
loss:  1170.2963359741932
loss:  1173.2599272212733
loss:  1171.7916823993905
loss:  1171.3103594720021
loss:  1172.4083820299281
loss:  1097.3157516764547
loss:  1173.39663810918
loss:  1153.9361227832226
loss:  1098.2374220164088
loss:  1171.5878799974482
loss:  1172.6353626390585
loss:  1172.7244613998569
loss:  1157.527086109359
loss:  1109.9045679839346
loss:  1170.4702754104017
loss:  1162.9245828919877
loss:  1174.1321490539444
loss:  1173.2982255023192
loss:  1105.3317461510933
loss:  1167.3362764549051
loss:  1177.7305393617298
loss:  1147.767504531366
loss:  1127.2078305513228
loss:  1149.722245923496
loss:  1148.8157784260081
loss:  1110.1777622174347
loss:  1159.720288544459
loss:  1171.1539771548903
loss:  1156.3506070552316
loss:  1141.6445081348822
loss:  1109.670440318852
loss:  1174.6101795511063
loss:  1167.7740708013555
loss:  1170.0379411331437
loss:  1169.6880950807583
loss:  1170.1337012158735
loss:  1172.6338659605312
loss:  1171.8300826445045
loss:  1176.2678047937306
loss:  1166.628773445196
loss:  1141.323559295482
loss:  1165.1261488528262
loss:  1173.313699312517
loss:  1096.64567668924
loss:  1155.0789903970826
loss:  1173.2428357057677
loss:  1166.753353513503
loss:  1172.500977839716
loss:  1150.7703226036522
loss:  1109.9427840826738
loss:  1132.5236195675
loss:  1166.6418497217292
loss:  1098.3294774340684
loss:  1170.1186975908242
loss:  1174.6414453884329
loss:  1149.4899827496772
loss:  1098.9293665212158
loss:  1164.854941391783
loss:  1099.4427511544861
loss:  1169.8748903265846
loss:  1097.6550813942647
loss:  1168.6976108260621
loss:  1150.2223701843407
loss:  1162.6069363304234
loss:  1173.0124967153895
loss:  1174.8721700552978
loss:  1097.6534574857365
loss:  1169.0624529207853
loss:  1167.8109523325968
loss:  1174.938087104604
loss:  1167.2221634219334
loss:  1165.3862161750817
loss:  1146.7469640191684
loss:  1098.4541799818714
loss:  1120.7670529318752
loss:  1166.0365470599988
loss:  1106.5708914312877
loss:  1105.2135010874715
loss:  1170.0278911178339
loss:  1109.8223837757555
loss:  1166.137090248591
loss:  1153.9632956817
loss:  1102.2594555060791
loss:  1157.8824059989122
loss:  1098.2291828452167
loss:  1174.766728424882
loss:  1144.0914219053673
loss:  1152.69109418335
loss:  1173.1112092046533
loss:  1159.1657486450183
loss:  1148.001034306752
loss:  1147.7472613933874
loss:  1169.8594394813838
loss:  1165.4864511548615
loss:  1173.2959347894812
loss:  1172.236788036898
loss:  1139.106907586613
loss:  1171.8659480873976
loss:  1164.1373975211686
loss:  1167.8901423026896
loss:  1149.8108087908702
loss:  1166.6584343785983
loss:  1167.3083263200897
loss:  1169.40970378156
loss:  1167.8794207688527
loss:  1171.083093750786
loss:  1172.6377596592968
loss:  1177.8100757715322
loss:  1132.8722595060885
loss:  1098.0606119018705
loss:  1109.7209254701809
loss:  1175.4330106329874
loss:  1169.9914524656326
loss:  1166.1417701649418
loss:  1099.2465257516128
loss:  1166.758537205841
loss:  1105.1546253421682
loss:  1132.5221198825018
loss:  1158.0162842241903
loss:  1147.085032845586
loss:  1170.651311326358
loss:  1173.0406861123759
loss:  1126.5247150832759
loss:  1151.5956199747466
loss:  1165.4008611756203
loss:  1138.2889795214328
loss:  1142.6881449979105
loss:  1171.1343370896727
loss:  1151.0739383987243
loss:  1167.9341436277014
loss:  1168.3197481667323
loss:  1171.0918247140744
loss:  1169.395165746318
loss:  1173.2064758338174
loss:  1169.8760921384264
loss:  1095.6773665605149
loss:  1100.784917414374
loss:  1168.2949005736152
loss:  1164.2880356920566
loss:  1166.37276031926
loss:  1142.9351135759182
loss:  1174.6476724286656
loss:  1174.226939755536
loss:  1164.8346459580864
loss:  1170.967155142164
loss:  1154.0247189989034
loss:  1145.7285312632225
loss:  1142.1780346431785
loss:  1163.772012584756
loss:  1167.8523306739119
loss:  1095.157092074991
loss:  1158.476345135744
loss:  1167.8918849418883
loss:  1139.421192823296
loss:  1125.708070668516
loss:  1153.8638831208243
loss:  1130.0823096367023
loss:  1103.83177294965
loss:  1146.7784362699342
loss:  1095.3267590407681
loss:  1171.2309003831256
loss:  1137.2744080373498
loss:  1100.4617382828626
weights:  [0.0003034  0.00077697 0.00087249 ... 0.00092401 0.0005113  0.00012043]
cy dot constraint : -5.768669561862874
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 1095.157092074991
   maxcv: 5.768769561862873
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 600
  status: 4
 success: False
       x: array([0.0003034 , 0.00077697, 0.00087249, ..., 0.00092401, 0.0005113 ,
       0.00012043])
Train data:
------------
Train accuracy :  0.7778571428571428
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 684 (15%)
Protected in positive class: 204 (9%)
P-rule is: 61%

Test data: 
------------
Test accuracy :  0.7733333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 303 (15%)
Protected in positive class: 98 (10%)
P-rule is: 70%
------------------------------------------------------------------------
------------------------------------------------------------------------
