iter:  300.0 , lambda:  1 , alpha:  0.1111111111111111 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00016479 0.00097073 0.00048047 ... 0.00022499 0.00076113 0.00033331]
loss:  218.10114977403882
loss:  233.58993455863626
loss:  233.25803359393817
loss:  234.0953684753063
loss:  231.59443686179984
loss:  232.841012998567
loss:  235.03047433139474
loss:  235.3286812952043
loss:  223.84100984759257
loss:  217.92616251655136
loss:  234.8566183312648
loss:  217.80193428378746
loss:  234.42652362354482
loss:  231.03532290043216
loss:  229.43850838080158
loss:  233.1453385777636
loss:  224.52338515874644
loss:  234.3769993187115
loss:  233.48762193280697
loss:  234.67252077951204
loss:  233.64221394446534
loss:  234.177881880957
loss:  217.9495164694994
loss:  217.32329574081677
loss:  233.40234093233764
loss:  233.81597695603446
loss:  223.17221528202123
loss:  232.6278257066832
loss:  223.01527356951473
loss:  233.74506992247612
loss:  230.19389557258262
loss:  230.65201767302975
loss:  232.47469887924953
loss:  231.40439726448838
loss:  230.0645630293887
loss:  219.13158985995986
loss:  218.20440584249215
loss:  231.56472178855
loss:  221.2620329922998
loss:  233.4730281061563
loss:  218.75483444141324
loss:  233.67589778748956
loss:  228.83285944088703
loss:  230.74924579696196
loss:  234.5345967945753
loss:  233.64984356455915
loss:  233.01672367921591
loss:  231.27537524751608
loss:  233.02969645355955
loss:  228.2787119843165
loss:  232.85017778875653
loss:  233.89213759403805
loss:  232.97239794544421
loss:  233.052884029659
loss:  225.45677323734134
loss:  221.30836591826167
loss:  217.35103496827153
loss:  232.5049751570506
loss:  233.56214927321508
loss:  234.08176793606862
loss:  226.62715884824297
loss:  218.01241035978694
loss:  217.4220523506097
loss:  232.53374420382696
loss:  232.7601425360116
loss:  222.82018697039675
loss:  217.0743230963139
loss:  233.0018837140894
loss:  233.0224384366885
loss:  233.0540247581614
loss:  231.84253771352792
loss:  233.01517956013217
loss:  230.11634701663766
loss:  231.86602491263244
loss:  232.075894514272
loss:  232.72639115662687
loss:  217.5830378960554
loss:  225.32622911705457
loss:  233.35526551956985
loss:  231.6033404850823
loss:  232.8527855289938
loss:  232.46580691663152
loss:  224.644261550517
loss:  218.87993627348254
loss:  232.99763408686067
loss:  233.02986991681033
loss:  217.01489822173855
loss:  231.93439690083716
loss:  232.0093375908625
loss:  231.77371734873805
loss:  230.82172430018267
loss:  232.21765671479125
loss:  224.24032992547944
loss:  225.32133918580845
loss:  232.35505335370033
loss:  224.78932872673465
loss:  231.28950083518146
loss:  231.29398180814854
loss:  232.02094800072177
loss:  232.1499290264397
loss:  232.22232347526943
loss:  228.90325621851093
loss:  232.0953000187092
loss:  221.34305920934437
loss:  230.09749279484663
loss:  232.22812758319503
loss:  226.99667751772816
loss:  232.22725360839607
loss:  232.1629733949506
loss:  232.8832953370592
loss:  230.93892667794753
loss:  231.85706752350478
loss:  218.91791763976696
loss:  231.2597170907974
loss:  231.0012922731898
loss:  230.7270092944323
loss:  232.0490910349598
loss:  232.08032394390438
loss:  230.69117364455886
loss:  232.19674708325385
loss:  230.1071215654828
loss:  229.72212667953445
loss:  230.68859108865118
loss:  223.13331578489255
loss:  226.07400284078764
loss:  231.82134535674666
loss:  232.86966706552678
loss:  232.20898307140016
loss:  231.5059908436293
loss:  229.46990328425179
loss:  230.6727989401175
loss:  228.6054208255971
loss:  228.75439565803998
loss:  229.41399336786537
loss:  232.98864679581143
loss:  219.34995771433665
loss:  232.22450447759064
loss:  230.74959464454548
loss:  219.5441004591089
loss:  231.65760935387493
loss:  230.94701534796798
loss:  224.5917277276503
loss:  230.30594571984537
loss:  231.88171235209248
loss:  231.9430680609504
loss:  230.6498765349313
loss:  230.9290848770557
loss:  230.27400717218578
loss:  232.28436249880184
loss:  232.05641247381294
loss:  229.82683001731115
loss:  225.55928399348298
loss:  232.75333005657708
loss:  230.88263253094374
loss:  224.28494233265053
loss:  230.5610234661501
loss:  231.43445652759502
loss:  230.931654721969
loss:  231.59287168216747
loss:  230.89830833594777
loss:  230.28746387288038
loss:  232.06020274324848
loss:  231.91610030832388
loss:  228.7536302731587
loss:  226.94668576345964
loss:  232.17017088737555
loss:  231.37411632577872
loss:  217.8706823900797
loss:  231.869593698117
loss:  229.42514351571896
loss:  225.63193297031341
loss:  232.4191890922622
loss:  231.36191061318667
loss:  232.46414475318363
loss:  225.4331623236642
loss:  231.87628746381102
loss:  220.58527762213615
loss:  231.58501238717704
loss:  231.28362750191067
loss:  224.86628347323605
loss:  231.8594604177783
loss:  227.14193938667597
loss:  230.28483621233576
loss:  230.52593652709209
loss:  227.1404698195886
loss:  232.1254271818695
loss:  231.18317687314763
loss:  231.45863886350938
loss:  225.0213766997648
loss:  229.44736952519722
loss:  231.27893526646352
loss:  230.7043376227733
loss:  230.9458268571098
loss:  231.030275269973
loss:  227.3398928503777
loss:  223.11765334136123
loss:  231.87922717060417
loss:  216.83558339535256
loss:  230.83386778800718
loss:  229.69026435662008
loss:  229.21620926691364
loss:  228.4564217791975
loss:  229.32366572587924
loss:  230.2816733319672
loss:  225.51124344177822
loss:  225.13271784481276
loss:  225.9281072310072
loss:  230.78279268396614
loss:  230.3813872563715
loss:  228.7786260351777
loss:  228.02062144695773
loss:  229.64792063826246
loss:  225.6628219503029
loss:  218.3972968680521
loss:  230.84278000212507
loss:  229.90134378488037
loss:  230.79454774286046
loss:  225.7840034010508
loss:  217.5131400717003
loss:  230.9007538133312
loss:  224.2849167082369
loss:  226.81222313509267
loss:  217.29386270168024
loss:  230.56768406993694
loss:  227.24870588938975
loss:  230.80844829025335
loss:  223.46570790797978
loss:  230.05419445760663
loss:  229.82843804567517
loss:  229.57628251467202
loss:  230.65641528220527
loss:  230.80065616954946
loss:  229.692459796209
loss:  223.43005679432972
loss:  229.30662054732778
loss:  229.7911882028888
loss:  229.6882755109853
loss:  219.43260745103797
loss:  222.08115244190864
loss:  228.7779083099955
loss:  228.77713313235276
loss:  231.1728121563056
loss:  230.6740433918816
loss:  230.2489070371742
loss:  226.64638880310133
loss:  226.42182749378046
loss:  230.17862337492178
loss:  227.53936567329393
loss:  231.18911061691549
loss:  229.4536189919547
loss:  230.76027808260847
loss:  231.17143237412063
loss:  231.77219734971015
loss:  228.6990886342332
loss:  224.1984257768361
loss:  231.76091339996427
loss:  225.20555319674688
loss:  226.31792652409388
loss:  230.2026524782633
loss:  223.29107882285098
loss:  230.36251864264258
loss:  230.31595984583694
loss:  225.73900937603548
loss:  231.52893937989865
loss:  230.79313510561255
loss:  230.80227554762882
loss:  223.40247074837737
loss:  216.39121562775628
loss:  230.13352092891282
loss:  217.29470917267506
loss:  229.51702286117444
loss:  230.22938047972002
loss:  230.72281234831647
loss:  230.57277351404477
loss:  225.41251806483152
loss:  225.43191690563992
loss:  230.21218629866846
loss:  217.9700108732723
loss:  217.92384732327287
loss:  225.98247813080368
loss:  230.64321781006248
loss:  229.89964670981715
loss:  221.9646772716819
loss:  223.6125363335288
loss:  230.06132368708424
loss:  225.24387291799846
loss:  228.25493476451538
loss:  217.9033823442961
loss:  224.56592052165033
loss:  225.46785636232084
loss:  230.16200640253098
loss:  225.65868973162225
loss:  229.11855914936476
loss:  229.4900414228501
loss:  228.96787449745287
loss:  228.91207337648794
loss:  216.09125542648317
loss:  229.1545586640188
loss:  223.3882380345145
loss:  228.30886030931038
weights:  [0.00016479 0.00097073 0.00048047 ... 0.00022499 0.00076113 0.00033331]
cy dot constraint : -7.788793041465342
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 216.09125542648317
   maxcv: 7.788893041465342
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 300
  status: 4
 success: False
       x: array([0.00016479, 0.00097073, 0.00048047, ..., 0.00022499, 0.00076113,
       0.00033331])
Train data:
------------
Train accuracy :  0.7802857142857142
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 634 (13%)
Protected in positive class: 191 (8%)
P-rule is: 62%

Test data: 
------------
Test accuracy :  0.7773333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 277 (13%)
Protected in positive class: 88 (9%)
P-rule is: 69%
------------------------------------------------------------------------
------------------------------------------------------------------------
