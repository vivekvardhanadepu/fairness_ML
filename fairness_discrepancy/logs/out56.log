iter:  700.0 , lambda:  1 , alpha:  0.5555555555555556 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00075555 0.00030146 0.00061819 ... 0.00040865 0.00071599 0.00089192]
loss:  1098.0134595417624
loss:  1170.415669569953
loss:  1168.8087247161502
loss:  1172.876306411774
loss:  1160.5908132586933
loss:  1166.7835387286239
loss:  1177.723175696492
loss:  1179.053590374268
loss:  1135.558280573558
loss:  1099.1271570415486
loss:  1177.4777409566973
loss:  1099.3875936575953
loss:  1178.70181247686
loss:  1161.8534811782904
loss:  1154.0230828029753
loss:  1172.325986537314
loss:  1140.8847088630953
loss:  1178.5204985349592
loss:  1174.3526011392476
loss:  1179.9818344795194
loss:  1174.912884424191
loss:  1177.434426663123
loss:  1100.712232416266
loss:  1097.6133044580197
loss:  1175.5423760125157
loss:  1177.6789790885368
loss:  1135.9079215462152
loss:  1171.7279473651017
loss:  1135.0801629139778
loss:  1177.36487369073
loss:  1159.6584961542007
loss:  1162.1407907700361
loss:  1170.9336645862343
loss:  1165.7319038894736
loss:  1159.3887099384453
loss:  1108.4616941806253
loss:  1098.9611041253731
loss:  1166.5162836306365
loss:  1126.1905240904703
loss:  1175.8936157332469
loss:  1106.4575584230997
loss:  1176.9174227693165
loss:  1152.984148820366
loss:  1162.4338725148198
loss:  1181.4726403333866
loss:  1177.5616375068564
loss:  1173.6166195776589
loss:  1164.9921312866604
loss:  1173.6761232662905
loss:  1150.2644487470677
loss:  1172.8213486651173
loss:  1178.0827954464742
loss:  1173.4152549580654
loss:  1174.1202420573422
loss:  1147.7252505232411
loss:  1126.334202404239
loss:  1099.7546184892735
loss:  1171.0925122145413
loss:  1176.3390540294845
loss:  1179.1030013295524
loss:  1142.198440655624
loss:  1102.926889020643
loss:  1098.0103714710567
loss:  1171.3197899778122
loss:  1172.7324326504536
loss:  1134.2984282927814
loss:  1098.3430901325376
loss:  1177.4764406654833
loss:  1177.5752756309848
loss:  1177.766911075676
loss:  1171.675003860223
loss:  1177.5522113060242
loss:  1163.1307360863716
loss:  1172.0365412700464
loss:  1172.8598685454233
loss:  1176.1124499369134
loss:  1102.1110440815357
loss:  1147.5402423467015
loss:  1179.6364589110747
loss:  1171.0481345823312
loss:  1176.820656480887
loss:  1174.8201833551752
loss:  1144.040113166551
loss:  1108.2325402385263
loss:  1177.5013113460877
loss:  1177.6302966592848
loss:  1099.3954842561993
loss:  1176.236126437937
loss:  1176.541922154326
loss:  1175.3642337784577
loss:  1170.6302025542584
loss:  1177.5951631776525
loss:  1141.1294963814753
loss:  1146.8213496070937
loss:  1178.3372588929215
loss:  1144.0496088903476
loss:  1172.9526792248682
loss:  1173.0226911106543
loss:  1176.597005808873
loss:  1177.2400977710488
loss:  1177.6324008982822
loss:  1161.0784929203705
loss:  1176.9674210263827
loss:  1126.0294363903088
loss:  1167.018984782022
loss:  1177.6352326075053
loss:  1151.589898564698
loss:  1177.6376159578097
loss:  1177.3314878761246
loss:  1180.9217640107045
loss:  1171.2405020320255
loss:  1175.783250141819
loss:  1108.4444724127875
loss:  1172.8036795444857
loss:  1171.5636271415299
loss:  1170.2011605285213
loss:  1176.7350250152772
loss:  1176.9083994905898
loss:  1169.9801040381371
loss:  1177.4938531946734
loss:  1167.073432502267
loss:  1165.1562504034928
loss:  1169.9732623785776
loss:  1135.476507205051
loss:  1146.9949708642187
loss:  1175.6514505564999
loss:  1180.8765744057873
loss:  1177.545957162661
loss:  1174.0303399321967
loss:  1163.9400429676791
loss:  1169.8964997200658
loss:  1159.6004675664065
loss:  1160.340240552555
loss:  1163.6065384592948
loss:  1181.5050281508381
loss:  1110.4966464991123
loss:  1177.6201339657125
loss:  1170.3802663307376
loss:  1111.4371823286363
loss:  1174.784096833858
loss:  1171.252701267638
loss:  1143.0024856870514
loss:  1168.0697933381614
loss:  1175.8999268149485
loss:  1176.3170225242793
loss:  1169.773999462309
loss:  1171.1773726133608
loss:  1167.9202519629512
loss:  1177.9212156166875
loss:  1176.8071778436952
loss:  1165.6663928711123
loss:  1147.9715690086553
loss:  1180.266875783701
loss:  1170.92098469492
loss:  1141.5394213091708
loss:  1169.4112603787628
loss:  1173.7135969966216
loss:  1171.190239103026
loss:  1174.4616455719793
loss:  1171.1203674547678
loss:  1167.9785919078063
loss:  1176.7889016861873
loss:  1176.1184424299556
loss:  1160.3354364709298
loss:  1151.3384063303474
loss:  1177.342610375215
loss:  1173.3666627470334
loss:  1103.5518829311052
loss:  1175.8437577248399
loss:  1163.6830601648285
loss:  1144.794725148106
loss:  1178.5907673502218
loss:  1173.3197705778948
loss:  1178.8484444212393
loss:  1147.3800246436551
loss:  1175.87208034816
loss:  1122.134279393522
loss:  1174.4275970776291
loss:  1172.9259498432853
loss:  1144.4114360164283
loss:  1175.7952381627115
loss:  1152.31298567259
loss:  1167.9656034333211
loss:  1169.2332881828447
loss:  1152.304591303748
loss:  1177.136079112396
loss:  1172.4251529893386
loss:  1173.800513716134
loss:  1145.2440850747153
loss:  1163.780454714332
loss:  1172.9040082537185
loss:  1170.0461446799359
loss:  1171.2467675584414
loss:  1171.735774575945
loss:  1153.2974090321047
loss:  1135.0764379781247
loss:  1175.8941561990578
loss:  1098.8723119689148
loss:  1177.5147089662069
loss:  1171.8296930870536
loss:  1169.4456207055123
loss:  1165.3020180049796
loss:  1169.9896690289022
loss:  1174.7906190328395
loss:  1150.879479488374
loss:  1144.8311406003738
loss:  1152.9812437429089
loss:  1177.2822481254484
loss:  1175.1097108253525
loss:  1167.2475139366832
loss:  1163.499290876514
loss:  1171.5850700723786
loss:  1147.5418396385867
loss:  1108.8062868523396
loss:  1177.5760655337863
loss:  1172.8894641837978
loss:  1177.3418330845761
loss:  1147.9630147295998
loss:  1099.2556704672104
loss:  1177.849004039417
loss:  1140.5280128984275
loss:  1157.090412439262
loss:  1102.6017405010593
loss:  1176.1068146385828
loss:  1159.6043194497838
loss:  1177.361959902356
loss:  1136.2653785822088
loss:  1173.662001865031
loss:  1172.2994912916586
loss:  1171.257492662013
loss:  1176.5715825351358
loss:  1177.3151032680043
loss:  1171.8403517313284
loss:  1140.339323050584
loss:  1169.9076003776947
loss:  1172.3392704122246
loss:  1171.8212232720505
loss:  1111.2602021786572
loss:  1128.9010847029856
loss:  1167.2440541402373
loss:  1167.240157104797
loss:  1179.2124173856917
loss:  1176.7447277885235
loss:  1174.4494820502964
loss:  1156.5884778016382
loss:  1155.4720428494281
loss:  1174.1201320993316
loss:  1161.0709506623987
loss:  1179.17549615994
loss:  1170.6435009357137
loss:  1177.1708068286555
loss:  1179.064074622017
loss:  1182.0729797397364
loss:  1166.8792106422543
loss:  1139.778441243334
loss:  1182.0123977447906
loss:  1149.3370083017712
loss:  1154.9253053309485
loss:  1174.3656861803559
loss:  1135.1872626334873
loss:  1175.1942766833595
loss:  1174.9570555187638
loss:  1147.907534159917
loss:  1180.9194818387002
loss:  1177.3321119191846
loss:  1177.332220277061
loss:  1135.9079770112892
loss:  1097.4757751398345
loss:  1176.0833871899774
loss:  1103.441782672985
loss:  1173.0132135483286
loss:  1176.5212321864374
loss:  1178.971470883962
loss:  1178.2428391256808
loss:  1148.2599321566186
loss:  1148.3319998059246
loss:  1176.4484509273268
loss:  1106.4171445472075
loss:  1106.1984212551029
loss:  1150.993261237941
loss:  1178.601258969449
loss:  1174.9240983440232
loss:  1130.260131025232
loss:  1138.6978325859036
loss:  1175.7295600463772
loss:  1147.4981292750226
loss:  1166.6876667056267
loss:  1106.1032654595779
loss:  1147.8889717638363
loss:  1148.3460907376398
loss:  1176.2230541097326
loss:  1153.702889542748
loss:  1171.0260754656942
loss:  1172.8863910095401
loss:  1170.2363381147923
loss:  1169.9931294689593
loss:  1097.8710205108914
loss:  1173.4218596556902
loss:  1138.624327246699
loss:  1169.8774013733134
loss:  1178.874911936433
loss:  1139.8316832656012
loss:  1151.0674501093035
loss:  1100.5225776323311
loss:  1125.0971077887334
loss:  1160.2608112781472
loss:  1174.0532369793132
loss:  1147.8340270403874
loss:  1174.9413933582555
loss:  1106.2547807038293
loss:  1174.4547092317691
loss:  1175.574726632022
loss:  1141.3924495865872
loss:  1174.2402720390996
loss:  1175.2554145364477
loss:  1173.2133461850613
loss:  1152.6649834953182
loss:  1174.920308440725
loss:  1164.6889791312467
loss:  1100.1032409844424
loss:  1174.8201306261133
loss:  1159.1134572520605
loss:  1176.960858233673
loss:  1174.7752949562484
loss:  1172.1120464840283
loss:  1178.3762126291815
loss:  1168.768059252656
loss:  1153.8362474950193
loss:  1155.4144157128965
loss:  1125.8248929091103
loss:  1165.7438054131987
loss:  1099.0373431025203
loss:  1137.0831910635388
loss:  1166.7311317424937
loss:  1142.2363375276896
loss:  1178.4713840402264
loss:  1165.269014296859
loss:  1144.998083025152
loss:  1140.1451545494338
loss:  1161.7440397842379
loss:  1173.2260386643416
loss:  1173.52688782803
loss:  1163.5434342475914
loss:  1179.3869558223937
loss:  1172.6532095838245
loss:  1153.0764675021376
loss:  1177.7841564867404
loss:  1175.5941130478848
loss:  1179.3236441344845
loss:  1139.4210374638442
loss:  1176.9841063701845
loss:  1164.4312325289195
loss:  1171.1141425765425
loss:  1156.7954252110314
loss:  1169.871095570843
loss:  1165.7822005872042
loss:  1177.43603702025
loss:  1177.2277804852456
loss:  1168.8105862482112
loss:  1175.5205764088068
loss:  1172.2552900205562
loss:  1176.1028211948258
loss:  1141.459105544586
loss:  1176.4340056015703
loss:  1176.063472197099
loss:  1110.907681736767
loss:  1140.1212609809086
loss:  1179.9109107080508
loss:  1136.229221783424
loss:  1163.4143652546975
loss:  1175.555021745976
loss:  1175.6124897706309
loss:  1174.1815937651425
loss:  1136.0490766709877
loss:  1173.9457622246196
loss:  1176.5916010572773
loss:  1142.7571543583076
loss:  1174.9158696411876
loss:  1106.3281382664634
loss:  1167.3314797591697
loss:  1171.9928420478311
loss:  1170.579038285985
loss:  1127.2373896377987
loss:  1172.2221040765717
loss:  1099.4566163150237
loss:  1171.716077561066
loss:  1165.8625871856502
loss:  1174.4083522142378
loss:  1171.7922434563768
loss:  1169.3676250310066
loss:  1103.2676649987004
loss:  1167.4155647399593
loss:  1159.24347242136
loss:  1178.4605581260616
loss:  1175.7487542912163
loss:  1101.52575971801
loss:  1143.9737778162876
loss:  1168.768228110002
loss:  1174.3558013673432
loss:  1172.6970635756963
loss:  1169.9561338457236
loss:  1175.3547295918609
loss:  1143.4930815331975
loss:  1176.0662224790462
loss:  1176.3443144863922
loss:  1168.7481739094474
loss:  1168.7275912257903
loss:  1167.7138398592826
loss:  1114.9338092060884
loss:  1098.1673297410132
loss:  1128.4106261802076
loss:  1175.7849204780791
loss:  1140.0830809078184
loss:  1175.5505235217126
loss:  1140.6415812692408
loss:  1171.1092363564135
loss:  1127.2485988468607
loss:  1140.7929712537932
loss:  1176.4211172235625
loss:  1139.8032552242223
loss:  1166.7116980765347
loss:  1177.6102445840668
loss:  1172.049016517378
loss:  1151.7597057812948
loss:  1129.0687110410047
loss:  1156.6665829557685
loss:  1178.4114015537025
loss:  1178.8069288089305
loss:  1168.1068918759902
loss:  1172.7414103794906
loss:  1134.1517752136356
loss:  1179.9707875802176
loss:  1148.151392240901
loss:  1178.2465784485585
loss:  1173.1328489949658
loss:  1148.4837651247074
loss:  1176.3262079555152
loss:  1173.5307375054426
loss:  1176.4426836279922
loss:  1174.8899632659472
loss:  1174.4061602538304
loss:  1175.597551531962
loss:  1098.006843107125
loss:  1176.5516509794418
loss:  1150.5938529260611
loss:  1099.0716036149886
loss:  1174.6285759189261
loss:  1175.8383843943311
loss:  1175.9233767017568
loss:  1160.5610747534804
loss:  1111.166478018205
loss:  1173.702992192878
loss:  1166.1089101013097
loss:  1177.3111926639147
loss:  1176.4708385339393
loss:  1106.4287728595957
loss:  1170.591685992018
loss:  1180.8306734175248
loss:  1144.4861843716355
loss:  1124.0170757440665
loss:  1152.9956061901403
loss:  1145.4957864473604
loss:  1111.4410956101487
loss:  1162.9705257846938
loss:  1174.388743672862
loss:  1159.4206527032811
loss:  1144.6383790670545
loss:  1110.9244988312817
loss:  1177.814124330102
loss:  1171.0253145422605
loss:  1173.2776267465172
loss:  1172.9319831558175
loss:  1173.3751733548852
loss:  1175.836875135577
loss:  1175.0269920432345
loss:  1179.4110096694762
loss:  1169.8857983432597
loss:  1138.0551775786314
loss:  1168.2211940818715
loss:  1176.5307940841615
loss:  1097.571166700966
loss:  1158.3624599281595
loss:  1176.425206367394
loss:  1170.0097420107757
loss:  1175.7029422485887
loss:  1147.4451295812607
loss:  1111.2054354190498
loss:  1129.336683998319
loss:  1169.8931349722925
loss:  1099.333749366808
loss:  1173.3583953957007
loss:  1177.8344545036177
loss:  1146.1843945788912
loss:  1099.9519838373183
loss:  1168.1207526306036
loss:  1100.207628953578
loss:  1173.113349681134
loss:  1098.48924089498
loss:  1171.9265394779395
loss:  1146.9221482715961
loss:  1165.8782561518385
loss:  1176.2056427841005
loss:  1177.9871104931285
loss:  1098.4876960786203
loss:  1172.307274421146
loss:  1171.0625758959902
loss:  1177.9624224188058
loss:  1170.4764131729144
loss:  1168.6487944641096
loss:  1143.4419439796627
loss:  1099.248425944603
loss:  1117.8175610552362
loss:  1169.2869670410898
loss:  1107.037112213635
loss:  1106.3104833189545
loss:  1173.1165177511707
loss:  1110.1061423013462
loss:  1169.366555930333
loss:  1156.9845084429828
loss:  1103.3477231082525
loss:  1161.1639899825416
loss:  1098.918725112343
loss:  1177.9455211421998
loss:  1140.8559244151256
loss:  1155.901452439341
loss:  1176.2766452544188
loss:  1162.4396128723179
loss:  1144.6509730988469
loss:  1144.4662022660546
loss:  1172.9233220208182
loss:  1168.7502472112228
loss:  1176.4631677527213
loss:  1175.3491080245326
loss:  1142.3596980456157
loss:  1174.9750230783816
loss:  1167.4056004250053
loss:  1171.1404599758635
loss:  1153.085339365739
loss:  1169.729823035051
loss:  1170.563827883312
loss:  1172.6516422102864
loss:  1171.13162701898
loss:  1174.309460050108
loss:  1175.8363170162606
loss:  1180.921305696774
loss:  1129.6379354133494
loss:  1098.9613778763833
loss:  1110.983913350457
loss:  1178.6227097750311
loss:  1173.2290505600151
loss:  1169.402671719434
loss:  1100.2704414758437
loss:  1170.016834069813
loss:  1106.2516682671446
loss:  1129.335361435829
loss:  1160.9904262024859
loss:  1150.3651234704546
loss:  1173.7297406800008
loss:  1176.229366861556
loss:  1123.34687191345
loss:  1148.2532788202313
loss:  1168.663357010369
loss:  1135.0752353563885
loss:  1145.9600247778114
loss:  1174.2191047656563
loss:  1154.3552157357622
loss:  1171.1764346340665
loss:  1171.566667698322
loss:  1174.3181480638768
loss:  1172.6372048623812
loss:  1176.3562679259894
loss:  1173.1144793968667
loss:  1096.6103966794171
loss:  1101.4339841626283
loss:  1171.532756967286
loss:  1167.512973401578
loss:  1169.6237501280689
loss:  1139.6306046415189
loss:  1177.7579888138532
loss:  1177.3618542830068
loss:  1168.0890385806395
loss:  1174.1768160591225
loss:  1157.3066521167136
loss:  1142.4405015018904
loss:  1138.9722119248217
loss:  1167.0343550738964
loss:  1171.0944958366156
loss:  1096.087142746278
loss:  1161.5568183830117
loss:  1170.989845402509
loss:  1136.3947895312228
loss:  1122.5859392888224
loss:  1157.0981461406357
loss:  1126.8931311708754
loss:  1104.9218411701127
loss:  1143.4930602979846
loss:  1096.2704298287288
loss:  1174.4443139071707
loss:  1140.5270269970308
loss:  1101.4921143165777
loss:  1140.6457627858083
loss:  1172.9883776788672
loss:  1172.676072778981
loss:  1168.7436184700803
loss:  1173.355649858418
loss:  1105.0434031113657
loss:  1097.0635735689043
loss:  1140.1088583229018
loss:  1166.986043719115
loss:  1169.3510305669536
loss:  1167.6632495358351
loss:  1153.576061870854
loss:  1174.6130635866998
loss:  1173.9303557508338
loss:  1175.5651467183013
loss:  1170.9271100133087
loss:  1173.668958417019
loss:  1174.030306188012
loss:  1130.9948643019177
loss:  1168.8678856926408
loss:  1140.6824646643258
loss:  1171.6580371036807
loss:  1179.1727226869793
loss:  1096.173729396212
loss:  1108.6008215987406
loss:  1174.510140021456
loss:  1096.892623633873
loss:  1164.171627000129
loss:  1133.326632431538
loss:  1144.8324406291158
loss:  1170.3934911620447
loss:  1169.5861490616867
loss:  1096.0438946656222
loss:  1174.4490511177353
loss:  1096.3455958189825
loss:  1105.6020085841128
loss:  1168.952606920976
loss:  1174.676325152057
loss:  1155.880918826703
loss:  1178.2867429689825
loss:  1096.9988086604767
loss:  1168.675742003456
loss:  1140.909774347829
loss:  1126.7362459696462
loss:  1173.7492742416778
loss:  1166.498922431838
loss:  1139.702576159708
loss:  1171.7613110018267
loss:  1174.0579960596128
loss:  1174.4727308833285
loss:  1165.2644229794691
loss:  1179.2974354121632
loss:  1097.4295132265395
loss:  1168.4548605673506
loss:  1156.250209046238
loss:  1170.0610654350107
loss:  1152.6763064174252
loss:  1161.5788265480528
loss:  1172.4426163751318
loss:  1176.2729290900945
loss:  1100.1665412936452
loss:  1125.707953074737
loss:  1171.0528997393917
loss:  1149.920954452462
loss:  1159.8415264446833
loss:  1131.0881789119712
loss:  1169.817743703491
loss:  1139.458446705142
loss:  1168.9006385174916
loss:  1171.030054182747
loss:  1161.5352481603395
loss:  1165.7158360496728
loss:  1171.3435093689113
loss:  1167.7730862210935
loss:  1110.8107779945603
loss:  1171.042614313712
loss:  1174.2971903978112
loss:  1148.6840913927142
loss:  1155.6644287316155
loss:  1144.1060899919823
loss:  1171.7194055556013
loss:  1174.7227444371379
loss:  1132.88713088567
loss:  1144.7405392108024
loss:  1172.0980973525316
loss:  1172.246944613627
loss:  1173.0294462251056
loss:  1171.0400430183151
loss:  1179.3663565645209
loss:  1144.275739951476
loss:  1176.6871370062643
loss:  1168.5351598022748
loss:  1143.4964434280964
loss:  1165.0962653375439
loss:  1143.5514308200543
loss:  1079.3828664503085
loss:  1068.3798663620905
loss:  1152.033654038523
loss:  1154.6511918574204
loss:  1152.8448334551033
weights:  [0.00075555 0.00030146 0.00061819 ... 0.00040865 0.00071599 0.00089192]
cy dot constraint : -4.7929324983696295
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 1068.3798663620905
   maxcv: 4.793032498369629
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 700
  status: 4
 success: False
       x: array([0.00075555, 0.00030146, 0.00061819, ..., 0.00040865, 0.00071599,
       0.00089192])
Train data:
------------
Train accuracy :  0.7735714285714286
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 715 (15%)
Protected in positive class: 219 (10%)
P-rule is: 63%

Test data: 
------------
Test accuracy :  0.7683333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 315 (15%)
Protected in positive class: 103 (11%)
P-rule is: 71%
------------------------------------------------------------------------
------------------------------------------------------------------------
