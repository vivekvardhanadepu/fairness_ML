iter:  300.0 , lambda:  1 , alpha:  0.3333333333333333 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [3.20076897e-04 9.74132736e-04 7.00976344e-04 ... 9.73541660e-04
 7.63997262e-04 1.24168439e-05]
loss:  658.302751746669
loss:  702.904553699718
loss:  701.9297376017803
loss:  704.3971910714669
loss:  696.9861526443653
loss:  700.7016825403275
loss:  707.3456251831182
loss:  708.1105408362276
loss:  678.5118582887113
loss:  658.7753159491801
loss:  707.1876257091808
loss:  658.8941168951022
loss:  707.9340848804687
loss:  697.7764207319724
loss:  693.0396682259907
loss:  704.0630696722043
loss:  681.6912876349365
loss:  707.8272370644153
loss:  705.305922718988
loss:  708.6653494821228
loss:  705.5898696390357
loss:  707.16193691398
loss:  659.729254288664
loss:  657.8578529211636
loss:  705.8191366118565
loss:  707.1197962995384
loss:  678.5288184154867
loss:  703.5041975558339
loss:  678.0356501221962
loss:  706.93092716714
loss:  696.2487931884884
loss:  697.7720410074241
loss:  703.0665869179257
loss:  699.8679934301466
loss:  696.1026428447377
loss:  664.322191668115
loss:  658.4331855810844
loss:  700.3434491250482
loss:  672.7356769890533
loss:  706.032246006543
loss:  663.0726716907584
loss:  706.6538505451844
loss:  692.2135112108695
loss:  697.9489882791643
loss:  709.3611112342046
loss:  706.9250601445716
loss:  704.6722714422292
loss:  699.4607603606232
loss:  704.7086081921442
loss:  690.5806082374911
loss:  704.1674178184912
loss:  707.3173278858617
loss:  704.5283710049077
loss:  704.9687568380253
loss:  685.6017562811533
loss:  672.8182074068831
loss:  658.8982085253862
loss:  703.1629739698809
loss:  706.301705022986
loss:  707.9149071364086
loss:  685.7045423472305
loss:  660.8815365752062
loss:  658.1111396266186
loss:  703.3040299916472
loss:  704.1279423686099
loss:  677.5956276880149
loss:  658.0736050605029
loss:  706.992994379129
loss:  707.0547377719148
loss:  707.1723367011407
loss:  703.5073543435128
loss:  707.0405035317006
loss:  698.3314887484809
loss:  703.7128762210976
loss:  704.1910580343343
loss:  706.1431708343159
loss:  660.3765001707025
loss:  685.5050016889679
loss:  708.1882744329915
loss:  702.9285957067157
loss:  706.6012102750124
loss:  705.405105846758
loss:  683.3903947782964
loss:  664.1840570529165
loss:  707.0131132630613
loss:  707.0894917890704
loss:  658.6944250004884
loss:  706.2315823811665
loss:  706.4361535300649
loss:  705.7198360338939
loss:  702.839021432815
loss:  707.0695405525673
loss:  681.6461492106025
loss:  685.0666505325592
loss:  707.4857106150389
loss:  683.3959477612594
loss:  704.2468899557405
loss:  704.3192059104929
loss:  706.4583325745076
loss:  706.8454761981808
loss:  707.0358267049002
loss:  697.0862531306462
loss:  706.6846195465404
loss:  672.6357091822056
loss:  700.6640283456159
loss:  707.0916289639212
loss:  691.3671475613045
loss:  707.0683710769091
loss:  706.9103839019123
loss:  709.051912504848
loss:  703.1753844172198
loss:  705.964885250758
loss:  664.3120034356087
loss:  704.1562532629378
loss:  703.3546557920029
loss:  702.6207805382904
loss:  706.5435020976294
loss:  706.6246690624552
loss:  702.4436450127257
loss:  707.0086012764452
loss:  700.7019692367552
loss:  699.53953494766
loss:  702.4945213436496
loss:  678.3184622218554
loss:  688.5951905111936
loss:  705.8955317438492
loss:  709.0190695067424
loss:  707.0368134010378
loss:  704.9005647600037
loss:  698.5408286643235
loss:  702.3933981028098
loss:  696.1896415709494
loss:  696.6385693516173
loss:  698.6403183246912
loss:  709.3619931501732
loss:  665.5349687037377
loss:  707.0821495286888
loss:  702.4904203337899
loss:  666.1014601797857
loss:  705.35888834157
loss:  703.2163361877899
loss:  682.7676404439109
loss:  701.2852509812859
loss:  706.0361454468145
loss:  706.1580719703232
loss:  702.3193078664531
loss:  703.2182023894175
loss:  701.1825942084065
loss:  707.2494754705856
loss:  706.5943750431019
loss:  699.8947037730799
loss:  685.773281707962
loss:  708.6579224247647
loss:  703.0589746918768
loss:  681.898548546549
loss:  702.1138308708328
loss:  704.666560375135
loss:  703.1616513902687
loss:  705.1621036239605
loss:  703.0088104525067
loss:  701.2305526570232
loss:  706.5751707184735
loss:  706.1773951222015
loss:  696.6356320820383
loss:  691.2180816021514
loss:  706.9126473936052
loss:  704.520786474511
loss:  661.2623367552053
loss:  706.0017275358332
loss:  698.6458913479736
loss:  687.2755101078128
loss:  707.6626699266354
loss:  704.4698739184988
loss:  707.8124094198079
loss:  685.4063735523464
loss:  706.019216562016
loss:  670.3338550593278
loss:  705.1411600223092
loss:  704.2313793084045
loss:  683.6170944978804
loss:  705.9719981584251
loss:  691.8059108401684
loss:  701.2227822678789
loss:  702.00659052431
loss:  691.8009241036768
loss:  706.7912697912551
loss:  703.9271029838958
loss:  704.7615219250995
loss:  684.1170351061205
loss:  698.7256177315344
loss:  704.2179924119362
loss:  702.4837789182593
loss:  703.2128048724081
loss:  703.4257434057373
loss:  692.3960240940888
loss:  678.0334013836654
loss:  706.0311769130155
loss:  658.3715513554796
loss:  707.0189730756848
loss:  703.5665161668716
loss:  702.1195173898307
loss:  699.6387891867033
loss:  702.4505797036396
loss:  705.3628015314989
loss:  690.953902384095
loss:  683.8682568704643
loss:  692.211748640835
loss:  706.8751028010727
loss:  705.4717958647755
loss:  700.7870725161956
loss:  698.5616117737162
loss:  703.3956590342015
loss:  685.5113915802672
loss:  664.0878194361048
loss:  707.0540963845142
loss:  704.209410663107
loss:  706.9109591801868
loss:  685.7894425776079
loss:  658.6221471574534
loss:  707.2211466742125
loss:  681.2912085082945
loss:  694.705949968715
loss:  660.4816049250508
loss:  706.1703227550981
loss:  696.192019605515
loss:  706.9292098883449
loss:  678.749512800912
loss:  704.7002270465458
loss:  703.8649368362532
loss:  703.2193307450993
loss:  706.4493980239014
loss:  706.9005133346876
loss:  703.5728467207754
loss:  684.5748486876529
loss:  702.4000390473442
loss:  703.8745838331727
loss:  703.5613429062973
loss:  665.9944856406514
loss:  674.3842498169447
loss:  700.7850419163893
loss:  700.782669572333
loss:  708.0248272469131
loss:  706.5492878623041
loss:  705.0720507919945
loss:  694.3814401398153
loss:  693.7228383915855
loss:  704.9665153299705
loss:  697.0818329296209
loss:  707.9396879981982
loss:  702.8468849014137
loss:  706.8083091386426
loss:  707.9240462075355
loss:  709.7000897976877
loss:  700.625894903379
loss:  680.8384701169556
loss:  709.6632614640696
loss:  690.0139131637801
loss:  693.3724293738296
loss:  705.1284688081447
loss:  678.14887617099
loss:  705.6077322095476
loss:  705.4585738380775
loss:  685.7359910466629
loss:  709.0189090112415
loss:  706.9065137318192
loss:  706.911103792708
loss:  678.5350134982458
loss:  657.564949339817
loss:  705.9536905988077
loss:  660.9858188019617
loss:  704.0893127440015
loss:  706.1739285526503
loss:  707.6828829141244
loss:  707.2488352368473
loss:  685.7581162908791
loss:  685.8048879225738
loss:  706.1778167675243
loss:  662.8378564534382
loss:  662.7062704934858
loss:  687.4004308306876
loss:  707.4599268692709
loss:  705.249160901126
loss:  675.0228781452117
loss:  680.0042378189911
loss:  705.7383589981793
loss:  685.2854496280971
loss:  700.2515320870981
loss:  662.6486614563103
loss:  688.9570725520863
loss:  685.8329515606763
loss:  706.0385224844065
loss:  692.4542990588207
loss:  702.8830219283515
loss:  704.0350103072774
loss:  702.3827629042884
loss:  702.3011371699167
loss:  657.5872319889928
loss:  704.1738850984083
loss:  679.9611350966354
loss:  702.1863134287673
weights:  [3.20076897e-04 9.74132736e-04 7.00976344e-04 ... 9.73541660e-04
 7.63997262e-04 1.24168439e-05]
cy dot constraint : -3.8151694296001497
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 657.564949339817
   maxcv: 3.81526942960015
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 300
  status: 4
 success: False
       x: array([3.20076897e-04, 9.74132736e-04, 7.00976344e-04, ...,
       9.73541660e-04, 7.63997262e-04, 1.24168439e-05])
Train data:
------------
Train accuracy :  0.7764285714285715
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 687 (15%)
Protected in positive class: 211 (9%)
P-rule is: 63%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
