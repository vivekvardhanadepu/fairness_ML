iter:  100.0 , lambda:  1 , alpha:  0.5555555555555556 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [5.93181861e-06 3.64185126e-04 8.33019970e-04 ... 4.04960894e-04
 8.25982439e-04 9.00474903e-04]
loss:  1098.2619844172452
loss:  1171.832936808889
loss:  1170.2136564937857
loss:  1174.3153800899872
loss:  1161.9655215374053
loss:  1168.1730288030844
loss:  1179.299167579637
loss:  1180.5335144162227
loss:  1133.1962610732376
loss:  1099.3777721820547
loss:  1178.9743497540526
loss:  1099.5864997692809
loss:  1180.2323878752968
loss:  1163.2673466858873
loss:  1155.3913058321004
loss:  1173.7595209264105
loss:  1138.5290577183328
loss:  1180.077454723991
loss:  1175.9554212151309
loss:  1181.4687826983113
loss:  1176.3465356732916
loss:  1178.9299062415794
loss:  1100.9789829083927
loss:  1097.8478641024185
loss:  1177.0145103524637
loss:  1179.2176896033682
loss:  1133.5618066112709
loss:  1173.157367231677
loss:  1132.7340111611722
loss:  1178.9159993444862
loss:  1161.0514772924876
loss:  1163.6702791174453
loss:  1172.4203755401898
loss:  1167.1112423201719
loss:  1160.9418980070477
loss:  1108.9684686509374
loss:  1099.14600428108
loss:  1167.901232917184
loss:  1123.8545271872665
loss:  1177.3706691915045
loss:  1106.820276380631
loss:  1178.414690074678
loss:  1154.3420196473253
loss:  1163.8992953627676
loss:  1183.0348971253172
loss:  1179.1311308542147
loss:  1175.096884729286
loss:  1166.4014635738174
loss:  1175.1562418526266
loss:  1151.627401764502
loss:  1174.2615747773198
loss:  1179.5499756682398
loss:  1174.8608006501754
loss:  1175.7079500373522
loss:  1145.4229288153167
loss:  1123.988848101403
loss:  1099.8993495906793
loss:  1172.5841256173878
loss:  1177.8213160458756
loss:  1180.5732589428649
loss:  1143.5319915264622
loss:  1103.2496075967804
loss:  1098.2534546457184
loss:  1172.8505677917115
loss:  1174.3270995451333
loss:  1132.008849648278
loss:  1098.543430347018
loss:  1178.9881482566257
loss:  1179.0916855038195
loss:  1179.30575596515
loss:  1173.150560960291
loss:  1179.0730791225951
loss:  1164.5237034346324
loss:  1173.6248809972917
loss:  1174.3001989973798
loss:  1177.5693878572547
loss:  1102.408901303598
loss:  1145.266080403786
loss:  1181.1918083319304
loss:  1172.4681664275508
loss:  1178.3792396261244
loss:  1176.3431484936946
loss:  1141.7178153545872
loss:  1108.7380914270013
loss:  1179.0483647687374
loss:  1179.1598799940816
loss:  1099.5635770053445
loss:  1177.828791746278
loss:  1178.0557299749873
loss:  1176.8488705407085
loss:  1172.048642166602
loss:  1179.1381354032903
loss:  1138.7920850187284
loss:  1144.5306805244306
loss:  1179.927638823042
loss:  1141.727271986578
loss:  1174.392456357442
loss:  1174.5916764011943
loss:  1178.0844195096265
loss:  1178.7336992689793
weights:  [5.93181861e-06 3.64185126e-04 8.33019970e-04 ... 4.04960894e-04
 8.25982439e-04 9.00474903e-04]
cy dot constraint : -2.8120360710207586
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 1097.8478641024185
   maxcv: 2.812136071020759
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 100
  status: 4
 success: False
       x: array([5.93181861e-06, 3.64185126e-04, 8.33019970e-04, ...,
       4.04960894e-04, 8.25982439e-04, 9.00474903e-04])
Train data:
------------
Train accuracy :  0.776
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 690 (15%)
Protected in positive class: 211 (9%)
P-rule is: 62%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
