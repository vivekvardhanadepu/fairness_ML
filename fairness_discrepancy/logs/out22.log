iter:  300.0 , lambda:  1 , alpha:  0.2222222222222222 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00035304 0.00078305 0.00084558 ... 0.00018089 0.00023833 0.00095234]
loss:  437.74713658440714
loss:  467.0139001606662
loss:  466.36349104216686
loss:  468.00810559082265
loss:  463.0480279806891
loss:  465.5447348799605
loss:  469.9314007821838
loss:  470.49152730210835
loss:  452.31139360738695
loss:  437.89238364675714
loss:  469.8589112894895
loss:  438.0133175877262
loss:  470.3465902520419
loss:  463.55888465631773
loss:  460.39244995596033
loss:  467.785746897084
loss:  454.42817147026386
loss:  470.26354868459356
loss:  468.54880913856147
loss:  470.8626570740321
loss:  468.8172258779804
loss:  469.8420266969433
loss:  438.5159027673387
loss:  437.2906286710239
loss:  468.7882522526139
loss:  469.6340674446924
loss:  452.1446067610398
loss:  467.25010666047854
loss:  451.81603984176706
loss:  469.50181412585664
loss:  462.3794011987538
loss:  463.3407567557415
loss:  466.9252470924557
loss:  464.82705728758754
loss:  462.2089626068579
loss:  441.27485079567293
loss:  437.5461675367837
loss:  465.1442319896343
loss:  448.25011720989113
loss:  468.92934260677555
loss:  440.5232037274301
loss:  469.33850969890244
loss:  459.68000579568275
loss:  463.48768059784487
loss:  471.13991479407173
loss:  469.5073207059533
loss:  468.0105788019433
loss:  464.5350304003963
loss:  468.03507856522174
loss:  458.57712356013946
loss:  467.69132056106935
loss:  469.80317510656766
loss:  467.9318674176222
loss:  468.1672231409826
loss:  456.85426257302333
loss:  448.31788196323043
loss:  437.8857534955043
loss:  466.9878788196318
loss:  469.10804844218205
loss:  470.2060364598868
loss:  455.30836575511205
loss:  439.1157178002567
loss:  437.46087879345816
loss:  467.0657146441806
loss:  467.5989976170924
loss:  451.4754695966833
loss:  437.30766503803835
loss:  469.56006718166805
loss:  469.59989291337814
loss:  469.6691271341036
loss:  467.2276863806787
loss:  469.58824339985773
loss:  463.78301461216597
loss:  467.32113236733767
loss:  467.70726116856264
loss:  469.0154182976459
loss:  438.7564997683831
loss:  456.7731670864606
loss:  470.3614745294954
loss:  466.88420205605905
loss:  469.2768466606559
loss:  468.4848271860291
loss:  455.37659192150807
loss:  441.18337053373415
loss:  469.5587823574498
loss:  469.6175370064307
loss:  437.7307797741761
loss:  469.0169876493793
loss:  469.18387948770896
loss:  468.7148336158918
loss:  466.80777754146897
loss:  469.59847479330443
loss:  454.2188478455152
loss:  456.48505131108175
loss:  469.86434314907706
loss:  455.3802970374874
loss:  467.7447796786178
loss:  467.73490551035417
loss:  469.2111289640656
loss:  469.4696140413349
loss:  469.61886472297635
loss:  462.9531294007469
loss:  469.3592591473294
loss:  448.18924821406546
loss:  465.3486259535337
loss:  469.6227530470716
loss:  459.1160111850977
loss:  469.6284225959865
loss:  469.4861352985753
loss:  470.9379255429424
loss:  467.04355584341073
loss:  468.88395942169063
loss:  441.26795475733815
loss:  467.6839688547324
loss:  467.16691154333466
loss:  466.5896911485745
loss:  469.2665920633416
loss:  469.334287062078
loss:  466.54422256938534
loss:  469.55573169294894
loss:  465.37349519286454
loss:  464.5993486360197
loss:  466.5234446427426
loss:  451.9489049896117
loss:  457.25486764247603
loss:  468.79507438279325
loss:  470.9094953379155
loss:  469.58577844058505
loss:  468.17953627181555
loss:  463.9441179095938
loss:  466.5103563230191
loss:  462.3552318148243
loss:  462.65460967942823
loss:  463.9728386493587
loss:  471.1419025469867
loss:  442.0905566170381
loss:  469.61624891754195
loss:  466.5693447218966
loss:  442.4670924768583
loss:  468.4833713582079
loss:  467.0590135581519
loss:  454.9636837880853
loss:  465.77244478206404
loss:  468.9319770930807
loss:  469.0378159780445
loss:  466.4609296855612
loss:  467.00449695488203
loss:  465.7083699242974
loss:  469.74127702817435
loss:  469.27071481364675
loss:  464.7983475149791
loss:  456.94783426938983
loss:  470.675050695692
loss:  466.9201316911395
loss:  454.3705089180724
loss:  466.2408209401897
loss:  468.0391855629371
loss:  467.02937907108134
loss:  468.3530740459578
loss:  466.872768391277
loss:  465.73584524538353
loss:  469.2877274131696
loss:  468.9857545042106
loss:  462.6526676036091
loss:  459.01382926798783
loss:  469.5083861308015
loss:  467.9110173616999
loss:  439.3417257910006
loss:  468.908745852421
loss:  464.003331520321
loss:  456.3605962275688
loss:  470.00679009892053
loss:  467.89235400324475
loss:  470.0904665167654
loss:  456.70893828568506
loss:  468.92062865116503
loss:  446.62763291431463
loss:  468.33886168068994
loss:  467.73443387858237
loss:  455.52653608692344
loss:  468.8887107297368
loss:  459.408439007875
loss:  465.7306275493961
loss:  466.16914346667147
loss:  459.405134438238
loss:  469.4100760297334
loss:  467.5321098906525
loss:  468.08615791580667
loss:  455.85733585001424
loss:  464.0455948507521
loss:  467.72540685681713
loss:  466.5711438940989
loss:  467.0566766421601
loss:  467.2187850142056
loss:  459.8070167999017
loss:  451.8144712353172
loss:  468.9282425368106
loss:  437.55130481898857
loss:  469.5680794029768
loss:  467.29216641373966
loss:  466.3285628172567
loss:  464.57715069073373
loss:  466.54900421707674
loss:  468.4859841125255
loss:  458.825643416812
loss:  455.6927298427651
loss:  459.67882353617256
loss:  469.48505665608116
loss:  468.5828735506068
loss:  465.44009370393036
loss:  463.9304917056227
loss:  467.1863891638384
loss:  456.7761213147321
loss:  441.58389129286553
loss:  469.60129202148903
loss:  467.71964736798606
loss:  469.5069613923257
loss:  456.9511264297006
loss:  437.6714173180992
loss:  469.70571412941894
loss:  453.96768201613224
loss:  461.2673027870106
loss:  439.0470368167283
loss:  468.98095980538756
loss:  462.3568782808407
loss:  469.50064248203176
loss:  452.2636716309355
loss:  468.0294801455741
loss:  467.42114206211664
loss:  467.0610668056584
loss:  469.1772062475605
loss:  469.4794577169989
loss:  467.29639516457434
loss:  454.55634631239553
loss:  466.5148175731607
loss:  467.49662091163725
loss:  467.2886138866086
loss:  442.3962791549403
loss:  449.3228081779001
loss:  465.4387557824103
loss:  465.437130308773
loss:  470.2607179852445
loss:  469.27044801396124
loss:  468.3164392437437
loss:  461.1381987430375
loss:  460.6847953878174
loss:  468.16703304670114
loss:  462.9501527239255
loss:  470.2246070572454
loss:  466.8129762320783
loss:  469.44037509819935
loss:  470.16121775927024
loss:  471.38590439234724
loss:  465.2873299788491
loss:  453.68203770295173
loss:  471.361241397756
loss:  458.20299669641855
loss:  460.46512982235674
loss:  468.30331406162816
loss:  451.8354527740517
loss:  468.648417496863
loss:  468.55242389884194
loss:  456.92304083694995
loss:  470.93582795213706
loss:  469.50432292929804
loss:  469.48844148982874
loss:  452.1219042807307
loss:  436.9476965243055
loss:  468.71209247060705
loss:  439.009831470885
loss:  467.4806040528803
loss:  468.88509415411994
loss:  469.86188457985804
loss:  469.5734907757108
loss:  456.7612720738515
loss:  456.79103130540346
loss:  468.84920444531116
loss:  440.2192709995937
loss:  440.1312519390657
loss:  457.86354276507984
loss:  469.7211159238164
loss:  468.2490875611335
loss:  449.56648578181324
loss:  452.9517624689731
loss:  468.57143873327544
loss:  456.4530509657796
loss:  464.9262248288105
loss:  440.0931184595694
loss:  457.2681445016053
loss:  456.80233483972927
loss:  468.76495973532576
loss:  459.68358850662344
loss:  466.6786043308717
loss:  467.42637399066246
loss:  466.35427511717614
loss:  466.2549613828814
loss:  436.8193374204115
loss:  466.94905635570615
loss:  452.9930273717344
loss:  465.55328809393063
weights:  [0.00035304 0.00078305 0.00084558 ... 0.00018089 0.00023833 0.00095234]
cy dot constraint : -4.781298548732335
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 436.8193374204115
   maxcv: 4.7813985487323345
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 300
  status: 4
 success: False
       x: array([0.00035304, 0.00078305, 0.00084558, ..., 0.00018089, 0.00023833,
       0.00095234])
Train data:
------------
Train accuracy :  0.777
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 684 (15%)
Protected in positive class: 210 (9%)
P-rule is: 63%

Test data: 
------------
Test accuracy :  0.7726666666666666
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 99 (10%)
P-rule is: 71%
------------------------------------------------------------------------
------------------------------------------------------------------------
