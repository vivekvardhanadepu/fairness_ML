iter:  100.0 , lambda:  1 , alpha:  0.1111111111111111 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00049103 0.00016496 0.00037106 ... 0.00066816 0.00096595 0.00032369]
loss:  218.1021160710409
loss:  233.61561669881522
loss:  233.28385185569584
loss:  234.1209775267714
loss:  231.62356101357514
loss:  232.86680104786038
loss:  235.05507917859393
loss:  235.35118159540974
loss:  223.79344318309862
loss:  217.9278334317422
loss:  234.88223354586304
loss:  217.79973361219984
loss:  234.44922379639985
loss:  231.06377524011103
loss:  229.46800263242434
loss:  233.168631865761
loss:  224.4766230734353
loss:  234.39943226122847
loss:  233.50940803411902
loss:  234.69189577446883
loss:  233.6618730036715
loss:  234.20086886269164
loss:  217.95328056877432
loss:  217.3211762565507
loss:  233.425521049824
loss:  233.838680825084
loss:  223.1263588511288
loss:  232.65116424095953
loss:  222.96953084356954
loss:  233.76764477297013
loss:  230.22253512568892
loss:  230.68057589717876
loss:  232.5011681065483
loss:  231.42798469378502
loss:  230.09250678029804
loss:  219.15326136934857
loss:  218.2024369882381
loss:  231.5882886872983
loss:  221.22187182477273
loss:  233.4961966052181
loss:  218.7341656176036
loss:  233.69886890694363
loss:  228.86250760653502
loss:  230.77876003354692
loss:  234.55153167471443
loss:  233.66047500234077
loss:  233.04153916530635
loss:  231.3021934290313
loss:  233.05458457291792
loss:  228.30960924356083
loss:  232.87346684051468
loss:  233.91169157263226
loss:  232.9957499581183
loss:  233.07494394602566
loss:  225.40850181239915
loss:  221.2665624140789
loss:  217.34682347857077
loss:  232.53136556250792
loss:  233.5852372012951
loss:  234.0994563386733
loss:  226.65940501624735
loss:  218.00919313616475
loss:  217.41974238926613
loss:  232.55983874059314
loss:  232.78203579500902
loss:  222.77935942696365
loss:  217.06931820232722
loss:  233.02250553619885
loss:  233.04313855680186
loss:  233.07446389295197
loss:  231.86620978097875
loss:  233.03575624091837
loss:  230.1409301706163
loss:  231.88641961594266
loss:  232.09699931991372
loss:  232.74566334778214
loss:  217.5829818485207
loss:  225.27937293725722
loss:  233.36449560109756
loss:  231.60999728293027
loss:  232.8729323933986
loss:  232.48806160370984
loss:  224.59779803553454
loss:  218.89856096215993
loss:  233.01804315779165
loss:  233.05048464699692
loss:  217.01168504222892
loss:  231.95414645949674
loss:  232.03246528484812
loss:  231.79702409581455
loss:  230.84472874485698
loss:  232.23984375774413
loss:  224.19559702083058
loss:  225.27610920533644
loss:  232.3730941753629
loss:  224.74439139851597
loss:  231.3124238606329
loss:  231.31734651778186
loss:  232.04353270325691
loss:  232.1721588289418
weights:  [0.00049103 0.00016496 0.00037106 ... 0.00066816 0.00096595 0.00032369]
cy dot constraint : -4.79870303180029
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 217.01168504222892
   maxcv: 4.79880303180029
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 100
  status: 4
 success: False
       x: array([0.00049103, 0.00016496, 0.00037106, ..., 0.00066816, 0.00096595,
       0.00032369])
Train data:
------------
Train accuracy :  0.7755714285714286
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 658 (14%)
Protected in positive class: 204 (9%)
P-rule is: 63%

Test data: 
------------
Test accuracy :  0.7716666666666666
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 290 (14%)
Protected in positive class: 96 (10%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
