iter:  400.0 , lambda:  1 , alpha:  0.3333333333333333 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [1.41473736e-04 8.34292957e-05 4.97754757e-04 ... 8.95360305e-04
 9.21589762e-04 3.65080366e-04]
loss:  657.997710513404
loss:  701.8426159800941
loss:  700.8730661066911
loss:  703.326148394182
loss:  695.9330851750674
loss:  699.6516404046682
loss:  706.2233781144478
loss:  707.0318012806781
loss:  679.8306226355465
loss:  658.467387514758
loss:  706.0941085527418
loss:  658.6239627499372
loss:  706.8274395164318
loss:  696.7016204973613
loss:  691.9820922094797
loss:  702.9943388193219
loss:  683.013874392538
loss:  706.7112188482189
loss:  704.1763287564771
loss:  707.5861757320353
loss:  704.532222091835
loss:  706.0686499148467
loss:  659.4079684581119
loss:  657.5595051566434
loss:  704.735320417431
loss:  706.0111114038153
loss:  679.8384483353539
loss:  702.4377149362955
loss:  679.3441423497968
loss:  705.8179656691838
loss:  695.1831444000256
loss:  696.653547747213
loss:  701.9678804343386
loss:  698.8225102767266
loss:  694.9789726132732
loss:  663.9284971375058
loss:  658.169816740841
loss:  699.2955704567197
loss:  674.0225144994876
loss:  704.9465035785335
loss:  662.624822313734
loss:  705.5604805279548
loss:  691.1606321183491
loss:  696.851597523104
loss:  708.2609011665577
loss:  705.8460196735613
loss:  703.58049652441
loss:  698.3929186249936
loss:  703.6167725672376
loss:  689.5227302945955
loss:  703.0965193802504
loss:  706.2465641246359
loss:  703.4551062741418
loss:  703.8448360683612
loss:  686.9076918965901
loss:  674.1111075985228
loss:  658.6511525702936
loss:  702.0624985579994
loss:  705.2140541934658
loss:  706.8489058069738
loss:  684.6574304784658
loss:  660.5678733948324
loss:  657.807268235745
loss:  702.1895609746701
loss:  703.0026245665648
loss:  678.8718340516045
loss:  657.8140325105663
loss:  705.89435486049
loss:  705.9540607375384
loss:  706.0637049636771
loss:  702.414590456319
loss:  705.9383987756715
loss:  697.2706054678883
loss:  702.5877860084298
loss:  703.1200431567026
loss:  705.0707273923915
loss:  660.052328687468
loss:  686.7965488091535
loss:  707.1103951681996
loss:  701.9163383002372
loss:  705.4860910530048
loss:  704.2975377461995
loss:  684.6992934174177
loss:  663.7906024342661
loss:  705.9015309528404
loss:  705.9840405274845
loss:  658.4440746759277
loss:  705.1109394966583
loss:  705.3345023227181
loss:  704.6292538359266
loss:  701.7768667805249
loss:  705.9593668285256
loss:  682.9584795579462
loss:  686.3647045536311
loss:  706.3707173158082
loss:  684.7049005106026
loss:  703.176055815471
loss:  703.1967093097658
loss:  705.3687371897952
loss:  705.7543926031489
loss:  705.9710185791472
loss:  696.0326636837524
loss:  705.5910766183575
loss:  673.9262837308113
loss:  699.6042895973209
loss:  705.9892528311159
loss:  690.3193342718608
loss:  705.9878249627204
loss:  705.7962049147983
loss:  707.9518314566499
loss:  702.1276171634945
loss:  704.8793070235674
loss:  663.9181662240708
loss:  703.085805303737
loss:  702.3125894432245
loss:  701.49516763711
loss:  705.4518739702479
loss:  705.5484462316026
loss:  701.3842571492013
loss:  705.8969934701103
loss:  699.6401903014687
loss:  698.4847059670946
loss:  701.3811669375112
loss:  679.5807061359023
loss:  687.5499554332962
loss:  704.7746175398727
loss:  707.9149672899686
loss:  705.9346695588238
loss:  703.8252388227188
loss:  697.5620204820203
loss:  701.333712915654
loss:  695.1409275707716
loss:  695.5873419892029
loss:  697.5614621112438
loss:  708.2666676685275
loss:  665.1454372148678
loss:  705.9797983599419
loss:  701.4706135976171
loss:  665.7116290406864
loss:  704.2792571436189
loss:  702.1519544534655
loss:  684.07850413789
loss:  700.2325338968707
loss:  704.9503677606236
loss:  705.1192477923022
loss:  701.2600754585789
loss:  702.1009739694824
loss:  700.1363222436473
loss:  706.1599609650431
loss:  705.4776314686832
loss:  698.7996737803606
loss:  687.0571918704183
loss:  707.5595315836155
loss:  701.9602495029006
loss:  683.2011071369612
loss:  700.9880761480391
loss:  703.6135238214011
loss:  702.1064094174593
loss:  704.0847600905439
loss:  701.9367044351875
loss:  700.1777669534495
loss:  705.483862482483
loss:  705.0575446528038
loss:  695.5844532897272
loss:  690.1683314924652
loss:  705.8155211856664
loss:  703.4308979545795
loss:  660.9158879868959
loss:  704.9160541504434
loss:  697.5958778326399
loss:  686.2239341122686
loss:  706.5614396379548
loss:  703.3967513977298
loss:  706.6987779895672
loss:  686.700259911409
loss:  704.9335360439386
loss:  671.5985265015393
loss:  704.0638132558263
loss:  703.160277799466
loss:  684.9232816851012
loss:  704.8864654511267
loss:  690.7555804387973
loss:  700.1699669462852
loss:  700.8810499528141
loss:  690.7506016999247
loss:  705.6805922998608
loss:  702.8583455620573
loss:  703.6862814886848
loss:  685.4210395558023
loss:  697.6624172332424
loss:  703.1469018190799
loss:  701.4242315652909
loss:  702.1484285628318
loss:  702.3943452176295
loss:  691.3477593416134
loss:  679.3418370972107
loss:  704.9457064570544
loss:  658.1239117495666
loss:  705.9122015063156
loss:  702.4998421801442
loss:  701.0621930618863
loss:  698.5132686113024
loss:  701.3907150238196
loss:  704.2831505634177
loss:  689.8936749173126
loss:  685.1737467133262
loss:  691.1588861957993
loss:  705.7798436980485
loss:  704.4288502364692
loss:  699.7367531800685
loss:  697.4938920294626
loss:  702.3413139071363
loss:  686.7995356963264
loss:  663.9891499040536
loss:  705.9552354651762
loss:  703.1382424731706
loss:  705.8141074394426
loss:  687.0578525861015
loss:  658.3616947207357
loss:  706.1156320182055
loss:  682.5956561459357
loss:  693.5828181246826
loss:  660.2981004628052
loss:  705.0504588457121
loss:  695.1433173845165
loss:  705.816192776841
loss:  680.0461124101848
loss:  703.6083373924295
loss:  702.7393866563893
loss:  702.1549170816438
loss:  705.3368441013333
loss:  705.7862771808866
loss:  702.5062140102923
loss:  683.5353405033952
loss:  701.3403919412801
loss:  702.80588852547
loss:  702.4946190450287
loss:  665.6048640025709
loss:  675.6457883767446
loss:  699.734718253484
loss:  699.7323235761666
loss:  706.9344691971797
loss:  705.4576754845114
loss:  704.0310967348339
loss:  693.3301409896071
loss:  692.6605760894519
loss:  703.8444210022078
loss:  696.0281732721357
loss:  706.8790688863169
loss:  701.7847360704002
loss:  705.7129708903664
loss:  706.8109921141469
loss:  708.6191123806016
loss:  699.5290394804838
loss:  682.151001316345
loss:  708.5824973403547
loss:  688.9627938371347
loss:  692.3265048805683
loss:  704.0251942475065
loss:  679.4091527449274
loss:  704.5260464097865
loss:  704.3816508556763
loss:  687.019482527092
loss:  707.941128510327
loss:  705.8093512966615
loss:  705.79811153743
loss:  679.8325888873355
loss:  657.2836058328081
loss:  704.8644155981551
loss:  660.656460685271
loss:  703.0198702244898
loss:  705.113400199148
loss:  706.5861467588115
loss:  706.153602972822
loss:  687.0317956312666
loss:  687.0758336182587
loss:  705.0771330407443
loss:  662.407291222747
loss:  662.2760572166324
loss:  688.6721816895118
loss:  706.3703742665417
loss:  704.1693433967287
loss:  676.2643123130717
loss:  681.3074430986937
loss:  704.6527078241303
loss:  686.5716769993511
loss:  699.2070606886414
loss:  662.2191633729782
loss:  687.8574769886075
loss:  687.0890821643236
loss:  704.9459715191382
loss:  691.4027337100099
loss:  701.822299240492
loss:  702.9472159931455
loss:  701.3364922324188
loss:  701.2068908077741
loss:  657.3259402777131
loss:  703.1536544040955
loss:  681.2639379711319
loss:  701.1301260853975
loss:  706.5173394555293
loss:  681.9823283367981
loss:  689.8127823232753
loss:  658.9154901627469
loss:  673.2026509612107
loss:  695.3526021119766
loss:  703.645742875154
loss:  686.7746305432682
loss:  704.1795590896752
loss:  662.3100121562873
loss:  703.8612298605092
loss:  704.5425260462501
loss:  682.9183014354381
loss:  703.758128077878
loss:  704.3684392580569
loss:  703.1403356816156
loss:  690.7736424322736
loss:  704.1591739033842
loss:  698.0002969535005
loss:  658.6559243826015
loss:  704.0826949778046
loss:  694.6557017092892
loss:  705.3875814532192
loss:  704.060755361096
loss:  702.4772008026099
loss:  706.2358915119235
loss:  700.4621062273648
loss:  691.4406741348301
loss:  692.4339706125241
loss:  673.6050284809196
loss:  698.6350047590548
loss:  658.0296194149469
loss:  680.3442483209207
loss:  699.2337609212556
loss:  684.4880438765674
loss:  706.2922760210685
loss:  698.360966967609
loss:  686.1448545558111
loss:  682.1720953477437
loss:  696.2424132635175
loss:  703.1015547936141
loss:  703.3292646871739
loss:  697.1279182098923
loss:  706.8354830936546
loss:  702.8033472603505
loss:  691.0247718400053
loss:  705.8736330213136
loss:  704.5703166096744
loss:  706.8009540869788
loss:  682.7897576871361
loss:  705.3607579348034
loss:  697.8462205974524
loss:  701.8760179649947
loss:  693.2434686262886
loss:  701.1257342901384
loss:  698.6609000207578
loss:  705.6663713175981
loss:  705.5264195847899
loss:  700.4985978001098
loss:  704.5274760349055
loss:  702.5256732377846
loss:  704.8760934879793
loss:  682.9541447497012
loss:  705.0744118003524
loss:  704.8524291589692
loss:  665.1951422251846
loss:  682.1578565944724
loss:  707.1412034108326
loss:  679.8256948220674
loss:  697.2319382597229
loss:  704.5307370212835
loss:  704.5826087311219
loss:  703.7222432448278
loss:  679.7188599257881
loss:  703.5812304863435
loss:  705.1654561374412
loss:  683.7334402868586
loss:  704.1069936106556
loss:  662.3538355141428
loss:  699.5951032674138
loss:  702.3667113900492
loss:  701.553690906027
loss:  674.4498316352716
loss:  702.53434320423
loss:  658.2792807990676
loss:  702.2130694331016
loss:  698.7062566465781
loss:  703.858575502311
loss:  702.2843827444104
loss:  700.8134565122555
loss:  660.5519076990104
loss:  699.6465263275927
loss:  694.7401344873671
loss:  706.2552081812431
loss:  704.6586991288259
loss:  659.5013035340573
loss:  684.462278819274
loss:  700.4622046908715
loss:  703.8274993432865
loss:  702.814097059167
weights:  [1.41473736e-04 8.34292957e-05 4.97754757e-04 ... 8.95360305e-04
 9.21589762e-04 3.65080366e-04]
cy dot constraint : -3.797473992518134
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 657.2836058328081
   maxcv: 3.797573992518134
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 400
  status: 4
 success: False
       x: array([1.41473736e-04, 8.34292957e-05, 4.97754757e-04, ...,
       8.95360305e-04, 9.21589762e-04, 3.65080366e-04])
Train data:
------------
Train accuracy :  0.7764285714285715
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 687 (15%)
Protected in positive class: 211 (9%)
P-rule is: 63%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
