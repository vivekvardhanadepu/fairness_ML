iter:  400.0 , lambda:  1 , alpha:  0.4444444444444444 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [3.26433459e-04 6.52694536e-04 7.04331747e-04 ... 7.80196614e-05
 9.57868572e-04 3.84715391e-05]
loss:  878.195008952548
loss:  937.1258564689979
loss:  935.8201001564503
loss:  939.1314291280207
loss:  929.216519459575
loss:  934.17325476894
loss:  943.2414624870092
loss:  944.1226183319592
loss:  905.8235956594656
loss:  878.9898146470471
loss:  942.910021005967
loss:  879.1497395791856
loss:  943.9471808609115
loss:  930.3055509095065
loss:  923.9714599251049
loss:  938.6818622241219
loss:  910.1155167458312
loss:  943.846586245468
loss:  940.5919435706252
loss:  944.8718867983124
loss:  940.72706560639
loss:  942.8745229626725
loss:  880.2727864518938
loss:  877.7644374651393
loss:  941.2298237377466
loss:  943.0522100675885
loss:  906.0199199738989
loss:  938.1055329637984
loss:  905.3543553135166
loss:  942.8219999337797
loss:  928.4239767448387
loss:  930.6518691808698
loss:  937.5912268762719
loss:  933.2247049109486
loss:  928.4934184424711
loss:  886.466772490024
loss:  878.6979794530824
loss:  933.8616564929853
loss:  898.2920385991322
loss:  941.5193210520952
loss:  884.8135311200365
loss:  942.3723600485722
loss:  923.032972851069
loss:  930.7767120616127
loss:  946.0886830238904
loss:  942.9400046253849
loss:  939.7148618445734
loss:  932.7059383811971
loss:  939.7625933613415
loss:  920.8759093542407
loss:  938.9985749168438
loss:  943.2284970964432
loss:  939.4831449423997
loss:  940.2897669671434
loss:  915.5636788239028
loss:  898.3682563852581
loss:  879.3141090638771
loss:  937.726436954671
loss:  941.884606087045
loss:  944.0375885095044
loss:  914.3880890210951
loss:  881.999678630302
loss:  878.0935587463318
loss:  937.9753256194148
loss:  939.1924135298431
loss:  904.8823805007656
loss:  878.2137512681976
loss:  942.8440768535174
loss:  942.931396873948
loss:  943.1227892722382
loss:  938.1606744856766
loss:  942.9201098357909
loss:  931.188856301855
loss:  938.6299850897742
loss:  939.0296518679488
loss:  941.6477459505451
loss:  881.3091871493474
loss:  915.4762082400721
loss:  944.5765502608624
loss:  937.4640846614768
loss:  942.3988347470183
loss:  940.7500592721749
loss:  912.5914866131153
loss:  886.2830907155711
loss:  942.9242423875542
loss:  942.9979828353846
loss:  879.0329367649447
loss:  941.9789986613354
loss:  942.1051015620391
loss:  941.1132142808641
loss:  937.2091377023265
loss:  942.992362984606
loss:  910.2300061627525
loss:  914.8735076082451
loss:  943.6429962659017
loss:  912.5989925842125
loss:  939.1037023886523
loss:  939.390891755773
loss:  942.0995520657
loss:  942.6219813650628
loss:  942.8477602106916
loss:  929.5187427383291
loss:  942.4127102282577
loss:  898.1423271245385
loss:  934.3065064245579
loss:  942.9871985095989
loss:  921.8988080552526
loss:  942.9029672473415
loss:  942.8021320131913
loss:  945.6256642934054
loss:  937.6559148404168
loss:  941.4306601967273
loss:  886.4531122270702
loss:  938.983537237845
loss:  937.901062317851
loss:  937.1480264500884
loss:  942.2182049179453
loss:  942.3018643597785
loss:  936.678990540143
loss:  942.9184035152347
loss:  934.3541058557541
loss:  932.7923812452024
loss:  936.889628825814
loss:  905.8804032571829
loss:  918.2138721667186
loss:  941.4944470858863
loss:  945.6146250558122
loss:  942.9152187706287
loss:  939.9845371156931
loss:  931.7662047769358
loss:  936.6126251421599
loss:  928.3175609474748
loss:  928.9188963089067
loss:  931.6375122328067
loss:  946.1087673193027
loss:  888.087503180618
loss:  942.9751466345983
loss:  937.0245846781214
loss:  888.8408179649956
loss:  940.6052146251067
loss:  937.7163660960493
loss:  911.7471779875385
loss:  935.1236047661006
loss:  941.5245180329898
loss:  941.7496797799669
loss:  936.5132950735632
loss:  937.8753927514487
loss:  934.9838921112747
loss:  943.1650581559339
loss:  942.3954471594191
loss:  933.3540439686114
loss:  915.8506418176999
loss:  945.0928917432262
loss:  937.581090626265
loss:  910.6096836468089
loss:  936.5551243341708
loss:  939.6601375377014
loss:  937.6371014529567
loss:  940.338358105281
loss:  937.8090661507914
loss:  935.0504628718385
loss:  942.260900954377
loss:  941.8625219160997
loss:  928.9149653378024
loss:  921.7048755130184
loss:  942.7291338320738
loss:  939.5063755992779
loss:  882.4363106573783
loss:  941.4793085595735
loss:  931.5950904511624
loss:  916.4793725275333
loss:  943.7508463862563
loss:  939.4066432716648
loss:  944.0252040522483
loss:  915.3385861249959
loss:  941.5019436909057
loss:  895.1066068996078
loss:  940.3112258321185
loss:  939.0826330147911
loss:  912.8988393012029
loss:  941.440149074235
loss:  922.4866316596192
loss:  935.040111613751
loss:  936.4118032866008
loss:  922.4798686130563
loss:  942.6294997465176
loss:  938.672775840073
loss:  939.8001313343617
loss:  913.5792816199103
loss:  931.7179631011217
loss:  939.0651187411485
loss:  936.7322228871882
loss:  937.711562679343
loss:  938.0179065854071
loss:  923.267312147388
loss:  905.3514876320639
loss:  941.5196141458166
loss:  878.5761732824286
loss:  942.9124557765682
loss:  938.1872707071486
loss:  936.2427692348417
loss:  933.2684384843237
loss:  936.6874195699287
loss:  940.6106189161632
loss:  921.3769222696284
loss:  913.2409373683025
loss:  923.0305635930949
loss:  942.672866693122
loss:  940.762313856886
loss:  934.4560088485229
loss:  931.5098558906079
loss:  937.9523268362092
loss:  915.488459921523
loss:  886.3706484807452
loss:  942.924216839584
loss:  939.0536943198574
loss:  942.7282863854026
loss:  915.8966788175502
loss:  878.9435165744459
loss:  943.1736411131511
loss:  909.7865886650476
loss:  926.6700102711254
loss:  881.5049241522336
loss:  941.8532719751253
loss:  928.3206558353012
loss:  942.819803736843
loss:  906.3865366259324
loss:  939.7514326032157
loss:  938.8500555108589
loss:  937.720359971537
loss:  942.189295869287
loss:  942.7891272058013
loss:  938.1956906211298
loss:  912.873788069956
loss:  936.6214032003818
loss:  938.6040236550302
loss:  938.1807022331524
loss:  888.6987727113662
loss:  900.5851939664101
loss:  934.4532269130849
loss:  934.4502101748284
loss:  944.2059252136563
loss:  942.2259983841196
loss:  940.2254408361254
loss:  925.9134658550212
loss:  925.0598020360463
loss:  940.2819543623771
loss:  929.512801548251
loss:  944.086046706895
loss:  937.2197885474685
loss:  942.5834988238423
loss:  944.2136649573341
loss:  946.4937720177662
loss:  934.3340017068656
loss:  909.1380648228999
loss:  946.444746668687
loss:  920.108403408746
loss:  924.563064900737
loss:  940.3681868826026
loss:  905.6519262855942
loss:  940.9422540178076
loss:  940.7346398753579
loss:  915.7999991023831
loss:  945.5413390084686
loss:  942.7208219818933
loss:  942.795946823887
loss:  906.0938287829755
loss:  877.5326689340317
loss:  941.6119747610494
loss:  882.2264267139423
loss:  939.0691088104037
loss:  941.8602707029928
loss:  943.9723319211561
loss:  943.3764543438274
loss:  915.9946249622644
loss:  916.0623367668154
loss:  941.957708723338
loss:  884.659242022112
loss:  884.4843600305188
loss:  918.1929006466239
loss:  943.6395341348378
loss:  940.6424045197125
loss:  901.6290923224756
loss:  908.1752262389359
loss:  941.3125124289679
loss:  915.3366184426012
loss:  933.9071619383127
loss:  884.4078779217997
loss:  919.1291483676096
loss:  916.1256139887289
loss:  941.7392845739245
loss:  923.5247688080957
loss:  937.4432340791564
loss:  939.0378368055347
loss:  936.7628675908525
loss:  936.7448476881896
loss:  877.7312552265859
loss:  939.3210164226082
loss:  908.1169751427923
loss:  936.5056805211061
loss:  943.7705603516151
loss:  909.2150962715936
loss:  921.3791797998368
loss:  879.7897980490455
loss:  897.2949829533194
loss:  928.89551757131
loss:  939.9197992861559
loss:  915.6246948536011
loss:  940.6565245795317
loss:  884.529270607202
loss:  940.4265978962715
loss:  941.2992353113711
loss:  910.3533437845523
loss:  940.0775400653811
loss:  940.9160609120382
loss:  939.2300634813661
loss:  922.8074010375018
loss:  940.5841227788121
loss:  932.2858758449172
loss:  879.5690653563437
loss:  940.7128137489451
loss:  927.8267931872914
loss:  942.2609581811083
loss:  940.658174371747
loss:  938.3300169450397
loss:  943.4515513209957
loss:  935.599786676663
loss:  923.9235500022041
loss:  924.9211262608698
loss:  897.96773388473
loss:  933.4238250505401
loss:  878.5865830347401
loss:  906.87696671691
loss:  933.9420669832036
loss:  914.2793772787766
loss:  943.5356230017021
loss:  932.7912186968824
loss:  916.6682868285326
loss:  909.3432625829448
loss:  929.9681003702119
loss:  939.1360011363023
loss:  939.4896269092609
loss:  931.3682889622098
loss:  944.2299524669176
loss:  938.7726725123393
loss:  923.0143785492716
loss:  942.8971398522972
loss:  941.2037136771761
loss:  944.1985257058662
loss:  912.0201501800889
loss:  942.200642524691
loss:  932.0731312406666
loss:  937.5139824935477
loss:  926.2395963731518
loss:  936.5005731717655
loss:  933.172000310442
loss:  942.6196814635922
loss:  942.6305031981628
loss:  935.7492479342393
loss:  941.1367107649335
loss:  938.6887223554031
loss:  941.6274586297403
loss:  910.4502550963933
loss:  941.8668149310658
loss:  941.5959587287483
loss:  888.2881947351763
loss:  909.3242271633552
loss:  944.7325145468723
loss:  906.2617786038793
loss:  931.2486802576348
loss:  941.2835681089099
loss:  941.2106134016834
loss:  940.0307505324267
loss:  906.1195025795537
loss:  939.8336058994987
loss:  942.0578388470956
loss:  911.4566647601931
loss:  940.8171118188275
loss:  884.5879336733763
loss:  934.4316035601951
loss:  938.4859289789396
loss:  937.0765888779989
loss:  899.1177305240886
loss:  938.3799090244712
loss:  878.9453550131117
loss:  938.2464472474687
loss:  933.5191593367516
loss:  940.2196290573711
loss:  938.0658480838379
loss:  936.336052934785
loss:  882.087334946438
loss:  934.4991281603096
loss:  927.9826817483668
loss:  943.4124136317125
loss:  941.2648005797392
loss:  880.6980808922261
loss:  912.5220286353766
loss:  935.599989096663
loss:  940.1700852004196
loss:  939.0032749936325
weights:  [3.26433459e-04 6.52694536e-04 7.04331747e-04 ... 7.80196614e-05
 9.57868572e-04 3.84715391e-05]
cy dot constraint : -3.8261341413038616
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 877.5326689340317
   maxcv: 3.826234141303862
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 400
  status: 4
 success: False
       x: array([3.26433459e-04, 6.52694536e-04, 7.04331747e-04, ...,
       7.80196614e-05, 9.57868572e-04, 3.84715391e-05])
Train data:
------------
Train accuracy :  0.7764285714285715
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 687 (15%)
Protected in positive class: 211 (9%)
P-rule is: 63%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
