iter:  200.0 , lambda:  1 , alpha:  0.2222222222222222 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00024132 0.00048131 0.00057169 ... 0.00027768 0.00048051 0.00056411]
loss:  438.62914511953284
loss:  469.5434597675967
loss:  468.88780190704995
loss:  470.5454988911919
loss:  465.5865370849184
loss:  468.0628116029497
loss:  472.4883850360679
loss:  473.0146586335916
loss:  449.7937721084941
loss:  438.77992426885777
loss:  472.409798957577
loss:  438.8264991464366
loss:  472.90399256311576
loss:  466.12623255448005
loss:  462.9465911375801
loss:  470.32115205073103
loss:  451.88057751556954
loss:  472.8222900130872
loss:  471.0963301491853
loss:  473.3803980606975
loss:  471.3188413839254
loss:  472.3931692531006
loss:  439.44523976436096
loss:  438.160269188884
loss:  471.3299959593463
loss:  472.1850920078624
loss:  449.64292362121876
loss:  469.7791252006955
loss:  449.3199444975092
loss:  472.05277211228866
loss:  464.93473354893246
loss:  465.91741170426656
loss:  469.4948152531183
loss:  467.3372715587557
loss:  464.7718668321378
loss:  442.4557747769527
loss:  438.34674741310386
loss:  467.6567565566895
loss:  445.81076124145943
loss:  471.4721856874675
loss:  441.5454094128724
loss:  471.8848033502558
loss:  462.2259422530196
loss:  466.0681247665956
loss:  473.6416862741591
loss:  471.9259903973188
loss:  470.56734287312804
loss:  467.0844860296077
loss:  470.5921902438718
loss:  461.1307285335962
loss:  470.22378828334524
loss:  472.31053793352186
loss:  470.4666822954453
loss:  470.7098337829614
loss:  454.30509899341143
loss:  445.8758522212969
loss:  438.63492050994154
loss:  469.55793008095674
loss:  471.6515805412437
loss:  472.6953478195536
loss:  457.8497951602485
loss:  440.0672290328405
loss:  438.34173680489573
loss:  469.63790204198295
loss:  470.1366590501804
loss:  449.008214251634
loss:  438.10422103835253
loss:  471.0124545932758
loss:  471.0542270361134
loss:  471.1304597257421
loss:  468.6916311682014
loss:  471.0436744543813
loss:  465.2362449491134
loss:  468.8122443140952
loss:  469.14383835967294
loss:  470.44403483792263
loss:  439.68266193445527
loss:  454.5952949847854
loss:  471.77979289534767
loss:  468.26141761041254
loss:  470.745898761028
loss:  469.95398236947057
loss:  453.19996759190633
loss:  442.3928004496498
loss:  471.0234757459331
loss:  471.07643693198094
loss:  438.43665212533375
loss:  470.4905886032007
loss:  470.6423532832485
loss:  470.16533019525957
loss:  468.2411580925334
loss:  471.06163452152987
loss:  452.06054030655037
loss:  454.3026596014987
loss:  471.32558282651394
loss:  453.2035478014889
loss:  469.1815489785152
loss:  469.2221547180858
loss:  470.65713086146127
loss:  470.9151028415315
loss:  471.03408991759636
loss:  464.40406861542107
loss:  470.8081297034236
loss:  446.143575451683
loss:  466.7906226012125
loss:  471.07851972712484
loss:  460.58724229756797
loss:  471.06100407790143
loss:  470.9536868763694
loss:  472.3804184255009
loss:  468.458810556527
loss:  470.3277484293878
loss:  442.4798442760583
loss:  469.12096568015164
loss:  468.5749209559947
loss:  468.0880691014146
loss:  470.71429020081945
loss:  470.7644677453323
loss:  467.9768890243611
loss:  471.0204521835381
loss:  466.8163300123628
loss:  466.0401641631731
loss:  468.01291935623004
loss:  449.8632425122969
loss:  458.7366348723426
loss:  470.27244462703476
loss:  472.3544632837005
loss:  471.0412166379298
loss:  469.6179415715653
loss:  465.310285027376
loss:  467.9427172988973
loss:  463.8046789574238
loss:  464.1047766948835
loss:  465.4452494685171
loss:  472.57370717865797
loss:  443.3140165366007
loss:  471.07186230038167
loss:  467.9550926859958
loss:  443.6971960104642
loss:  469.92410973421977
loss:  468.49318915957707
loss:  452.7925480835079
loss:  467.20267612669164
loss:  470.3760055540943
loss:  470.4271342969391
loss:  467.893987121186
loss:  468.4939296754263
loss:  467.13165526717006
loss:  471.182517739312
loss:  470.74171072191496
loss:  466.28203621957147
loss:  454.7788586646561
loss:  472.1185692426954
loss:  468.39258941500833
loss:  452.20550848738833
loss:  467.73731583230557
loss:  469.45281888771933
loss:  468.4532440240811
loss:  469.79259018761695
loss:  468.3067562444405
loss:  467.1661880077354
loss:  470.7353501819451
loss:  470.4612870376536
loss:  464.10289474630747
loss:  460.4882163395159
loss:  470.9598428982341
loss:  469.3670963764432
loss:  440.280101232841
loss:  470.3525672034633
loss:  465.4427555497422
loss:  457.85593397301346
loss:  471.45837174430227
loss:  469.3298060592328
loss:  471.55146408191763
loss:  454.529103313172
loss:  470.36476611830767
loss:  444.6270443169348
loss:  469.77806361203466
loss:  469.1711302836463
loss:  453.35378535868
loss:  470.3324707294405
loss:  460.88041772345764
loss:  467.16101544234033
loss:  467.6660835594445
loss:  460.8772088403622
loss:  470.87535947990796
loss:  468.9679479900262
loss:  469.5245733032673
loss:  453.6796507126455
loss:  465.49970047550397
loss:  469.16202828040804
loss:  468.0036905138474
loss:  468.4908326338234
loss:  468.6144055646383
loss:  461.2740135557736
loss:  449.71272594773154
loss:  470.371779379963
loss:  438.16369767281606
loss:  471.0278515589112
loss:  468.72716245602
weights:  [0.00024132 0.00048131 0.00057169 ... 0.00027768 0.00048051 0.00056411]
cy dot constraint : -3.825283190526837
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 438.10422103835253
   maxcv: 3.8253831905268374
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 200
  status: 4
 success: False
       x: array([0.00024132, 0.00048131, 0.00057169, ..., 0.00027768, 0.00048051,
       0.00056411])
Train data:
------------
Train accuracy :  0.7767142857142857
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 686 (15%)
Protected in positive class: 210 (9%)
P-rule is: 63%

Test data: 
------------
Test accuracy :  0.773
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 303 (15%)
Protected in positive class: 99 (10%)
P-rule is: 71%
------------------------------------------------------------------------
------------------------------------------------------------------------
