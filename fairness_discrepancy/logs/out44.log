iter:  500.0 , lambda:  1 , alpha:  0.4444444444444444 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00015651 0.00032734 0.00013737 ... 0.00014355 0.00054329 0.00093483]
loss:  878.8506779811786
loss:  938.5824629096427
loss:  937.2852750336094
loss:  940.5703211810987
loss:  930.7219758336453
loss:  935.6502004923881
loss:  944.5273795213288
loss:  945.5050447157143
loss:  905.0610175786445
loss:  879.6466746396984
loss:  944.2924974137679
loss:  879.7732077331398
loss:  945.2936864327439
loss:  931.7883703520537
loss:  925.4897749524334
loss:  940.1252424389048
loss:  909.3012107221997
loss:  945.1601966769678
loss:  941.828920856049
loss:  946.241944259288
loss:  942.1440618340935
loss:  944.2583220810103
loss:  880.9481971807313
loss:  878.4138563091016
loss:  942.6254295017229
loss:  944.3729690988264
loss:  905.2584967876356
loss:  939.539586942152
loss:  904.6009051157229
loss:  944.126228073086
loss:  929.9114516210645
loss:  931.9820243725867
loss:  938.9858834625824
loss:  934.6978079612953
loss:  929.7734721605084
loss:  887.27538171419
loss:  879.315114280829
loss:  935.3306705347701
loss:  897.5719504526235
loss:  942.9099384252895
loss:  885.5722451886862
loss:  943.741363192447
loss:  924.5471759836141
loss:  932.195904283215
loss:  947.3420155704916
loss:  944.0999953194952
loss:  941.1103971577617
loss:  934.1747517302301
loss:  941.1586288564979
loss:  922.3858001130869
loss:  940.4232793147702
loss:  944.6040331641904
loss:  940.9039151368977
loss:  941.533623378126
loss:  914.6859152353579
loss:  897.6686850334894
loss:  879.9155704700524
loss:  939.1154006848433
loss:  943.2694332349433
loss:  945.3944941818901
loss:  915.9099112134944
loss:  882.6631186462672
loss:  878.7476612507226
loss:  939.315405166837
loss:  940.4199894354076
loss:  904.0596117681392
loss:  878.8250464114114
loss:  944.1963347686612
loss:  944.2794979731289
loss:  944.4429500033325
loss:  939.5654089649355
loss:  944.2621439735684
loss:  932.6705960483819
loss:  939.8692250052428
loss:  940.4545938177594
loss:  943.0465327437747
loss:  881.980004758966
loss:  914.5713678132313
loss:  945.7733299879025
loss:  938.7480070308768
loss:  943.6910502325625
loss:  942.0967317549165
loss:  911.7451766307049
loss:  887.0910026595883
loss:  944.234031137235
loss:  944.3299376621106
loss:  879.643513901566
loss:  943.2070221567848
loss:  943.4594830727851
loss:  942.4994308893002
loss:  938.6531976624166
loss:  944.3076096558576
loss:  909.41680701935
loss:  913.984869203372
loss:  944.8661749564274
loss:  911.7525602594861
loss:  940.5288932217632
loss:  940.6704700918744
loss:  943.4789325085628
loss:  943.9935925632814
loss:  944.2260495452786
loss:  931.0125342300865
loss:  943.7821601115119
loss:  897.4329806726486
loss:  935.766336472429
loss:  944.3298795336998
loss:  923.420139839081
loss:  944.2810683122061
loss:  944.1004261906006
loss:  946.9302241578393
loss:  939.0894797110855
loss:  942.8205539534836
loss:  887.2619537430288
loss:  940.4086137261608
loss:  939.3239540946149
loss:  938.4201991668315
loss:  943.5932550491746
loss:  943.6892087231338
loss:  938.1270033989528
loss:  944.2280494213218
loss:  935.8170581371173
loss:  934.2688844925217
loss:  938.2403839439504
loss:  905.0276052112856
loss:  919.7413521678487
loss:  942.759997637401
loss:  946.8904032707002
loss:  944.2572090398738
loss:  941.3998663078199
loss:  932.9159658314708
loss:  938.0601352389483
loss:  929.8188761242735
loss:  930.4164823159124
loss:  933.0960015388692
loss:  947.3403083433614
loss:  888.9104071402206
loss:  944.3173878819639
loss:  938.1793960547129
loss:  889.6655252181121
loss:  942.011270282413
loss:  939.155812096333
loss:  910.9136068819656
loss:  936.5842882029062
loss:  942.9150775515816
loss:  943.0480781910713
loss:  937.9617543861941
loss:  939.2040744995854
loss:  936.4432424864991
loss:  944.5288837519012
loss:  943.6831439064315
loss:  934.7745575021788
loss:  914.9310909924678
loss:  946.4065202439303
loss:  938.9757731321473
loss:  909.769288231708
loss:  937.7502156705671
loss:  941.0727668001493
loss:  939.0763628895166
loss:  941.7487425022672
loss:  938.9045301650208
loss:  936.5114898407954
loss:  943.635557902719
loss:  943.1332872199981
loss:  930.4125817328872
loss:  923.2242319358188
loss:  944.0879375187664
loss:  940.9077059622758
loss:  883.1237746518872
loss:  942.8694429103524
loss:  933.0792918091084
loss:  917.997104483529
loss:  945.0868195289528
loss:  940.8263675410445
loss:  945.2968825728626
loss:  914.4390826816684
loss:  942.892630604486
loss:  894.3900081933252
loss:  941.7209620603268
loss:  940.5081049091591
loss:  912.0479419409941
loss:  942.8300450113671
loss:  924.0043844495706
loss:  936.5011638974939
loss:  937.6075870659148
loss:  923.9977396725894
loss:  943.939501959741
loss:  940.1027000127802
loss:  941.2151380016126
loss:  912.7159306617094
loss:  933.1954839469735
loss:  940.4903200860499
loss:  938.1802664218898
loss:  939.1510758710477
loss:  939.4127402594887
loss:  924.7854657070222
loss:  904.5979504412763
loss:  942.9088354901519
loss:  879.1827639173533
loss:  944.2377893464406
loss:  939.6222056826377
loss:  937.6950799636917
loss:  934.4674930768791
loss:  938.1358994748905
loss:  942.0164469143602
loss:  922.8831113350898
loss:  912.3835857830826
loss:  924.544851219531
loss:  944.0367520059054
loss:  942.1364479239082
loss:  935.9211962981652
loss:  932.9823109325434
loss:  939.3868308022192
loss:  914.580485873679
loss:  886.7199132706284
loss:  944.277350369123
loss:  940.4789242086463
loss:  944.0859430477228
loss:  914.9561328549904
loss:  879.5558449636767
loss:  944.5049396618143
loss:  908.9583627674418
loss:  927.9218632373933
loss:  882.0078911637466
loss:  943.1239370386035
loss:  929.821996119238
loss:  944.1239228915783
loss:  905.5814124973724
loss:  941.1474379544348
loss:  940.0718046180682
loss:  939.1597079881817
loss:  943.4881696189099
loss:  944.0873283452404
loss:  939.630654566065
loss:  914.4049083422744
loss:  938.068975661698
loss:  940.0330351022266
loss:  939.6153584059967
loss:  889.5227400924025
loss:  899.7905188955505
loss:  935.9184611198149
loss:  935.915341205845
loss:  945.5584796737052
loss:  943.6009661342805
loss:  941.6041199609916
loss:  927.4220435959428
loss:  926.5578549480276
loss:  941.5297046131262
loss:  931.0066364924801
loss:  945.4240717709025
loss:  938.6637068440571
loss:  943.9478730993391
loss:  945.4462887501633
loss:  947.7766870226022
loss:  935.7466272728307
loss:  908.3390405307971
loss:  947.7278302823752
loss:  921.6281056019646
loss:  926.0790060454411
loss:  941.7265981101954
loss:  904.8016654510692
loss:  942.3432014027762
loss:  942.1417156758137
loss:  914.8813121200146
loss:  946.8687724621698
loss:  944.0797376663342
loss:  944.0998942384865
loss:  905.2946649673495
loss:  878.1754812781963
loss:  942.9689061869059
loss:  882.9072253681267
loss:  940.4783228170984
loss:  943.238730554723
loss:  945.2708324051745
loss:  944.6930197398627
loss:  915.0852803127289
loss:  915.148309882109
loss:  943.2764890082481
loss:  885.411087070826
loss:  885.2359776510172
loss:  917.2636312564115
loss:  944.9685877690221
loss:  942.0259849839572
loss:  900.8188532574685
loss:  907.3996896477801
loss:  942.6800630405463
loss:  914.451754818659
loss:  935.36574068874
loss:  885.159128861145
loss:  920.4377198335823
loss:  915.1890990423283
loss:  943.0845692395784
loss:  925.0255551921706
loss:  938.8705854237458
loss:  940.4214877812084
loss:  938.1958382215864
loss:  938.1267574614028
loss:  878.3573103122732
loss:  940.5655943022678
loss:  907.3422293675277
loss:  937.9423981257767
loss:  945.1098438225039
loss:  908.3704327531165
loss:  922.8946591728327
loss:  880.4148136908985
loss:  896.664199767431
loss:  930.3329782810522
loss:  941.3182972029563
loss:  914.7303493995074
loss:  942.0396704557685
loss:  885.2808802372476
loss:  941.6900475566526
loss:  942.5847865302629
loss:  909.5503878726486
loss:  941.4714560415026
loss:  942.2945007828253
loss:  940.6386220417037
loss:  924.2527205763224
loss:  941.9799457488597
loss:  933.7520196438386
loss:  880.2358079359759
loss:  941.9815621393883
loss:  929.3222408640515
loss:  943.6322911351268
loss:  941.9455184654138
loss:  939.7489657705538
loss:  944.7862911870088
loss:  937.0462995977756
loss:  925.2106838330121
loss:  926.4088809974305
loss:  897.2329871638073
loss:  934.7522125457912
loss:  879.1839770512399
loss:  906.1179386044673
loss:  935.4012073807401
loss:  915.8093918682498
loss:  944.8649134367984
loss:  934.2573357401328
loss:  918.1049486297081
loss:  908.5531104417728
loss:  931.4479457911459
loss:  940.5123941246754
loss:  940.8937300014413
loss:  932.4912226336734
loss:  945.56201647381
loss:  940.1866908455758
loss:  924.5188393396531
loss:  944.2608671289298
loss:  942.5704720975461
loss:  945.5277270242692
loss:  913.5512709104881
loss:  943.5379469662901
loss:  933.5442051112973
loss:  938.9420671120732
loss:  927.5882410821716
loss:  937.9365186010742
loss:  934.6350717129988
loss:  943.9873568738243
loss:  943.8886737158584
loss:  937.1625794765829
loss:  942.5097366942131
loss:  939.9246035447273
loss:  942.9844250218515
loss:  909.6247565617402
loss:  943.23318269916
loss:  942.9530122377819
loss:  889.0988926386041
loss:  908.5342312574453
loss:  946.0065754508072
loss:  905.4711361219253
loss:  932.7228763128178
loss:  942.5691559176813
loss:  942.5830787479335
loss:  941.4238839746903
loss:  905.3302619289466
loss:  941.2325643175105
loss:  943.3870875886483
loss:  910.638920523297
loss:  942.0107743465943
loss:  885.3395239494694
loss:  935.885642848973
loss:  939.7165220487285
loss:  938.5095348331829
loss:  898.3642307416707
loss:  939.8002887512114
loss:  879.5370685107879
loss:  939.5106685686233
loss:  934.8470070465396
loss:  941.6085526678615
loss:  939.488829952368
loss:  937.6457107026387
loss:  882.7681606606992
loss:  935.9540916968159
loss:  929.4658394527712
loss:  944.7428647278481
loss:  942.651778443184
loss:  881.3583095751354
loss:  911.6571995126767
loss:  937.0464583761839
loss:  941.5636285387037
loss:  940.2983952125105
loss:  938.0375070631486
loss:  942.373726886848
loss:  911.227526587647
loss:  942.933311821845
loss:  943.1932571335684
loss:  937.0303154751933
loss:  937.0920855218483
loss:  936.2742993716474
loss:  888.5932003297675
loss:  878.5750386761003
loss:  899.3076885319266
loss:  942.7261179426151
loss:  908.5751282271655
loss:  942.565461284685
loss:  908.9525154088641
loss:  938.9379513736114
loss:  898.3731523516755
loss:  909.1743413052901
loss:  943.2532098821093
loss:  908.2998915429193
loss:  935.3854036437268
loss:  944.1924581884108
loss:  939.7141323185105
loss:  923.4542798824589
loss:  899.8390936303158
loss:  927.3819194374427
loss:  944.8182344797088
loss:  945.0921566315794
loss:  936.5122942219344
loss:  940.3324198454046
loss:  903.8633408482617
loss:  946.0384408068787
loss:  915.0085895218999
loss:  944.6370247401699
loss:  940.5743688320997
loss:  915.2794379629372
loss:  943.1804663312208
loss:  940.8967762188253
loss:  943.2612466366035
loss:  942.037777498369
loss:  941.6471413167496
loss:  942.5850473428444
loss:  878.3407665072812
loss:  943.3588645663451
loss:  916.9368263438922
loss:  879.2802104404941
loss:  941.5470865015526
loss:  942.7668750668985
loss:  942.8381570669476
loss:  930.594012345022
loss:  889.3181706518699
loss:  940.9614541400097
loss:  935.0317702543632
loss:  943.8134324789808
loss:  943.288678445592
loss:  885.4204089779442
loss:  938.5197119840016
loss:  946.6826306575183
loss:  912.0233052843553
loss:  895.8670107753231
loss:  924.4529666401074
loss:  912.8620936646881
loss:  889.5395053394994
loss:  932.4791868815201
loss:  941.551219308634
loss:  929.6901136611193
loss:  917.8186026430568
loss:  889.1121389404598
loss:  944.3018424471437
loss:  938.8700978149844
loss:  940.631565381641
loss:  940.3724830356399
loss:  940.7319294726749
loss:  942.765649918307
loss:  941.9674044201895
loss:  945.610615998174
loss:  937.9490749926579
loss:  906.9371838016277
loss:  936.7276150008547
loss:  943.2462665032881
loss:  878.1991634503004
loss:  928.7304651425351
loss:  943.2468961422175
loss:  938.0480020106565
loss:  942.6573112631174
loss:  914.410353744289
loss:  889.3499807019138
loss:  899.9955650635338
loss:  937.9864983323876
loss:  879.6234965189366
loss:  940.7170241888389
loss:  944.3438740955623
loss:  913.3887679846313
loss:  880.0946831842228
loss:  936.5232917882804
loss:  880.1696127552948
loss:  940.5588654542322
loss:  878.8143594452588
loss:  939.6400929064392
loss:  913.9734166713789
weights:  [0.00015651 0.00032734 0.00013737 ... 0.00014355 0.00054329 0.00093483]
cy dot constraint : -3.8267789185047634
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 878.1754812781963
   maxcv: 3.8268789185047636
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 500
  status: 4
 success: False
       x: array([0.00015651, 0.00032734, 0.00013737, ..., 0.00014355, 0.00054329,
       0.00093483])
Train data:
------------
Train accuracy :  0.7764285714285715
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 687 (15%)
Protected in positive class: 211 (9%)
P-rule is: 63%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
