iter:  200.0 , lambda:  1 , alpha:  0.0 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [4.61210947e-05 1.54633456e-04 2.19932822e-04 ... 5.61520195e-05
 7.17752571e-04 3.05347832e-04]
loss:  -1.8509099522053536
loss:  -0.5708523335648694
loss:  -0.5818999632400015
loss:  -0.5582703924116168
loss:  -0.6019802224840931
loss:  -0.5941178498421227
loss:  -0.6326129649260785
loss:  -0.5693622244324654
loss:  -4.045705087800369
loss:  -4.5425031111018965
loss:  -4.203054751114875
loss:  -5.044572443201222
loss:  -4.744957034284421
loss:  -4.6593832121062935
loss:  -4.650272996050482
loss:  -4.650738097099424
loss:  -6.293389073210814
loss:  -7.033305375882929
loss:  -6.889903129179258
loss:  -6.684503959436199
loss:  -6.646553001518172
loss:  -6.6870867943104235
loss:  -7.524429913018792
loss:  -8.021380314459355
loss:  -7.659493909831492
loss:  -7.746538033365571
loss:  -9.224797471949417
loss:  -9.737859312404954
loss:  -10.834145099579246
loss:  -11.634697832182972
loss:  -11.10082095835712
loss:  -11.51741224711768
loss:  -11.276432238939977
loss:  -11.03043776113131
loss:  -11.648628842609046
loss:  -12.131222493455017
loss:  -11.746066708005227
loss:  -10.76590741105025
loss:  -14.125318530397665
loss:  -13.766407590938721
loss:  -14.590715417546306
loss:  -14.279758589533667
loss:  -13.995757974931953
loss:  -14.171639412688867
loss:  -14.554256248810269
loss:  -14.793796054101385
loss:  -13.591065824504394
loss:  -13.528173209728333
loss:  -13.590358607121043
loss:  -13.573628682416242
loss:  -13.544747427310796
loss:  -13.591494824659815
loss:  -13.546143234212042
loss:  -13.8721828455705
loss:  -16.953544754476365
loss:  -17.92261709706106
loss:  -18.41828789267438
loss:  -19.170223064581762
loss:  -18.908446283273783
loss:  -18.93487666475267
loss:  -18.6797367835511
loss:  -19.650701205773807
loss:  -19.62657134348639
loss:  -19.518484142107702
loss:  -19.770943844711468
loss:  -21.830493768419004
loss:  -22.32276896437354
loss:  -22.166610233401503
loss:  -22.165708147459103
loss:  -22.20944973936021
loss:  -22.062938406379523
loss:  -22.1789117300694
loss:  -21.964026471236966
loss:  -22.32215194850657
loss:  -22.066860718000292
loss:  -22.116039724635435
loss:  -23.367075077117217
loss:  -24.578646977092156
loss:  -25.630031309964636
loss:  -25.62642334205583
loss:  -25.620049806304195
loss:  -25.57014763374321
loss:  -26.682594178853172
loss:  -26.170839073820673
loss:  -27.644074768731883
loss:  -27.58623656965142
loss:  -28.14456082692416
loss:  -28.17346959504679
loss:  -27.14686463915089
loss:  -27.157041506201807
loss:  -27.194692260373607
loss:  -27.15029803351569
loss:  -30.191392629511846
loss:  -31.22463942705501
loss:  -32.178594912762456
loss:  -33.17394304329231
loss:  -34.19614063112032
loss:  -34.24964513819067
loss:  -33.33049543487934
loss:  -33.320446882479146
loss:  -33.38430716280472
loss:  -33.42452652388615
loss:  -33.31708288448378
loss:  -36.185506678985874
loss:  -36.181774503589686
loss:  -36.20067565352921
loss:  -35.28098261903777
loss:  -35.25613086335824
loss:  -35.230427130701614
loss:  -35.25455362168844
loss:  -35.311802660221886
loss:  -35.23507689801071
loss:  -35.69013170114256
loss:  -35.24293628473367
loss:  -35.33592694650935
loss:  -35.24312483393603
loss:  -35.22086099221908
loss:  -35.268954253736645
loss:  -35.26369944371534
loss:  -35.22238676642989
loss:  -35.26100216158856
loss:  -35.26701346276047
loss:  -35.204097723448484
loss:  -38.084046342746106
loss:  -38.002006289471524
loss:  -38.037851436694126
loss:  -38.12368503335912
loss:  -37.128197180035585
loss:  -37.1921687212667
loss:  -37.665073272713485
loss:  -37.24778873502139
loss:  -37.258813210582325
loss:  -37.24923245213472
loss:  -37.14870838240417
loss:  -37.22219737507169
loss:  -38.037743450761006
loss:  -37.123078593996155
loss:  -37.547268479400756
loss:  -38.01702385358847
loss:  -37.17459139160469
loss:  -37.223110349410675
loss:  -40.13162243769311
loss:  -40.20181210767929
loss:  -39.24255025282747
loss:  -39.4802656254315
loss:  -39.2540953691006
loss:  -39.24395559539115
loss:  -39.289365551313146
loss:  -39.27361288003024
loss:  -39.27752374931842
loss:  -39.21232848524599
loss:  -42.14853204001645
loss:  -42.203993923402805
loss:  -41.23502780036303
loss:  -44.131871316155355
loss:  -44.15879752688036
loss:  -43.48927697715048
loss:  -43.455780761346915
loss:  -43.361453083071204
loss:  -43.39717923820908
loss:  -43.4571742232257
loss:  -43.3042411947988
loss:  -43.21019803904356
loss:  -43.443705014810995
loss:  -43.42923212448433
loss:  -43.291487335745174
loss:  -43.30499509474704
loss:  -44.63353643861277
loss:  -43.815272599296705
loss:  -43.93415950381833
loss:  -43.89921848576624
loss:  -43.750636161143944
loss:  -43.86628293902925
loss:  -43.71042230585173
loss:  -46.462426810762125
loss:  -46.62553939391494
loss:  -48.39385083865115
loss:  -48.5340364665804
loss:  -47.67424335218553
loss:  -50.42541766365626
loss:  -50.54090074135508
loss:  -49.597312166638055
loss:  -49.688394875560185
loss:  -49.65969372983059
loss:  -49.59190962041214
loss:  -49.60197830664937
loss:  -49.6543844990908
loss:  -49.66767334510137
loss:  -52.42218705042296
loss:  -52.43779561226226
loss:  -51.5522867592369
loss:  -51.532476481576225
loss:  -51.54034174393146
loss:  -51.7710779670301
loss:  -51.41045630431261
loss:  -54.24612215594969
loss:  -54.33568868360436
loss:  -55.32031647876805
loss:  -54.49529906789474
loss:  -54.33415757610539
weights:  [4.61210947e-05 1.54633456e-04 2.19932822e-04 ... 5.61520195e-05
 7.17752571e-04 3.05347832e-04]
cy dot constraint : -0.8054393960433316
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: -55.32031647876805
   maxcv: 1.0009950371091338
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 200
  status: 4
 success: False
       x: array([4.61210947e-05, 1.54633456e-04, 2.19932822e-04, ...,
       5.61520195e-05, 7.17752571e-04, 3.05347832e-04])
Train data:
------------
Train accuracy :  0.7767142857142857
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 632 (13%)
Protected in positive class: 192 (8%)
P-rule is: 62%

Test data: 
------------
Test accuracy :  0.7733333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 280 (14%)
Protected in positive class: 91 (10%)
P-rule is: 71%
------------------------------------------------------------------------
------------------------------------------------------------------------
