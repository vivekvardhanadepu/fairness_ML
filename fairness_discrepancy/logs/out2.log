iter:  300.0 , lambda:  1 , alpha:  0.0 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [9.42248229e-04 6.02018547e-04 7.68290319e-04 ... 3.67490223e-05
 9.84391363e-04 6.48007430e-04]
loss:  -1.9182743942859135
loss:  -0.6496348547794994
loss:  -0.6603774616159739
loss:  -0.6375543894316293
loss:  -0.6793086985265777
loss:  -0.6722455406712142
loss:  -0.7143791387985772
loss:  -0.6498199556950013
loss:  -4.09855475368959
loss:  -4.594427142148252
loss:  -4.268104822985601
loss:  -5.0961566710217445
loss:  -4.810273085233746
loss:  -4.721481061200553
loss:  -4.711132755123603
loss:  -4.714116193587323
loss:  -6.330766993991836
loss:  -7.08482599989844
loss:  -6.956080112726072
loss:  -6.749322561012576
loss:  -6.710205490095746
loss:  -6.751678677432425
loss:  -7.577620755025663
loss:  -8.07284081881345
loss:  -7.723774658539945
loss:  -7.81201558189807
loss:  -9.261778900771626
loss:  -9.786767707671263
loss:  -10.86853936935312
loss:  -11.68329503660508
loss:  -11.159698243429993
loss:  -11.578513039325411
loss:  -11.337577860041652
loss:  -11.089797518407751
loss:  -11.709995813073341
loss:  -12.189973868029902
loss:  -11.804491312371592
loss:  -10.835555987649899
loss:  -14.16898805057289
loss:  -13.823004918094139
loss:  -14.628763855454968
loss:  -14.331122227931868
loss:  -14.043085755058879
loss:  -14.22115230819707
loss:  -14.607249645311022
loss:  -14.84700656722362
loss:  -13.657016300446523
loss:  -13.592367269833751
loss:  -13.656290457355187
loss:  -13.635929572355217
loss:  -13.610118901517433
loss:  -13.65779665405097
loss:  -13.611626922898349
loss:  -13.94001954745925
loss:  -16.99348534219992
loss:  -17.94743341839643
loss:  -18.440986201901673
loss:  -19.20555778693694
loss:  -18.956799761698697
loss:  -18.98347882948935
loss:  -18.72314228774015
loss:  -19.680254639452937
loss:  -19.654995862430386
loss:  -19.561311232329444
loss:  -19.81516543991645
loss:  -21.860718121844833
loss:  -22.352357540446135
loss:  -22.20977209009848
loss:  -22.208950794313544
loss:  -22.25305512580958
loss:  -22.104992443794824
loss:  -22.2222331036178
loss:  -22.004232801731575
loss:  -22.366208742910825
loss:  -21.24927956800802
loss:  -21.28854499756364
loss:  -23.412822642757554
loss:  -25.52806043793268
loss:  -25.705340686465266
loss:  -25.002117196287603
loss:  -24.730445487151066
loss:  -24.698446325158688
loss:  -27.70401944205501
loss:  -27.18963669478765
loss:  -27.70178888026615
loss:  -27.69068195409924
loss:  -28.206460758226278
loss:  -28.24062549726892
loss:  -27.267441937841298
loss:  -27.293758434439575
loss:  -27.357663426717977
loss:  -27.255625665790667
loss:  -30.222857042712885
loss:  -31.18981154132947
loss:  -32.15614868978047
loss:  -33.1020751818135
loss:  -34.210800545418124
loss:  -34.278828330408324
loss:  -33.42962172425666
loss:  -33.41654690128649
loss:  -33.50390961237992
loss:  -33.55337284527347
loss:  -33.410727239988894
loss:  -36.19233564652161
loss:  -36.27820392009938
loss:  -35.396151116929914
loss:  -35.38273976961784
loss:  -35.43345992500317
loss:  -35.41999363178899
loss:  -35.44610720032872
loss:  -35.4558657006186
loss:  -35.40846230613191
loss:  -35.764967734113064
loss:  -35.38434211422227
loss:  -35.48030644226091
loss:  -35.43107701378823
loss:  -35.3913271329469
loss:  -35.44497995743768
loss:  -35.39664393499695
loss:  -35.40986607991142
loss:  -35.403402946682434
loss:  -35.39441887109141
loss:  -35.376430461265535
loss:  -38.05540961156224
loss:  -37.986684354335246
loss:  -38.12776460816608
loss:  -37.31471291298803
loss:  -37.24961266266771
loss:  -37.29929460949809
loss:  -37.880038432182744
loss:  -37.35992264738674
loss:  -37.325315241143876
loss:  -37.31632603115714
loss:  -37.202009133146504
loss:  -37.38930753306259
loss:  -38.03956213911505
loss:  -37.24001873141287
loss:  -37.77352168109376
loss:  -38.01898194097931
loss:  -37.28293752737996
loss:  -37.32608197391109
loss:  -40.027541365303094
loss:  -40.191626441968886
loss:  -39.33806183279641
loss:  -39.653564402679564
loss:  -39.329024153610135
loss:  -39.30496246922156
loss:  -39.37945124751326
loss:  -39.39288303573226
loss:  -39.370058730291355
loss:  -39.25997920999796
loss:  -42.02878224967392
loss:  -42.197351007552385
loss:  -41.30368262708821
loss:  -43.9974151728732
loss:  -44.109579014534965
loss:  -43.57667222881376
loss:  -43.516304165505744
loss:  -43.41244612738915
loss:  -43.51193134206295
loss:  -43.50352490188176
loss:  -43.35516975465951
loss:  -43.2611113604227
loss:  -43.455636861108204
loss:  -43.402511282165385
loss:  -43.35069514607846
loss:  -43.345625423832836
loss:  -44.58374510454652
loss:  -43.87873256073601
loss:  -43.96288267317636
loss:  -43.85694032844822
loss:  -43.8143410900822
loss:  -43.929954126048884
loss:  -43.779469083129115
loss:  -46.30434227305562
loss:  -46.56985839778471
loss:  -48.15741723045729
loss:  -48.40938685914267
loss:  -47.65330444634111
loss:  -50.17814553183943
loss:  -50.41042807386926
loss:  -49.50039874582416
loss:  -49.653331033301775
loss:  -49.60636471700378
loss:  -49.49282733530109
loss:  -49.561099055213745
loss:  -49.62094260252574
loss:  -49.65077108229018
loss:  -52.171091478663044
loss:  -52.25316325831633
loss:  -51.474000524473674
loss:  -51.43875372048124
loss:  -51.45476712040333
loss:  -51.73165753822929
loss:  -51.25897374031962
loss:  -53.96223151452868
loss:  -54.16858852429444
loss:  -55.15018947934728
loss:  -54.44433359388226
loss:  -54.26328642526167
loss:  -54.241996124273825
loss:  -54.524605161330484
loss:  -54.24798038200882
loss:  -54.296832699924494
loss:  -54.03077066463645
loss:  -56.87912365076211
loss:  -56.680328811946424
loss:  -57.08015102216158
loss:  -56.44342454915072
loss:  -56.14755168432507
loss:  -56.08476152907704
loss:  -56.26875260326475
loss:  -58.87251586106208
loss:  -60.35573566768126
loss:  -60.56242396624401
loss:  -59.69994953815711
loss:  -59.806258497498106
loss:  -62.39353750831492
loss:  -62.416196560437555
loss:  -62.61265844739654
loss:  -64.37460239398752
loss:  -64.35951380199126
loss:  -64.84443969029219
loss:  -65.06405571865386
loss:  -64.16895509540949
loss:  -64.27936454194243
loss:  -66.65363318880637
loss:  -66.7526458550343
loss:  -66.01921060838566
loss:  -65.9273458090939
loss:  -65.89265353896414
loss:  -65.98374728979783
loss:  -65.92905316187525
loss:  -65.58780274916006
loss:  -65.97663487578501
loss:  -65.98022604972402
loss:  -65.94111285337074
loss:  -67.30523937912454
loss:  -68.70048813375335
loss:  -68.80439218142902
loss:  -67.90885340899487
loss:  -68.09778475564879
loss:  -67.98164138311286
loss:  -68.36601507853467
loss:  -67.64628222751773
loss:  -67.6275355875006
loss:  -68.20608109434176
loss:  -67.7355028400931
loss:  -68.4060853011762
loss:  -67.93759095652547
loss:  -67.993617336932
loss:  -68.36323719585533
loss:  -68.45050018598234
loss:  -67.71544662000461
loss:  -70.54241351099678
loss:  -71.11661959790362
loss:  -69.91625864996621
loss:  -69.95285925520615
loss:  -70.21092252036621
loss:  -72.58603382547834
loss:  -72.72273623427931
loss:  -71.87558281200985
loss:  -74.52453011960836
loss:  -74.95026574802958
loss:  -74.15576044656608
loss:  -74.25449027216732
loss:  -76.45553431213443
loss:  -76.95039738072352
loss:  -77.14173983519524
loss:  -76.6214420568782
loss:  -76.23705136576108
loss:  -76.46641357007667
loss:  -76.50786857923104
loss:  -76.44619785668368
loss:  -78.9347943368366
loss:  -79.73354299880168
loss:  -80.93813080227352
loss:  -80.39932373099714
loss:  -80.43431679094593
loss:  -81.80134166472725
loss:  -83.05577592961913
loss:  -83.20340927032842
loss:  -84.56554154884219
loss:  -85.43808676520378
loss:  -86.55601033123398
loss:  -87.43356910189118
loss:  -88.50798347337073
loss:  -87.98617353785596
loss:  -88.20912372455513
loss:  -89.42129878732126
loss:  -90.62672510319503
loss:  -90.34265190728361
loss:  -90.79248191824009
loss:  -89.82807447265071
loss:  -90.0477426296836
loss:  -89.7678377596761
loss:  -91.17357780485327
loss:  -91.13541703840527
loss:  -92.89502729692218
loss:  -92.97172096582527
weights:  [9.42248229e-04 6.02018547e-04 7.68290319e-04 ... 3.67490223e-05
 9.84391363e-04 6.48007430e-04]
cy dot constraint : -4.782413250020086
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: -92.97172096582527
   maxcv: 4.782513250020086
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 300
  status: 4
 success: False
       x: array([9.42248229e-04, 6.02018547e-04, 7.68290319e-04, ...,
       3.67490223e-05, 9.84391363e-04, 6.48007430e-04])
Train data:
------------
Train accuracy :  0.7794285714285715
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 658 (14%)
Protected in positive class: 197 (9%)
P-rule is: 61%

Test data: 
------------
Test accuracy :  0.7776666666666666
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 294 (14%)
Protected in positive class: 88 (9%)
P-rule is: 65%
------------------------------------------------------------------------
------------------------------------------------------------------------
