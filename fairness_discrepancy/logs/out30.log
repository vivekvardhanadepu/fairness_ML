iter:  100.0 , lambda:  1 , alpha:  0.3333333333333333 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00081024 0.00081095 0.00036517 ... 0.00029503 0.00090365 0.00063993]
loss:  657.882626438124
loss:  701.6240978706138
loss:  700.6493546502734
loss:  703.118171139281
loss:  695.6998847272899
loss:  699.4206928448878
loss:  706.099432178414
loss:  706.8430944564965
loss:  679.7677578803743
loss:  658.3512206940876
loss:  705.9177641919661
loss:  658.5143847274289
loss:  706.6713577646874
loss:  696.4906263357836
loss:  691.753339090583
loss:  702.7836879703463
loss:  682.9715688078822
loss:  706.572806254698
loss:  704.0769530452028
loss:  707.4010671382958
loss:  704.3201395042876
loss:  705.8918747642367
loss:  659.2939643291693
loss:  657.4439293038589
loss:  704.5494410290668
loss:  705.8663468655725
loss:  679.7777590389352
loss:  702.229521579604
loss:  679.2805118015076
loss:  705.6819857369157
loss:  694.9663806732135
loss:  696.5267689201381
loss:  701.7970384555058
loss:  698.5910216953995
loss:  694.8760114690374
loss:  663.7652482578359
loss:  658.059698234108
loss:  699.0665704697551
loss:  673.9664771911574
loss:  704.7634492718452
loss:  662.5685861320751
loss:  705.3892947529356
loss:  690.9313849792229
loss:  696.6788422910184
loss:  708.128595591744
loss:  705.7345464847375
loss:  703.4020785811133
loss:  698.1795617294144
loss:  703.4382325730805
loss:  689.3011356853293
loss:  702.8937788399928
loss:  706.0555844116399
loss:  703.2549150510671
loss:  703.739658585102
loss:  686.8936637189512
loss:  674.0393380582216
loss:  658.5279327260798
loss:  701.8946953378297
loss:  705.0340972391674
loss:  706.6598930723763
loss:  684.432233447316
loss:  660.439885518509
loss:  657.6909605669017
loss:  702.0480653662302
loss:  702.9052995619469
loss:  678.863713121913
loss:  657.6945139241278
loss:  705.7322156258456
loss:  705.794551542562
loss:  705.9191396394041
loss:  702.2354188764514
loss:  705.7819523170933
loss:  697.0493616685048
loss:  702.4871604524452
loss:  702.9172525095787
loss:  704.87497851011
loss:  659.9377152206981
loss:  686.8030524134934
loss:  706.983290036621
loss:  701.7233037790342
loss:  705.3560779306329
loss:  704.1457977142944
loss:  684.674774053584
loss:  663.6276562621314
loss:  705.7626105734553
loss:  705.8333005452122
loss:  658.3318285379421
loss:  705.0068776995498
loss:  705.1745007561635
loss:  704.4515933533128
loss:  701.5630730668787
loss:  705.8175515779807
loss:  682.9186867285204
loss:  686.3619791415869
loss:  706.2620438173833
loss:  684.680398578886
loss:  702.9732016597051
loss:  703.0797605160844
loss:  705.191767700498
loss:  705.5803811070685
weights:  [0.00081024 0.00081095 0.00036517 ... 0.00029503 0.00090365 0.00063993]
cy dot constraint : -2.793282653230392
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 657.4439293038589
   maxcv: 2.7933826532303923
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 100
  status: 4
 success: False
       x: array([0.00081024, 0.00081095, 0.00036517, ..., 0.00029503, 0.00090365,
       0.00063993])
Train data:
------------
Train accuracy :  0.776
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 690 (15%)
Protected in positive class: 211 (9%)
P-rule is: 62%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
