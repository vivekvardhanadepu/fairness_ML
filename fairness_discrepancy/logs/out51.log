iter:  200.0 , lambda:  1 , alpha:  0.5555555555555556 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00032029 0.00070814 0.00038132 ... 0.00081694 0.00064442 0.0005751 ]
loss:  1098.1341788487478
loss:  1171.7170730072849
loss:  1170.0947381439837
loss:  1174.2049364247473
loss:  1161.8305115421433
loss:  1168.0504847877148
loss:  1179.2282054164345
loss:  1180.4418107311799
loss:  1132.9929754780987
loss:  1099.2509705578761
loss:  1178.8797626513274
loss:  1099.4598211740658
loss:  1180.1471082973494
loss:  1163.1369319606513
loss:  1155.247608392137
loss:  1173.6477032581533
loss:  1138.3341017850541
loss:  1180.0002151956799
loss:  1175.894617502232
loss:  1181.3810733355856
loss:  1176.244932967824
loss:  1178.8346861662997
loss:  1100.8510830021658
loss:  1097.7199313347287
loss:  1176.915369572452
loss:  1179.1377613684076
loss:  1133.3596981477522
loss:  1173.0471732108235
loss:  1132.5306775461725
loss:  1178.8399710933186
loss:  1160.9183659240207
loss:  1163.5750926224898
loss:  1172.3165201618694
loss:  1166.9885918622194
loss:  1160.8570723749503
loss:  1108.8330426084513
loss:  1099.0192087406733
loss:  1167.779940777714
loss:  1123.6378029210082
loss:  1177.2728547391598
loss:  1106.6891591612552
loss:  1178.3226976168262
loss:  1154.1982569722747
loss:  1163.7826985349789
loss:  1182.9772867123395
loss:  1179.0963792816194
loss:  1174.9955202169765
loss:  1166.276966872128
loss:  1175.0546794372779
loss:  1151.4819902524891
loss:  1174.1541540117303
loss:  1179.459625195678
loss:  1174.7545189678003
loss:  1175.6449585811279
loss:  1145.2460137005062
loss:  1123.771353544045
loss:  1099.7741415356215
loss:  1172.4817828661405
loss:  1177.7250765041165
loss:  1180.489049028959
loss:  1143.3775875942563
loss:  1103.132372097762
loss:  1098.1251377523438
loss:  1172.7601026154684
loss:  1174.2672448380238
loss:  1131.8126369188833
loss:  1098.396122091519
loss:  1178.9004625471337
loss:  1179.0050267471174
loss:  1179.2259858288849
loss:  1173.0451924608506
loss:  1178.9879831116427
loss:  1164.3952558110568
loss:  1173.56119289029
loss:  1174.1927311977759
loss:  1177.4710974442135
loss:  1102.2785036826422
loss:  1145.093962608238
loss:  1181.1492416907226
loss:  1172.400201823638
loss:  1178.3060122548886
loss:  1176.2542873065781
loss:  1141.5310963778045
loss:  1108.6027581455155
loss:  1178.9710363418258
loss:  1179.0771281522818
loss:  1099.4307114849971
loss:  1177.77189824689
loss:  1177.9665347151918
loss:  1176.7514225540167
loss:  1171.9354142246275
loss:  1179.0595213570598
loss:  1138.5987088148127
loss:  1144.3541422899525
loss:  1179.874331393312
loss:  1141.5405998667068
loss:  1174.284739275528
loss:  1174.51820726545
loss:  1177.9896602430504
loss:  1178.6414617303697
loss:  1178.9996008717542
loss:  1162.3222613167595
loss:  1178.3727027756102
loss:  1123.469087338004
loss:  1168.302946807535
loss:  1179.0709912222649
loss:  1152.7903120230935
loss:  1179.0230105469
loss:  1178.8118336299835
loss:  1182.387803953238
loss:  1172.5372550722518
loss:  1177.1627651609185
loss:  1108.8160003117325
loss:  1174.1348568878013
loss:  1172.8615094740385
loss:  1171.6999888791113
loss:  1178.1336501298538
loss:  1178.2806068541909
loss:  1171.2775459140328
loss:  1178.9637319253836
loss:  1168.3589140669892
loss:  1166.4206465636003
loss:  1171.4024815825453
loss:  1133.0334979116922
loss:  1148.1786298327916
loss:  1177.1603956354127
loss:  1182.3684603505867
loss:  1178.9818465071814
loss:  1175.3765203818668
loss:  1165.307518045132
loss:  1171.1952258490169
loss:  1160.8307103698478
loss:  1161.5774242887974
loss:  1164.9087048114422
loss:  1183.0195039066195
loss:  1110.8322089526382
loss:  1179.056191070683
loss:  1171.8221445912884
loss:  1111.7756919737797
loss:  1176.1434574957918
loss:  1172.5647449185064
loss:  1140.483754934055
loss:  1169.3475535654081
loss:  1177.2792929146754
loss:  1177.7059832473421
loss:  1171.0707566490248
loss:  1172.6287866480063
loss:  1169.1912353518753
loss:  1179.327554190072
loss:  1178.297525560728
loss:  1167.017638962456
loss:  1145.5478027126899
loss:  1181.7226817214203
loss:  1172.3038331552789
loss:  1139.0292331291505
loss:  1170.9613949764541
loss:  1175.035761609107
loss:  1172.4917198480803
loss:  1175.814257690965
loss:  1172.6702967879694
loss:  1169.2563419422465
loss:  1178.1869671227746
loss:  1177.6228441735086
loss:  1161.572454147294
loss:  1152.5418067201242
loss:  1178.760193609268
loss:  1174.7387387069198
loss:  1103.7045424235757
loss:  1177.2232959255782
loss:  1164.9339057860334
loss:  1145.9877719345407
loss:  1180.0316449678885
loss:  1174.6595659785128
loss:  1180.3504958544627
loss:  1144.9263732391914
loss:  1177.2510872476366
loss:  1119.611468024811
loss:  1175.7808076616132
loss:  1174.2585993007506
loss:  1141.9099136651894
loss:  1177.174655724306
loss:  1153.5202553219272
loss:  1169.243410195989
loss:  1170.782391687596
loss:  1153.511721891168
loss:  1178.6034434805406
loss:  1173.7507506923557
loss:  1175.1468863458706
loss:  1142.7526873763504
loss:  1165.0516539377543
loss:  1174.236835328801
loss:  1171.3439742815472
loss:  1172.558753799441
loss:  1173.040272176683
loss:  1154.5037315173713
loss:  1132.527189127508
loss:  1177.2738484569852
loss:  1098.8588850807291
loss:  1178.9678179566695
loss:  1173.1486342000064
weights:  [0.00032029 0.00070814 0.00038132 ... 0.00081694 0.00064442 0.0005751 ]
cy dot constraint : -2.80746336200023
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 1097.7199313347287
   maxcv: 2.8075633620002303
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 200
  status: 4
 success: False
       x: array([0.00032029, 0.00070814, 0.00038132, ..., 0.00081694, 0.00064442,
       0.0005751 ])
Train data:
------------
Train accuracy :  0.776
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 690 (15%)
Protected in positive class: 211 (9%)
P-rule is: 62%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
