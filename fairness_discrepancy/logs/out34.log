iter:  500.0 , lambda:  1 , alpha:  0.3333333333333333 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [0.00039269 0.00047816 0.00092851 ... 0.00019044 0.00072998 0.00042057]
loss:  658.8823711060874
loss:  704.9405357618667
loss:  703.9619713077107
loss:  706.4372197245302
loss:  699.003871423878
loss:  702.7307471255748
loss:  709.3799155157789
loss:  710.1589298396673
loss:  676.032936249961
loss:  659.3603034763245
loss:  709.2316943368575
loss:  659.4122547297985
loss:  709.977730607841
loss:  699.7958643259117
loss:  695.0429786516752
loss:  706.1020543718766
loss:  679.1760227480275
loss:  709.8672266097716
loss:  707.3248334391681
loss:  710.7146813624629
loss:  707.6314480163677
loss:  709.2054349100092
loss:  660.361319757663
loss:  658.4247599430873
loss:  707.8562624396767
loss:  709.1536912816368
loss:  676.0622847630507
loss:  705.5363328204102
loss:  675.5753645126728
loss:  708.9622832293707
loss:  698.2583923809715
loss:  699.765271732689
loss:  705.0925732401068
loss:  701.8891611376025
loss:  698.0784095059291
loss:  665.1392631183302
loss:  658.9386188290478
loss:  702.3660538244752
loss:  670.2747195999708
loss:  708.06949269392
loss:  663.7253255588914
loss:  708.6912135243875
loss:  694.2086487879164
loss:  699.9564324073029
loss:  711.3973412678985
loss:  708.9315773602174
loss:  706.7048266104841
loss:  701.481236159696
loss:  706.7411765118055
loss:  692.5676029971701
loss:  706.201158269014
loss:  709.358214960722
loss:  706.5630920157946
loss:  706.9839284167253
loss:  683.0885421660977
loss:  670.378160050281
loss:  659.365814516471
loss:  705.188739354502
loss:  708.3388944389275
loss:  709.9554655593824
loss:  687.6671833445196
loss:  661.5552704992377
loss:  658.6874674924698
loss:  705.324157949287
loss:  706.1364910692394
loss:  675.0902731141522
loss:  658.5709581356662
loss:  709.0294381626427
loss:  709.0914070588982
loss:  709.2060302176135
loss:  705.536381421747
loss:  709.0764709745027
loss:  700.3488866257035
loss:  705.7206291373044
loss:  706.2249857082844
loss:  708.1818528168325
loss:  661.0138202597838
loss:  682.9838018369655
loss:  710.2077358065872
loss:  704.9237505803239
loss:  708.6298776973769
loss:  707.4350868214927
loss:  680.8754564911865
loss:  665.0001894921733
loss:  709.0453935478898
loss:  709.1243754004273
loss:  659.1785815391662
loss:  708.249788085617
loss:  708.4710465943656
loss:  707.7555366668378
loss:  704.8695865985258
loss:  709.1026862506695
loss:  679.1453697680419
loss:  682.5444778039711
loss:  709.5095596098889
loss:  680.8809432322169
loss:  706.2802587129638
loss:  706.335815223038
loss:  708.4956230419482
loss:  708.8839811520514
loss:  709.0757635033174
loss:  699.0994794122167
loss:  708.7222591964878
loss:  670.1829624358564
loss:  702.6859995189582
loss:  709.1279277749991
loss:  693.3589648955528
loss:  709.1085148804729
loss:  708.9404359252142
loss:  711.0916919949846
loss:  705.2051072388043
loss:  708.0019117725986
loss:  665.1293724908214
loss:  706.1888328779302
loss:  705.3830383205122
loss:  704.6285013687427
loss:  708.5811514102201
loss:  708.6634602078464
loss:  704.4720357697344
loss:  709.0408955930953
loss:  702.7264179246348
loss:  701.5605533784304
loss:  704.5127909078064
loss:  675.8179280954556
loss:  690.5740619254481
loss:  707.9179946609586
loss:  711.056347628213
loss:  709.0728346389856
loss:  706.9355893901744
loss:  700.4566544241707
loss:  704.4225551043068
loss:  698.199942005882
loss:  698.6505021803923
loss:  700.6560101634186
loss:  711.3953710238824
loss:  666.3612860231535
loss:  709.1183975172936
loss:  704.4534753572706
loss:  666.929955843895
loss:  707.3952441619504
loss:  705.2478045845764
loss:  680.2571604380835
loss:  703.3107103808096
loss:  708.0734338934565
loss:  708.1800575236028
loss:  704.3475034651154
loss:  705.236641300833
loss:  703.2072562153614
loss:  709.2886967060173
loss:  708.6224406749116
loss:  701.9109793768002
loss:  683.2555754163672
loss:  710.6961418760166
loss:  705.0849641794872
loss:  679.378072284763
loss:  704.1059504257954
loss:  706.7002198137388
loss:  705.1925557304241
loss:  707.1974162349879
loss:  704.9731448870419
loss:  703.2562958187339
loss:  708.6120500725918
loss:  708.2010528846142
loss:  698.6474391887757
loss:  693.2087882928938
loss:  708.9499013117867
loss:  706.5533062970827
loss:  661.895992857317
loss:  708.0389227125415
loss:  700.6640622044539
loss:  689.2463297004882
loss:  709.700491976144
loss:  706.504315728357
loss:  709.8440010395177
loss:  682.884447633293
loss:  708.0563048590576
loss:  667.8857433406146
loss:  707.176541312492
loss:  706.2654951643641
loss:  681.1034605395377
loss:  708.0089581881796
loss:  693.7993997118045
loss:  703.2485909270645
loss:  703.9984064778745
loss:  693.7943749775743
loss:  708.8218688135084
loss:  705.9599942949284
loss:  706.7965710195555
loss:  681.5995432795421
loss:  700.7440186512903
loss:  706.2520597455498
loss:  704.5125066113766
loss:  705.2442756823115
loss:  705.4487926825178
loss:  694.3919787021651
loss:  675.5732651163402
loss:  708.0678284249118
loss:  658.7890782117196
loss:  709.0530272209844
loss:  705.5989918130335
loss:  704.1472990848795
loss:  701.6172823102613
loss:  704.4800958991356
loss:  707.3991058549162
loss:  692.942128625484
loss:  681.3519096913134
loss:  694.2068721471115
loss:  708.9122737755697
loss:  707.5021074352159
loss:  702.8112455847555
loss:  700.578365418496
loss:  705.4264155094087
loss:  682.9932210390119
loss:  664.1539684271137
loss:  709.0908692160361
loss:  706.2435207103422
loss:  708.9476317579306
loss:  683.2842154710772
loss:  659.1189157953519
loss:  709.2560886940595
loss:  678.7754641428642
loss:  696.6674753738325
loss:  660.7836832611181
loss:  708.1939821321668
loss:  698.2022594016914
loss:  708.9607007956967
loss:  676.2403023972997
loss:  706.7328721478536
loss:  705.8708210801522
loss:  705.250799951288
loss:  708.476674237819
loss:  708.9307011327437
loss:  705.6052525947316
loss:  686.5323268824361
loss:  704.4291747387731
loss:  705.90741691348
loss:  705.5938387161523
loss:  666.8219072641419
loss:  671.9012515753763
loss:  702.8092380576213
loss:  702.8068601111293
loss:  710.065627946463
loss:  708.5868825310102
loss:  707.1017850232649
loss:  696.3849311298713
loss:  695.7227839388505
loss:  706.9818900010101
loss:  699.0952376160499
loss:  709.9786619939083
loss:  704.8774776802072
loss:  708.845775633276
loss:  709.9508179517608
loss:  711.7404701376628
loss:  702.6435555481349
loss:  678.3456425827934
loss:  711.7032195033789
loss:  691.9993163810758
loss:  695.3719683807359
loss:  707.1574233459648
loss:  675.6527223504237
loss:  707.6444386163575
loss:  707.4949054297783
loss:  683.2192123880758
loss:  711.0613828830053
loss:  708.9437830930365
loss:  708.9425369301237
loss:  676.0279567664143
loss:  658.1022908287949
loss:  707.9782400600162
loss:  661.5897318518356
loss:  706.1120447525243
loss:  708.2010031703718
loss:  709.7075965608146
loss:  709.2741342943847
loss:  683.2584776236561
loss:  683.3069226344492
loss:  708.1988874814725
loss:  663.4605561271696
loss:  663.3293940095332
loss:  684.9114956147703
loss:  709.487035202692
loss:  707.2737799574119
loss:  672.5590454748
loss:  677.5359678261632
loss:  707.7633168049138
loss:  682.7803024316449
loss:  702.2638421968487
loss:  663.2715543682949
loss:  690.8844662239177
loss:  683.346177045633
loss:  708.0621441233261
loss:  694.4396453038655
loss:  704.9031055591755
loss:  706.0549731120914
loss:  704.4006383369036
loss:  704.3137805313323
loss:  658.0948535497421
loss:  705.4657831998076
loss:  677.8836769667457
loss:  703.4296246250746
loss:  708.8457243018865
loss:  678.5802306094264
loss:  692.076546688899
loss:  659.5816888106652
loss:  670.1191654959313
loss:  697.6599223062785
loss:  705.9640660525304
loss:  683.3666150965155
loss:  706.5049690977723
loss:  663.3928077212333
loss:  706.2248426000089
loss:  706.901244340182
loss:  679.516669576897
loss:  706.0787044260072
loss:  706.6959311062725
loss:  705.454141278405
loss:  693.0709269382694
loss:  706.47367010779
loss:  700.2797893063685
loss:  659.0094243001611
loss:  706.4445537404035
loss:  696.9299517410816
loss:  707.7135965863904
loss:  706.4180389312362
loss:  704.7862150687728
loss:  708.5771235182337
loss:  702.7566716478099
loss:  693.7744584014946
loss:  694.7157710809417
loss:  670.2755800721554
loss:  700.9794822939583
loss:  658.7648305975961
loss:  676.9819132912038
loss:  701.5206261500621
loss:  686.7329716571246
loss:  708.6355209798254
loss:  700.6496858737237
loss:  688.4295276595336
loss:  678.7781968459363
loss:  698.5266990068206
loss:  705.401790891964
loss:  705.6456624887882
loss:  699.3900306637616
loss:  709.1726956857686
loss:  705.1150819728974
loss:  693.2966242414007
loss:  708.1978457659039
loss:  706.902012486903
loss:  709.1414925133905
loss:  685.0289192253861
loss:  707.6791051473278
loss:  700.1247637780433
loss:  704.1806185395807
loss:  695.5716073913057
loss:  703.4249514008131
loss:  700.9443275991174
loss:  707.9902820128044
loss:  707.8904326028606
loss:  702.815997752973
loss:  706.8571074801051
loss:  704.8915835259778
loss:  707.2120526099853
loss:  679.534377274684
loss:  707.405437471049
loss:  707.1884461649864
loss:  666.40816011552
loss:  678.7642986253682
loss:  709.4989373489542
loss:  676.4252982292118
loss:  699.5067905291911
loss:  706.8895606989317
loss:  706.9120842497348
loss:  706.0430632940054
loss:  676.322919238168
loss:  705.8998118688311
loss:  707.5109107372554
loss:  680.3216376061573
loss:  706.4793255862342
loss:  663.4369649358267
loss:  701.8844758012177
loss:  704.7349616849087
loss:  703.8557609099486
loss:  671.1132916652418
loss:  704.8371425841832
loss:  659.4428441499651
loss:  704.5755113699856
loss:  701.0505560273442
loss:  706.1809897262798
loss:  704.5909716570225
loss:  703.1649582617306
loss:  661.5014935013047
loss:  701.9361217527847
loss:  697.0253479415379
loss:  708.5803618496753
loss:  706.9781967713897
loss:  660.4093574192466
loss:  681.050736040835
loss:  702.7568098428516
loss:  706.1477841252913
loss:  705.1702394215632
loss:  703.4910150305391
loss:  706.7552821959832
loss:  680.7589748690156
loss:  707.1761783533548
loss:  707.3626458581664
loss:  702.7447021207158
loss:  702.754785170921
loss:  702.150113540456
loss:  664.0222549177247
loss:  658.2334253524089
loss:  671.8138432157824
loss:  707.0183928636407
loss:  678.7335885328407
loss:  706.8867648700991
loss:  679.07532462397
loss:  704.1774853880613
loss:  671.1198206928228
loss:  679.1925095183371
loss:  707.4085635842051
loss:  678.5531315676741
loss:  701.508717780615
loss:  708.1203545474704
loss:  704.7549346257388
loss:  692.4969843105508
loss:  672.2095103247218
loss:  695.4596100333837
loss:  708.5995871129852
loss:  708.820895640906
loss:  702.3555234513209
loss:  705.1965398516098
loss:  675.2141847867784
loss:  709.5290929609477
loss:  683.5857528091971
loss:  708.4797457705735
loss:  705.4059801635896
loss:  683.7865769028407
loss:  707.3534408778236
loss:  705.6479504001295
loss:  707.4185115862635
loss:  706.4869794342505
loss:  706.1924411712598
loss:  706.9086818524057
loss:  657.965615170967
loss:  703.6477258313671
loss:  687.186913453616
loss:  658.5505482134215
loss:  702.7303222708335
loss:  703.178969213988
loss:  703.2336769256182
loss:  694.1784449376918
loss:  665.3313424703289
loss:  701.9054163154815
loss:  697.3554968817893
loss:  704.1414392855885
loss:  703.5838704335888
loss:  663.5956071051553
loss:  700.0004350580091
loss:  706.3104091072165
loss:  683.4664642689519
loss:  671.4392443345946
loss:  689.4365866890147
loss:  684.052524071254
loss:  665.4895994851663
loss:  695.4213742878717
loss:  702.2988302402789
loss:  693.4581444100728
loss:  684.6881018560226
loss:  664.9438782008893
loss:  704.3802915488491
loss:  700.2620007865767
loss:  701.6402771442113
loss:  701.4211434120817
loss:  701.6881170011987
loss:  703.1780857910645
loss:  702.7606719620999
loss:  705.3862099969197
loss:  699.5765585238025
loss:  679.6204239393339
loss:  698.7064535608798
loss:  703.6082730702708
loss:  657.8486035098408
loss:  691.5786978354515
loss:  702.459656137248
loss:  698.5666763606599
loss:  702.0067387605237
loss:  684.1401495105232
loss:  665.356552218694
loss:  673.4596840791557
loss:  698.4836338490907
loss:  658.9253875714124
loss:  700.5977937996762
loss:  703.3069413019404
loss:  683.3804578799857
loss:  659.2969769374894
loss:  697.4364903452457
loss:  659.0250023614968
loss:  700.4282894227114
loss:  659.1210810305266
loss:  699.7126227025573
loss:  683.853781287025
weights:  [0.00039269 0.00047816 0.00092851 ... 0.00019044 0.00072998 0.00042057]
cy dot constraint : -6.860384913923485
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 657.8486035098408
   maxcv: 6.860484913923485
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 500
  status: 4
 success: False
       x: array([0.00039269, 0.00047816, 0.00092851, ..., 0.00019044, 0.00072998,
       0.00042057])
Train data:
------------
Train accuracy :  0.7835714285714286
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 661 (14%)
Protected in positive class: 185 (8%)
P-rule is: 57%

Test data: 
------------
Test accuracy :  0.7803333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 291 (14%)
Protected in positive class: 89 (9%)
P-rule is: 67%
------------------------------------------------------------------------
------------------------------------------------------------------------
