iter:  300.0 , lambda:  1 , alpha:  0.5555555555555556 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [2.86765938e-05 8.67629161e-04 7.65462054e-04 ... 4.00806699e-04
 7.80296394e-04 1.98907553e-05]
loss:  1097.8233262281585
loss:  1169.997169734955
loss:  1168.3840381639338
loss:  1172.4703149664097
loss:  1160.1544301221277
loss:  1166.350353210745
loss:  1177.4186278486961
loss:  1178.671314122676
loss:  1135.641507836015
loss:  1098.9332142559315
loss:  1177.1098505392904
loss:  1099.1963661822836
loss:  1178.3581804362468
loss:  1161.4431589396522
loss:  1153.5918930369621
loss:  1171.9168196289215
loss:  1140.9945606871051
loss:  1178.198315647881
loss:  1174.0783406931516
loss:  1179.6040684993977
loss:  1174.5028910270685
loss:  1177.0658458234918
loss:  1100.5015791336516
loss:  1097.42350919792
loss:  1175.1632243023464
loss:  1177.3494447143455
loss:  1135.9924164818894
loss:  1171.3221918870374
loss:  1135.1607493894742
loss:  1177.0460182278541
loss:  1159.2417286603024
loss:  1161.8325942357071
loss:  1170.5719535052724
loss:  1165.298446891395
loss:  1159.1100153952918
loss:  1108.3101512980077
loss:  1098.7701116977005
loss:  1166.0857648514495
loss:  1126.2785691543688
loss:  1175.5178379157624
loss:  1106.256591913807
loss:  1176.556020136712
loss:  1152.5527748182328
loss:  1162.0695836239786
loss:  1181.1598545471159
loss:  1177.277292719243
loss:  1173.2461438188732
loss:  1164.579554141584
loss:  1173.3054245874403
loss:  1149.8422220768139
loss:  1172.4221025566746
loss:  1177.6984154075608
loss:  1173.0189307916705
loss:  1173.839208670804
loss:  1147.8687549547305
loss:  1126.4028175281323
loss:  1099.5897769559813
loss:  1170.7345124557091
loss:  1175.9670533144517
loss:  1178.721465591173
loss:  1141.7726894085677
loss:  1102.7345791987714
loss:  1097.8194716483429
loss:  1170.9935679096695
loss:  1172.4611840335308
loss:  1134.4464492998288
loss:  1098.1618889973406
loss:  1177.1260061006224
loss:  1177.2278711732513
loss:  1177.4376626125936
loss:  1171.3033939961433
loss:  1177.2086299030507
loss:  1162.7088354474104
loss:  1171.7610421543864
loss:  1172.4605247369664
loss:  1175.7218640830645
loss:  1101.904345719141
loss:  1147.708440543493
loss:  1179.3322010734028
loss:  1170.6661452852827
loss:  1176.5092399870598
loss:  1174.4819435908023
loss:  1144.1706321215877
loss:  1108.0806846459677
loss:  1177.178885043923
loss:  1177.2934892767325
loss:  1099.2400339897458
loss:  1175.9569263108556
loss:  1176.1938000173805
loss:  1174.9948696824315
loss:  1170.2176578865397
loss:  1177.2692078595587
loss:  1141.2409696084442
loss:  1146.9784934868976
loss:  1178.0528441772224
loss:  1144.180180657281
loss:  1172.553185147118
loss:  1172.7269156376308
loss:  1176.228625957784
loss:  1176.8752959709198
loss:  1177.242313933548
loss:  1160.6476854156704
loss:  1176.6058402782087
loss:  1126.107828845658
loss:  1166.601967638837
loss:  1177.2913984638099
loss:  1151.1529597571275
loss:  1177.2609378172485
loss:  1177.0163451693797
loss:  1180.5885295902149
loss:  1170.8199056241508
loss:  1175.407794311955
loss:  1108.2928378039346
loss:  1172.4041724194476
loss:  1171.1425858553616
loss:  1169.9141874762931
loss:  1176.3697420029025
loss:  1176.525257318858
loss:  1169.5645831660024
loss:  1177.1715013174444
loss:  1166.6563068116136
loss:  1164.7295806072882
loss:  1169.643045285209
loss:  1135.6456944936351
loss:  1146.5588648352766
loss:  1175.3570525405923
loss:  1180.5584046920496
loss:  1177.202402684407
loss:  1173.6376268552121
loss:  1163.6109826399868
loss:  1169.4812816900946
loss:  1159.1640001362011
loss:  1159.9065980402006
loss:  1163.208949843263
loss:  1181.197204580235
loss:  1110.3374872127101
loss:  1177.2765178357033
loss:  1170.0652723258827
loss:  1111.2793131320698
loss:  1174.3978777108143
loss:  1170.8433719235109
loss:  1143.1254492107219
loss:  1167.6452612639348
loss:  1175.5241884836294
loss:  1175.9414158624909
loss:  1169.3582390960678
loss:  1170.8583696857268
loss:  1167.4910340677893
loss:  1177.5577444449034
loss:  1176.498875596384
loss:  1165.29613258773
loss:  1148.1504302550002
loss:  1179.929395386664
loss:  1170.5592699095273
loss:  1141.675280048237
loss:  1169.158874896967
loss:  1173.3035933332649
loss:  1170.7734512300703
loss:  1174.0723058338617
loss:  1170.8706650242932
loss:  1167.553900440906
loss:  1176.4237485493045
loss:  1175.8207193633398
loss:  1159.9017679327428
loss:  1150.9042436457257
loss:  1176.9880955140056
loss:  1172.992413462221
loss:  1103.3464337543612
loss:  1175.4682018073631
loss:  1163.2506873030168
loss:  1144.3709035912984
loss:  1178.2478033662542
loss:  1172.9239435140548
loss:  1178.5419773281537
loss:  1147.5445135860175
loss:  1175.496262512031
loss:  1122.2362175249584
loss:  1174.038577493247
loss:  1172.5263637671721
loss:  1144.5454287078842
loss:  1175.419773464682
loss:  1151.8788314034903
loss:  1167.5409068303059
loss:  1168.9805131017247
loss:  1151.8703750844352
loss:  1176.8133521312418
loss:  1172.022397991048
loss:  1173.4079472285673
loss:  1145.3855550102571
loss:  1163.361614279467
loss:  1172.504551642493
loss:  1169.630635139198
loss:  1170.8373993785506
loss:  1171.3180612004778
loss:  1152.8602870674217
loss:  1135.1570909101397
loss:  1175.5190120449568
loss:  1098.7088002522974
loss:  1177.181966412245
loss:  1171.4235387485696
loss:  1169.0270791638231
loss:  1165.0532491812323
loss:  1169.5740025806194
loss:  1174.4045185105022
loss:  1150.460086094101
loss:  1144.9688536766478
loss:  1152.5498295693035
loss:  1176.9242990433797
loss:  1174.706405828727
loss:  1166.819824970567
loss:  1163.086912139753
loss:  1171.1693940525263
loss:  1147.7129960507395
loss:  1108.5652226357377
loss:  1177.2252523390398
loss:  1172.4900274189047
loss:  1176.987821855959
loss:  1148.1587677387115
loss:  1099.0774491685372
loss:  1177.5125189685214
loss:  1140.6565863690696
loss:  1156.8272032175146
loss:  1102.3748793738587
loss:  1175.8091435780657
loss:  1159.1678307364182
loss:  1177.043169289279
loss:  1136.3905129759755
loss:  1173.2913288317814
loss:  1172.0316364430987
loss:  1170.8482194885353
loss:  1176.25579789755
loss:  1177.0000211997524
loss:  1171.4341758838752
loss:  1139.905846507742
loss:  1169.4923431228356
loss:  1171.9369521591889
loss:  1171.4152303876554
loss:  1111.1019772266504
loss:  1129.0448242400605
loss:  1166.8163379360005
loss:  1166.8125029711769
loss:  1178.8529989557392
loss:  1176.3794901913734
loss:  1174.043126459772
loss:  1156.1543528769653
loss:  1155.0526129585567
loss:  1173.8362522178354
loss:  1160.6401400028542
loss:  1178.7969848397322
loss:  1170.2310253567493
loss:  1176.8126317624844
loss:  1178.7734978588146
loss:  1181.7313169413821
loss:  1166.5127649751973
loss:  1139.8822088482152
loss:  1181.6706237984604
loss:  1148.9061280648536
loss:  1154.4854713667455
loss:  1174.0196724958948
loss:  1135.3557595878738
loss:  1174.8115114281434
loss:  1174.5681737784237
loss:  1148.086106264815
loss:  1180.557901099673
loss:  1176.9775545036277
loss:  1177.013513820678
loss:  1136.029511955802
loss:  1097.287815267721
loss:  1175.7289751198707
loss:  1103.2382162082852
loss:  1172.6236245897278
loss:  1176.1336110504062
loss:  1178.6446753349212
loss:  1177.9079155470167
loss:  1148.435981658732
loss:  1148.5115641822258
loss:  1176.1190515990072
loss:  1106.218063393796
loss:  1105.9992712954122
loss:  1151.17452667552
loss:  1178.2562354573042
loss:  1174.5519064760604
loss:  1130.4187623965092
loss:  1138.7915212161663
loss:  1175.3680773623944
loss:  1147.6564407672167
loss:  1166.2632950792304
loss:  1105.9041240440988
loss:  1147.60013709702
loss:  1148.5431067272382
loss:  1175.8761555162266
loss:  1153.2802561712215
loss:  1170.6237333058377
loss:  1172.522839414807
loss:  1169.82056804514
loss:  1169.6386687799677
loss:  1097.6918075864644
loss:  1173.0651087574552
loss:  1138.7177547899394
loss:  1169.468540616948
weights:  [2.86765938e-05 8.67629161e-04 7.65462054e-04 ... 4.00806699e-04
 7.80296394e-04 1.98907553e-05]
cy dot constraint : -3.7848271310667796
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 1097.287815267721
   maxcv: 3.78492713106678
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 300
  status: 4
 success: False
       x: array([2.86765938e-05, 8.67629161e-04, 7.65462054e-04, ...,
       4.00806699e-04, 7.80296394e-04, 1.98907553e-05])
Train data:
------------
Train accuracy :  0.7764285714285715
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 687 (15%)
Protected in positive class: 211 (9%)
P-rule is: 63%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
