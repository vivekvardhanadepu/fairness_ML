iter:  200.0 , lambda:  1 , alpha:  0.3333333333333333 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [9.32480918e-04 8.93431543e-04 6.41541524e-04 ... 5.92327660e-04
 3.77758525e-04 3.00419439e-05]
loss:  657.6448775523864
loss:  700.8389799119155
loss:  699.864523990417
loss:  702.3335373069306
loss:  694.9117804161062
loss:  698.6357489943424
loss:  705.332392160903
loss:  706.0649490388422
loss:  680.5710222600275
loss:  658.1105049488159
loss:  705.1376596937922
loss:  658.2949154556599
loss:  705.8950283141245
loss:  695.7019195996508
loss:  690.9650569393124
loss:  701.9989326885942
loss:  683.7883025731514
loss:  705.8010750803804
loss:  703.3199679174209
loss:  706.62475931413
loss:  703.5414486723793
loss:  705.1117188721958
loss:  659.029412982983
loss:  657.212801336243
loss:  703.7721665174647
loss:  705.0978174468154
loss:  680.5766575723785
loss:  701.4499707690071
loss:  680.0771603998005
loss:  704.9159906535157
loss:  694.182422286229
loss:  695.7629290493244
loss:  701.0193579118676
loss:  697.8110137914513
loss:  694.12324256202
loss:  663.4615867991727
loss:  657.8468279702367
loss:  698.2865149318511
loss:  674.7546424434292
loss:  703.9866371076316
loss:  662.2898971447777
loss:  704.6146970787449
loss:  690.1481871792392
loss:  695.9009467678892
loss:  707.3727041835031
loss:  705.004385987205
loss:  702.6242228469299
loss:  697.396185865713
loss:  702.6602822581696
loss:  688.5193450866541
loss:  702.1146587278233
loss:  705.2834490377156
loss:  702.4758332873174
loss:  702.9851858897359
loss:  687.7159269761729
loss:  674.8224007421243
loss:  658.3515886908643
loss:  701.1176918199367
loss:  704.2579077887586
loss:  705.8918890505377
loss:  683.6552592475385
loss:  660.1780413559313
loss:  657.4564791170066
loss:  701.2779193262479
loss:  702.1546714108687
loss:  679.6720183181949
loss:  657.4937179897237
loss:  704.9596560834519
loss:  705.0222965686584
loss:  705.1507515457283
loss:  701.4565250553893
loss:  705.0105934480813
loss:  696.2657225031804
loss:  701.7347485913158
loss:  702.1380361862035
loss:  704.0990029088504
loss:  659.6622910942438
loss:  687.6282217315327
loss:  706.2445441805415
loss:  700.986753463538
loss:  704.5922635069595
loss:  703.3738268361693
loss:  685.4928970715596
loss:  663.3242320218949
loss:  704.995739094295
loss:  705.063269468748
loss:  658.1188691637215
loss:  704.2551023004733
loss:  704.4013441765203
loss:  703.6749206908117
loss:  700.7829661327645
loss:  705.049835136589
loss:  683.7304772331323
loss:  687.1857881395026
loss:  705.5110328880859
loss:  685.4985589921907
loss:  702.1940847553408
loss:  702.319135407312
loss:  704.4161188993932
loss:  704.8055506764488
loss:  705.0038625051294
loss:  695.0203944234387
loss:  704.6450455656895
loss:  674.648264928935
loss:  698.6062755527537
loss:  705.0611176378401
loss:  689.3025324700752
loss:  705.0293062656685
loss:  704.8982916561802
loss:  707.0380813479035
loss:  701.1310262558075
loss:  703.919807617292
loss:  663.451122549445
loss:  702.1040675024112
loss:  701.3184547225704
loss:  700.6274000050655
loss:  704.5022050843119
loss:  704.583984739441
loss:  700.3881053811328
loss:  704.9912845128746
loss:  698.6406665743158
loss:  697.4769185547225
loss:  700.4650299070134
loss:  680.400518851885
loss:  686.535663891244
loss:  703.8995222660134
loss:  707.0180362420623
loss:  705.0068790783022
loss:  702.8494160948458
loss:  696.7154332923905
loss:  700.3376091834675
loss:  694.1240503661143
loss:  694.5727144055044
loss:  696.5781140191734
loss:  707.3872908244603
loss:  664.6669454101783
loss:  705.0518866429891
loss:  700.620710472558
loss:  665.2316658270281
loss:  703.3093963273898
loss:  701.1609264320746
loss:  684.8637939000704
loss:  699.2284577134792
loss:  703.9905235358691
loss:  704.183918637998
loss:  700.2637437974155
loss:  701.1970054384708
loss:  699.1297197479969
loss:  705.2134499430505
loss:  704.5870817218528
loss:  697.8424375084314
loss:  687.8985771248238
loss:  706.6414516178057
loss:  701.0117235561947
loss:  683.9982203516698
loss:  700.1637787997705
loss:  702.6273895614884
loss:  701.1107067083744
loss:  703.1121594750806
loss:  701.142256946814
loss:  699.1734439231146
loss:  704.53455078695
loss:  704.178798604175
loss:  694.5698414002258
loss:  689.1540797181714
loss:  704.876568377735
loss:  702.4709339525806
loss:  660.5258786710328
loss:  703.956407283322
loss:  696.5834018473513
loss:  685.2218792512307
loss:  705.6349439075217
loss:  702.4180344563434
loss:  705.8120008158819
loss:  687.528485093752
loss:  703.9736703935897
loss:  672.3376110147318
loss:  703.0914940656357
loss:  702.1779786684724
loss:  685.7200420925791
loss:  703.9269705859671
loss:  689.7409473878857
loss:  699.1656203538018
loss:  700.056468099359
loss:  689.7359261826921
loss:  704.7750454112104
loss:  701.8733253933488
loss:  702.7105813346586
loss:  686.2258736848668
loss:  696.6602113269305
loss:  702.1647465477217
loss:  700.4279841579402
loss:  701.1573558783608
loss:  701.4100047769172
loss:  690.330473623824
loss:  680.0748791564496
loss:  703.9866270188586
loss:  657.8599706748766
loss:  704.995673763922
loss:  701.5116173147443
weights:  [9.32480918e-04 8.93431543e-04 6.41541524e-04 ... 5.92327660e-04
 3.77758525e-04 3.00419439e-05]
cy dot constraint : -2.7794279724818094
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 657.212801336243
   maxcv: 2.7795279724818096
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 200
  status: 4
 success: False
       x: array([9.32480918e-04, 8.93431543e-04, 6.41541524e-04, ...,
       5.92327660e-04, 3.77758525e-04, 3.00419439e-05])
Train data:
------------
Train accuracy :  0.776
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 690 (15%)
Protected in positive class: 211 (9%)
P-rule is: 62%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
