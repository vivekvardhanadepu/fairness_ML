iter:  200.0 , lambda:  1 , alpha:  0.4444444444444444 , kernel: rbf method:  cobyla , catol:  0.0001 batches:  100
x_init:  [7.52070014e-04 2.32416382e-04 2.92062773e-06 ... 3.28301461e-04
 8.46284334e-04 9.40952782e-04]
loss:  877.160045425898
loss:  933.0134766850852
loss:  931.7300370536888
loss:  934.9805440656037
loss:  925.1453166586191
loss:  930.1111323097175
loss:  938.8730324118275
loss:  939.9358764063937
loss:  911.174720232031
loss:  877.940573497003
loss:  938.6636495423214
loss:  878.2478165736895
loss:  939.6452762017494
loss:  926.1446841005502
loss:  919.8843846471063
loss:  934.5407505595481
loss:  915.4708234617071
loss:  939.5047658084285
loss:  936.1917731769242
loss:  940.6824714254509
loss:  936.6288628148978
loss:  938.6293386923961
loss:  879.1446990449678
loss:  876.7571082284761
loss:  937.0252591486882
loss:  938.7420036980222
loss:  911.3316067046197
loss:  933.9742970147788
loss:  910.6633040554034
loss:  938.4935582772982
loss:  924.3022512219131
loss:  926.3065540536933
loss:  933.3274239860121
loss:  929.1823628400745
loss:  924.1251595675337
loss:  885.00172346148
loss:  877.8236639844953
loss:  929.8091923900777
loss:  903.5190082476113
loss:  937.3065874580327
loss:  883.5691046406067
loss:  938.1271592329532
loss:  918.9662482114685
loss:  926.5219721820788
loss:  941.8081041925705
loss:  938.741843378706
loss:  935.4784521396386
loss:  928.5728981887625
loss:  935.5260202898771
loss:  916.7894970092603
loss:  934.8486190369033
loss:  939.0751448310249
loss:  935.3233245813086
loss:  935.9142576642698
loss:  920.8400784265565
loss:  903.6240910566491
loss:  878.5268093111519
loss:  933.4550691597424
loss:  937.6637048800162
loss:  939.90240107168
loss:  910.3483500059708
loss:  880.7812882808969
loss:  877.0631535655976
loss:  933.6444680881012
loss:  934.8107050541529
loss:  910.0494588345787
loss:  877.3444380243362
loss:  938.5764111809356
loss:  938.6552702879928
loss:  938.8127449171008
loss:  933.9213191922149
loss:  938.6377566110992
loss:  927.085175546447
loss:  934.2501132164215
loss:  934.8792595138347
loss:  937.4893479693835
loss:  880.1415474281968
loss:  920.6911267639437
loss:  940.3845779752115
loss:  933.5462916923984
loss:  938.0610047763913
loss:  936.4464987647585
loss:  917.8830440327965
loss:  884.8196189176953
loss:  938.6016908915252
loss:  938.7015906439352
loss:  878.2190723740866
loss:  937.6153956510034
loss:  937.8256117903672
loss:  936.8806632240344
loss:  933.0963596932937
loss:  938.6758049949534
loss:  915.5413541579467
loss:  920.1158753125954
loss:  939.3015039560672
loss:  917.8907490216757
loss:  934.9541624934598
loss:  935.0234399288435
loss:  937.8703695716027
loss:  938.3861424927767
loss:  938.7188801024081
loss:  925.4458547289586
loss:  938.1668502917664
loss:  903.3862562657727
loss:  930.2054736272113
loss:  938.7038375605939
loss:  917.8533982430469
loss:  938.7101038818091
loss:  938.4684614036963
loss:  941.3486788868578
loss:  933.6008104019777
loss:  937.2185011597455
loss:  884.9874715735214
loss:  934.8356392858985
loss:  933.8685040061054
loss:  932.7683337293884
loss:  937.9805077622873
loss:  938.1272671978743
loss:  932.5778997728044
loss:  938.5957371238854
loss:  930.2447038050199
loss:  928.7124667217547
loss:  932.564202491533
loss:  910.989522356866
loss:  914.1806112943606
loss:  937.1319434644281
loss:  941.3196092950557
loss:  938.6327061992703
loss:  935.8159243517713
loss:  927.9809329739629
loss:  932.5101629348869
loss:  924.2652049652409
loss:  924.8559593112491
loss:  927.4596942117556
loss:  941.8469303840151
loss:  886.5820673944316
loss:  938.6918436412384
loss:  933.0700394468041
loss:  887.3316684654764
loss:  936.4183932492256
loss:  933.594129780663
loss:  917.0487933448218
loss:  931.0506796078189
loss:  937.3116274907333
loss:  937.7228339429456
loss:  932.4129024352181
loss:  933.532679686581
loss:  930.9370791080092
loss:  938.9349345365912
loss:  938.0508738454746
loss:  929.1079958059096
loss:  921.0342873445833
loss:  940.8235360997319
loss:  933.3172378955227
loss:  915.876094794628
loss:  932.1707667086582
loss:  935.5818420637761
loss:  933.5519549549169
loss:  936.1612020048561
loss:  933.637875514764
loss:  930.9772634955003
loss:  938.0245399496284
loss:  937.5045740995002
loss:  924.8522447361846
loss:  917.6516161247016
loss:  938.4680485681706
loss:  935.278057738261
loss:  881.2563027009145
loss:  937.2667772995321
loss:  927.535478253092
loss:  912.4205494201988
loss:  939.4715211935787
loss:  935.247272232474
loss:  939.6918241949302
loss:  920.563301211535
loss:  937.2894909830244
loss:  900.2478927414633
loss:  936.1339505309343
loss:  934.9319263950761
loss:  918.1794999275687
loss:  937.2281667251066
loss:  918.4306084734563
loss:  930.9667825533351
loss:  932.0284367397292
loss:  918.4239088781477
loss:  938.3108375361671
loss:  934.5320188949587
loss:  935.6317985658987
loss:  918.84906616554
loss:  927.6045328361845
loss:  934.9144460330173
loss:  932.6304500081736
loss:  933.58936252678
loss:  934.0272781390414
loss:  919.2194392024196
loss:  910.6602120555566
loss:  937.307846706797
loss:  877.8751100797896
loss:  938.6102292563462
loss:  934.0553451330219
weights:  [7.52070014e-04 2.32416382e-04 2.92062773e-06 ... 3.28301461e-04
 8.46284334e-04 9.40952782e-04]
cy dot constraint : -2.7544041316556664
Optimization problem did not converge.. Check the solution returned by the optimizer.
Returned solution is:
     fun: 876.7571082284761
   maxcv: 2.7545041316556667
 message: 'Did not converge to a solution satisfying the constraints. See `maxcv` for magnitude of violation.'
    nfev: 200
  status: 4
 success: False
       x: array([7.52070014e-04, 2.32416382e-04, 2.92062773e-06, ...,
       3.28301461e-04, 8.46284334e-04, 9.40952782e-04])
Train data:
------------
Train accuracy :  0.776
Total data points: 7000
# non-protected examples: 4699
# protected examples: 2301
Non-protected in positive class: 690 (15%)
Protected in positive class: 211 (9%)
P-rule is: 62%

Test data: 
------------
Test accuracy :  0.7723333333333333
Total data points: 3000
# non-protected examples: 2055
# protected examples: 945
Non-protected in positive class: 304 (15%)
Protected in positive class: 100 (11%)
P-rule is: 72%
------------------------------------------------------------------------
------------------------------------------------------------------------
